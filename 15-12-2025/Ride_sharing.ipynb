{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ride-Sharing Trips Analytics (Uber / Ola–style)"
      ],
      "metadata": {
        "id": "lPtsn2sj5vPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Dataset in PySpark"
      ],
      "metadata": {
        "id": "MuoJnQ-550IM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d5irTV5c4c7i"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "(\"T001\",\"Amit\",\"Hyderabad\",\"Ramesh\",\"Sedan\",12.5,320,28,\"UPI\",\"Completed\"),\n",
        "(\"T002\",\"Neha\",\"Bangalore\",\"Suresh\",\"Mini\",8.2,210,22,\"Card\",\"Completed\"),\n",
        "(\"T003\",\"Rahul\",\"Delhi\",\"Anil\",\"Bike\",5.1,120,15,\"Cash\",\"Completed\"),\n",
        "(\"T004\",\"Pooja\",\"Mumbai\",\"Vikas\",\"SUV\",18.0,560,45,\"UPI\",\"Cancelled\"),\n",
        "(\"T005\",\"Arjun\",\"Chennai\",\"Kumar\",\"Mini\",7.8,200,20,\"UPI\",\"Completed\"),\n",
        "(\"T006\",\"Sneha\",\"Hyderabad\",\"Ramesh\",\"Sedan\",14.2,360,32,\"Card\",\"Completed\"),\n",
        "(\"T007\",\"Karan\",\"Delhi\",\"Anil\",\"Bike\",6.3,140,18,\"UPI\",\"Completed\"),\n",
        "(\"T007\",\"Karan\",\"Delhi\",\"Anil\",\"Bike\",6.3,140,18,\"UPI\",\"Completed\"),\n",
        "(\"T008\",\"Riya\",\"Bangalore\",\"Suresh\",\"Sedan\",11.0,300,27,\"Wallet\",\"Completed\"),\n",
        "(\"T009\",\"Vikas\",\"Mumbai\",\"Vijay\",\"SUV\",20.5,650,50,\"Card\",\"Completed\"),\n",
        "(\"T010\",\"Anjali\",\"Chennai\",\"Kumar\",\"Bike\",4.9,110,14,\"Cash\",\"Complete\"),\n",
        "(\"T011\",\"Farhan\",\"Delhi\",\"Anil\",\"Mini\",9.6,240,25,\"UPI\",\"Completed\"),\n",
        "(\"T012\",\"Megha\",\"Hyderabad\",\"Ramesh\",\"SUV\",19.2,610,48,\"Card\",\"Cancelled\"),\n",
        "(\"T013\",\"Suresh\",\"Bangalore\",\"Suresh\",\"Sedan\",13.0,340,30,\"UPI\",\"Completed\"),\n",
        "(\"T014\",\"Divya\",\"Mumbai\",\"Vikas\",\"Mini\",10.2,260,26,\"Wallet\",\"Completed\"),\n",
        "(\"T015\",\"Nikhil\",\"Delhi\",\"Anil\",\"Sedan\",15.5,390,34,\"UPI\",\"Completed\"),\n",
        "(\"T016\",\"Kavya\",\"Chennai\",\"Kumar\",\"Sedan\",12.1,315,29,\"UPI\",\"Completed\"),\n",
        "(\"T017\",\"Rohit\",\"Hyderabad\",\"Ramesh\",\"SUV\",22.0,700,55,\"Card\",\"Completed\"),\n",
        "(\"T018\",\"Simran\",\"Bangalore\",\"Suresh\",\"Bike\",5.8,130,16,\"Cash\",\"Completed\"),\n",
        "(\"T019\",\"Ayesha\",\"Mumbai\",\"Vijay\",\"Mini\",9.9,250,24,\"UPI\",\"Completed\"),\n",
        "(\"T020\",\"Manish\",\"Delhi\",\"Anil\",\"Bike\",6.0,135,17,\"Wallet\",\"Completed\"),\n",
        "(\"T021\",\"Priya\",\"Hyderabad\",\"Ramesh\",\"Sedan\",14.8,380,33,\"Card\",\"Completed\"),\n",
        "(\"T022\",\"Yash\",\"Chennai\",\"Kumar\",\"SUV\",21.3,680,52,\"UPI\",\"Completed\"),\n",
        "(\"T023\",\"Naina\",\"Bangalore\",\"Suresh\",\"Mini\",10.7,270,28,\"UPI\",\"Completed\"),\n",
        "(\"T024\",\"Sameer\",\"Mumbai\",\"Vikas\",\"Sedan\",13.9,350,31,\"Wallet\",\"Completed\"),\n",
        "(\"T025\",\"Ritika\",\"Delhi\",\"Anil\",\"Bike\",5.4,125,16,\"Cash\",\"Completed\"),\n",
        "(\"T026\",\"Gopal\",\"Hyderabad\",\"Ramesh\",\"Mini\",8.9,225,23,\"UPI\",\"Completed\"),\n",
        "(\"T027\",\"Tina\",\"Bangalore\",\"Suresh\",\"Sedan\",12.6,330,29,\"Card\",\"Completed\"),\n",
        "(\"T028\",\"Irfan\",\"Mumbai\",\"Vijay\",\"SUV\",23.4,740,58,\"Card\",\"Completed\"),\n",
        "(\"T029\",\"Sahil\",\"Chennai\",\"Kumar\",\"Mini\",9.4,235,24,\"UPI\",\"Completed\"),\n",
        "(\"T030\",\"Lavanya\",\"Delhi\",\"Anil\",\"Sedan\",14.1,365,32,\"Wallet\",\"Completed\"),\n",
        "(\"T031\",\"Deepak\",\"Hyderabad\",\"Ramesh\",\"Bike\",6.7,150,18,\"Cash\",\"Completed\"),\n",
        "(\"T032\",\"Shweta\",\"Bangalore\",\"Suresh\",\"Mini\",10.0,255,26,\"UPI\",\"Completed\"),\n",
        "(\"T033\",\"Aman\",\"Mumbai\",\"Vikas\",\"Sedan\",15.8,395,35,\"Card\",\"Completed\"),\n",
        "(\"T034\",\"Rekha\",\"Chennai\",\"Kumar\",\"Sedan\",13.5,345,30,\"UPI\",\"Completed\"),\n",
        "(\"T035\",\"Zubin\",\"Delhi\",\"Anil\",\"SUV\",24.0,760,60,\"Card\",\"Completed\"),\n",
        "(\"T036\",\"Pallavi\",\"Hyderabad\",\"Ramesh\",\"Mini\",9.1,230,23,\"Wallet\",\"Completed\"),\n",
        "(\"T037\",\"Naveen\",\"Bangalore\",\"Suresh\",\"Bike\",5.9,135,17,\"UPI\",\"Completed\"),\n",
        "(\"T038\",\"Sonia\",\"Mumbai\",\"Vijay\",\"SUV\",21.7,690,54,\"Card\",\"Completed\"),\n",
        "(\"T039\",\"Harish\",\"Chennai\",\"Kumar\",\"Mini\",8.5,215,21,\"Cash\",\"Completed\"),\n",
        "(\"T040\",\"Kriti\",\"Delhi\",\"Anil\",\"Sedan\",14.6,375,33,\"UPI\",\"Completed\"),\n",
        "(\"T041\",\"Apoorva\",\"Hyderabad\",\"Ramesh\",\"Sedan\",13.2,335,30,\"Card\",\"Completed\"),\n",
        "(\"T042\",\"Mohit\",\"Bangalore\",\"Suresh\",\"SUV\",19.9,620,49,\"UPI\",\"Completed\"),\n",
        "(\"T043\",\"Tanvi\",\"Mumbai\",\"Vikas\",\"Mini\",10.4,265,27,\"Wallet\",\"Completed\"),\n",
        "(\"T044\",\"Rakesh\",\"Chennai\",\"Kumar\",\"Bike\",6.2,140,18,\"Cash\",\"Completed\"),\n",
        "(\"T045\",\"Isha\",\"Delhi\",\"Anil\",\"Mini\",9.7,245,25,\"UPI\",\"Completed\")\n",
        "]\n",
        "columns = [\n",
        "\"trip_id\",\"rider_name\",\"city\",\"driver_name\",\"vehicle_type\",\n",
        "\"distance_km\",\"trip_fare\",\"trip_duration_minutes\",\n",
        "\"payment_mode\",\"trip_status\"\n",
        "]\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISES — MEDIUM LEVEL"
      ],
      "metadata": {
        "id": "2TK-MJsGN-tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSV, JSON, PARQUET (Ride-Sharing Use Case)"
      ],
      "metadata": {
        "id": "YUd05tcqOC7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION A — CSV"
      ],
      "metadata": {
        "id": "WnyRdX94OHLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1\n",
        "\n",
        "Write the full dataset to CSV with header enabled.\n",
        "Output:\n",
        "trips_csv/"
      ],
      "metadata": {
        "id": "R8W2ozMMOYNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession if not already initialized\n",
        "spark = SparkSession.builder.appName(\"RideSharingAnalytics\").getOrCreate()\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Write DataFrame to CSV with header enabled\n",
        "df.write.option(\"header\", \"true\").csv(\"trips_csv/\")\n",
        "\n",
        "print(\"Dataset successfully written to trips_csv/ with header enabled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_WkK2cb54ha",
        "outputId": "5fe042b0-8e54-4ac3-afe7-afdf312a2ee1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset successfully written to trips_csv/ with header enabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2\n",
        "\n",
        "Read the CSV and filter:\n",
        "trip_fare > 400\n",
        "trip_status = \"Completed\""
      ],
      "metadata": {
        "id": "gvxN3tpBREwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Read the CSV file with header enabled\n",
        "df_read = spark.read.option(\"header\", \"true\").csv(\"trips_csv/\")\n",
        "\n",
        "# Filter the DataFrame\n",
        "filtered_df = df_read.filter((col(\"trip_fare\") > 400) & (col(\"trip_status\") == \"Completed\"))\n",
        "\n",
        "# Show the filtered data\n",
        "filtered_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZdX9_otOyuk",
        "outputId": "0550c26f-fe21-4ea5-efbb-26e2a289f06a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+-----------+------------+-----------+---------+---------------------+------------+-----------+\n",
            "|trip_id|rider_name|     city|driver_name|vehicle_type|distance_km|trip_fare|trip_duration_minutes|payment_mode|trip_status|\n",
            "+-------+----------+---------+-----------+------------+-----------+---------+---------------------+------------+-----------+\n",
            "|   T028|     Irfan|   Mumbai|      Vijay|         SUV|       23.4|      740|                   58|        Card|  Completed|\n",
            "|   T035|     Zubin|    Delhi|       Anil|         SUV|       24.0|      760|                   60|        Card|  Completed|\n",
            "|   T038|     Sonia|   Mumbai|      Vijay|         SUV|       21.7|      690|                   54|        Card|  Completed|\n",
            "|   T042|     Mohit|Bangalore|     Suresh|         SUV|       19.9|      620|                   49|         UPI|  Completed|\n",
            "|   T009|     Vikas|   Mumbai|      Vijay|         SUV|       20.5|      650|                   50|        Card|  Completed|\n",
            "|   T017|     Rohit|Hyderabad|     Ramesh|         SUV|       22.0|      700|                   55|        Card|  Completed|\n",
            "|   T022|      Yash|  Chennai|      Kumar|         SUV|       21.3|      680|                   52|         UPI|  Completed|\n",
            "+-------+----------+---------+-----------+------------+-----------+---------+---------------------+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3\n",
        "\n",
        "From CSV, select:\n",
        "trip_id\n",
        "city\n",
        "vehicle_type\n",
        "trip_fare\n",
        "Sort by trip_fare descending."
      ],
      "metadata": {
        "id": "0r0_rtwbRYOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, desc\n",
        "\n",
        "selected_and_sorted_df = df_read.select(\"trip_id\", \"city\", \"vehicle_type\", col(\"trip_fare\").cast(\"double\")) \\\n",
        "                                  .orderBy(desc(\"trip_fare\"))\n",
        "\n",
        "\n",
        "selected_and_sorted_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtG8ldHURegR",
        "outputId": "7bc2ec08-a696-4761-9be8-d9a9ddd558f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+------------+---------+\n",
            "|trip_id|     city|vehicle_type|trip_fare|\n",
            "+-------+---------+------------+---------+\n",
            "|   T035|    Delhi|         SUV|    760.0|\n",
            "|   T028|   Mumbai|         SUV|    740.0|\n",
            "|   T017|Hyderabad|         SUV|    700.0|\n",
            "|   T038|   Mumbai|         SUV|    690.0|\n",
            "|   T022|  Chennai|         SUV|    680.0|\n",
            "|   T009|   Mumbai|         SUV|    650.0|\n",
            "|   T042|Bangalore|         SUV|    620.0|\n",
            "|   T012|Hyderabad|         SUV|    610.0|\n",
            "|   T004|   Mumbai|         SUV|    560.0|\n",
            "|   T033|   Mumbai|       Sedan|    395.0|\n",
            "|   T015|    Delhi|       Sedan|    390.0|\n",
            "|   T021|Hyderabad|       Sedan|    380.0|\n",
            "|   T040|    Delhi|       Sedan|    375.0|\n",
            "|   T030|    Delhi|       Sedan|    365.0|\n",
            "|   T006|Hyderabad|       Sedan|    360.0|\n",
            "|   T024|   Mumbai|       Sedan|    350.0|\n",
            "|   T034|  Chennai|       Sedan|    345.0|\n",
            "|   T013|Bangalore|       Sedan|    340.0|\n",
            "|   T041|Hyderabad|       Sedan|    335.0|\n",
            "|   T027|Bangalore|       Sedan|    330.0|\n",
            "+-------+---------+------------+---------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4\n",
        "\n",
        "Write only Bike trips to CSV using delimiter | ."
      ],
      "metadata": {
        "id": "Lv0Cwn42RqKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "bike_trips_df = df_read.filter(col(\"vehicle_type\") == \"Bike\")\n",
        "bike_trips_df.write.option(\"header\", \"true\").option(\"sep\", \"|\").csv(\"bike_trips_csv/\")\n",
        "print(\"Bike trips successfully written to bike_trips_csv/ with '|' delimiter.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FugtGa9FRv4M",
        "outputId": "35776a16-ef2a-4d9a-df2b-1ae245a43a8f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bike trips successfully written to bike_trips_csv/ with '|' delimiter.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION B — JSON"
      ],
      "metadata": {
        "id": "iTsood_PR7o-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5\n",
        "\n",
        "Write only trips from Mumbai to JSON.\n",
        "Output:\n",
        "mumbai_trips_json/"
      ],
      "metadata": {
        "id": "GVKpCieMR_1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "mumbai_trips_df = df_read.filter(col(\"city\") == \"Mumbai\")\n",
        "\n",
        "mumbai_trips_df.write.json(\"mumbai_trips_json/\")\n",
        "\n",
        "print(\"Mumbai trips successfully written to mumbai_trips_json/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEgE79FxR_Ve",
        "outputId": "8bc74f0e-7882-42f1-ca68-83c2138923d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mumbai trips successfully written to mumbai_trips_json/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5\n",
        "\n",
        "Write only trips from Mumbai to JSON.\n",
        "Output:\n",
        "mumbai_trips_json/"
      ],
      "metadata": {
        "id": "6JyNU7AwSYuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "mumbai_trips_df = df_read.filter(col(\"city\") == \"Mumbai\")\n",
        "\n",
        "mumbai_trips_df.write.mode(\"overwrite\").json(\"mumbai_trips_json/\")\n",
        "\n",
        "print(\"Mumbai trips successfully written to mumbai_trips_json/ (overwriting existing files if any).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-vbaDqmSc3B",
        "outputId": "939f50cf-fe4e-4e15-b980-cf5062734b02"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mumbai trips successfully written to mumbai_trips_json/ (overwriting existing files if any).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 7\n",
        "\n",
        "Filter JSON data:\n",
        "payment_mode = \"Card\"\n",
        "vehicle_type = \"SUV\""
      ],
      "metadata": {
        "id": "7RBdrvLSTDzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_json = spark.read.json(\"mumbai_trips_json/\")\n",
        "\n",
        "filtered_json_df = df_json.filter((col(\"payment_mode\") == \"Card\") & (col(\"vehicle_type\") == \"SUV\"))\n",
        "\n",
        "filtered_json_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT1N-gseTFgi",
        "outputId": "779bc138-830b-472d-a63f-af433d837de7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+-----------+------------+----------+---------------------+---------+-------+-----------+------------+\n",
            "|  city|distance_km|driver_name|payment_mode|rider_name|trip_duration_minutes|trip_fare|trip_id|trip_status|vehicle_type|\n",
            "+------+-----------+-----------+------------+----------+---------------------+---------+-------+-----------+------------+\n",
            "|Mumbai|       23.4|      Vijay|        Card|     Irfan|                   58|      740|   T028|  Completed|         SUV|\n",
            "|Mumbai|       21.7|      Vijay|        Card|     Sonia|                   54|      690|   T038|  Completed|         SUV|\n",
            "|Mumbai|       20.5|      Vijay|        Card|     Vikas|                   50|      650|   T009|  Completed|         SUV|\n",
            "+------+-----------+-----------+------------+----------+---------------------+---------+-------+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 8\n",
        "\n",
        "Force JSON output into a single partition and observe the output structure."
      ],
      "metadata": {
        "id": "9otUJMoBTTwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mumbai_trips_df.repartition(1).write.mode(\"overwrite\").json(\"mumbai_trips_json_single_partition/\")\n",
        "\n",
        "print(\"Mumbai trips successfully written to mumbai_trips_json_single_partition/ in a single partition.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIIIDV0FTXvC",
        "outputId": "63805470-1cd8-4773-affd-baf1e45bb8bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mumbai trips successfully written to mumbai_trips_json_single_partition/ in a single partition.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION C — PARQUET"
      ],
      "metadata": {
        "id": "_r0-EcufThjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 9\n",
        "\n",
        "Convert full dataset to Parquet.\n",
        "Output:\n",
        "trips_parquet/"
      ],
      "metadata": {
        "id": "EdBxWCcZTs7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.parquet(\"trips_parquet/\")\n",
        "\n",
        "print(\"Full dataset successfully written to trips_parquet/ in Parquet format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewLsgZ-ITpBC",
        "outputId": "116b8b6e-4cad-4904-c222-79df336cabed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full dataset successfully written to trips_parquet/ in Parquet format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 10\n",
        "\n",
        "Read Parquet and filter:\n",
        "trip_duration_minutes > 45"
      ],
      "metadata": {
        "id": "UdRIl5nET_T9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_parquet = spark.read.parquet(\"trips_parquet/\")\n",
        "\n",
        "filtered_parquet_df = df_parquet.filter(col(\"trip_duration_minutes\").cast(\"int\") > 45)\n",
        "\n",
        "filtered_parquet_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNCOK92GUBpJ",
        "outputId": "d6f01afa-7ed2-4c26-ea48-0ef59a9bd9e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+---------+-----------+------------+-----------+---------+---------------------+------------+-----------+\n",
            "|trip_id|rider_name|     city|driver_name|vehicle_type|distance_km|trip_fare|trip_duration_minutes|payment_mode|trip_status|\n",
            "+-------+----------+---------+-----------+------------+-----------+---------+---------------------+------------+-----------+\n",
            "|   T028|     Irfan|   Mumbai|      Vijay|         SUV|       23.4|      740|                   58|        Card|  Completed|\n",
            "|   T035|     Zubin|    Delhi|       Anil|         SUV|       24.0|      760|                   60|        Card|  Completed|\n",
            "|   T038|     Sonia|   Mumbai|      Vijay|         SUV|       21.7|      690|                   54|        Card|  Completed|\n",
            "|   T042|     Mohit|Bangalore|     Suresh|         SUV|       19.9|      620|                   49|         UPI|  Completed|\n",
            "|   T009|     Vikas|   Mumbai|      Vijay|         SUV|       20.5|      650|                   50|        Card|  Completed|\n",
            "|   T012|     Megha|Hyderabad|     Ramesh|         SUV|       19.2|      610|                   48|        Card|  Cancelled|\n",
            "|   T017|     Rohit|Hyderabad|     Ramesh|         SUV|       22.0|      700|                   55|        Card|  Completed|\n",
            "|   T022|      Yash|  Chennai|      Kumar|         SUV|       21.3|      680|                   52|         UPI|  Completed|\n",
            "+-------+----------+---------+-----------+------------+-----------+---------+---------------------+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 11\n",
        "\n",
        "Sort Parquet data by distance_km descending and write top 10 trips back to Parquet."
      ],
      "metadata": {
        "id": "C840apG3UREl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "top_10_trips_df = df_parquet.orderBy(desc(\"distance_km\")).limit(10)\n",
        "\n",
        "top_10_trips_df.write.mode(\"overwrite\").parquet(\"top_10_trips_parquet/\")\n",
        "\n",
        "print(\"Top 10 trips by distance_km successfully written to top_10_trips_parquet/.\")\n",
        "\n",
        "top_10_trips_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-Ne_73mUTiO",
        "outputId": "ab98b127-3df8-42b7-e08f-417c1712e787"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 trips by distance_km successfully written to top_10_trips_parquet/.\n",
            "+-------+----------+---------+-----------+------------+-----------+---------+---------------------+------------+-----------+\n",
            "|trip_id|rider_name|     city|driver_name|vehicle_type|distance_km|trip_fare|trip_duration_minutes|payment_mode|trip_status|\n",
            "+-------+----------+---------+-----------+------------+-----------+---------+---------------------+------------+-----------+\n",
            "|   T035|     Zubin|    Delhi|       Anil|         SUV|       24.0|      760|                   60|        Card|  Completed|\n",
            "|   T028|     Irfan|   Mumbai|      Vijay|         SUV|       23.4|      740|                   58|        Card|  Completed|\n",
            "|   T017|     Rohit|Hyderabad|     Ramesh|         SUV|       22.0|      700|                   55|        Card|  Completed|\n",
            "|   T038|     Sonia|   Mumbai|      Vijay|         SUV|       21.7|      690|                   54|        Card|  Completed|\n",
            "|   T022|      Yash|  Chennai|      Kumar|         SUV|       21.3|      680|                   52|         UPI|  Completed|\n",
            "|   T009|     Vikas|   Mumbai|      Vijay|         SUV|       20.5|      650|                   50|        Card|  Completed|\n",
            "|   T042|     Mohit|Bangalore|     Suresh|         SUV|       19.9|      620|                   49|         UPI|  Completed|\n",
            "|   T012|     Megha|Hyderabad|     Ramesh|         SUV|       19.2|      610|                   48|        Card|  Cancelled|\n",
            "|   T004|     Pooja|   Mumbai|      Vikas|         SUV|       18.0|      560|                   45|         UPI|  Cancelled|\n",
            "|   T033|      Aman|   Mumbai|      Vikas|       Sedan|       15.8|      395|                   35|        Card|  Completed|\n",
            "+-------+----------+---------+-----------+------------+-----------+---------+---------------------+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 12\n",
        "\n",
        "Compare storage size of:\n",
        "CSV\n",
        "JSON\n",
        "Parquet\n",
        "Answer which is smallest and why."
      ],
      "metadata": {
        "id": "uySFMTABUdoQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c6ef1d7",
        "outputId": "f28cf102-262a-4da0-dd63-1590f17ed89c"
      },
      "source": [
        "import os\n",
        "\n",
        "csv_dir = \"trips_csv/\"\n",
        "csv_size = 0\n",
        "\n",
        "if os.path.exists(csv_dir) and os.path.isdir(csv_dir):\n",
        "    for filename in os.listdir(csv_dir):\n",
        "        filepath = os.path.join(csv_dir, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            csv_size += os.path.getsize(filepath)\n",
        "    print(f\"Total size of files in {csv_dir}: {csv_size} bytes\")\n",
        "else:\n",
        "    print(f\"Directory {csv_dir} does not exist or is not a directory.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of files in trips_csv/: 2910 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "165ab086",
        "outputId": "9f1cffb9-c1f3-4f16-bb87-780270335a6f"
      },
      "source": [
        "import os\n",
        "\n",
        "json_dir = \"mumbai_trips_json_single_partition/\"\n",
        "json_size = 0\n",
        "\n",
        "if os.path.exists(json_dir) and os.path.isdir(json_dir):\n",
        "    for filename in os.listdir(json_dir):\n",
        "        filepath = os.path.join(json_dir, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            json_size += os.path.getsize(filepath)\n",
        "    print(f\"Total size of files in {json_dir}: {json_size} bytes\")\n",
        "else:\n",
        "    print(f\"Directory {json_dir} does not exist or is not a directory.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of files in mumbai_trips_json_single_partition/: 1978 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59a18e51",
        "outputId": "f3aeadbc-57e9-41f5-8755-a566492b8a8b"
      },
      "source": [
        "import os\n",
        "\n",
        "parquet_dir = \"trips_parquet/\"\n",
        "parquet_size = 0\n",
        "\n",
        "if os.path.exists(parquet_dir) and os.path.isdir(parquet_dir):\n",
        "    for filename in os.listdir(parquet_dir):\n",
        "        filepath = os.path.join(parquet_dir, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            parquet_size += os.path.getsize(filepath)\n",
        "    print(f\"Total size of files in {parquet_dir}: {parquet_size} bytes\")\n",
        "else:\n",
        "    print(f\"Directory {parquet_dir} does not exist or is not a directory.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of files in trips_parquet/: 8025 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SECTION D — FORMAT CONVERSION"
      ],
      "metadata": {
        "id": "ItavOf6CVqvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 13\n",
        "\n",
        "Convert:\n",
        "CSV → Parquet\n",
        "JSON → Parquet"
      ],
      "metadata": {
        "id": "livaluPjWLuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Converting CSV to Parquet...\")\n",
        "df_read.write.mode(\"overwrite\").parquet(\"csv_to_parquet/\")\n",
        "print(\"CSV data successfully converted to Parquet in 'csv_to_parquet/'.\")\n",
        "\n",
        "print(\"Converting JSON to Parquet...\")\n",
        "df_json.write.mode(\"overwrite\").parquet(\"json_to_parquet/\")\n",
        "print(\"JSON data successfully converted to Parquet in 'json_to_parquet/'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RoMc88vWINy",
        "outputId": "64a73626-76f9-450c-d0d4-617931ec9c4c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting CSV to Parquet...\n",
            "CSV data successfully converted to Parquet in 'csv_to_parquet/'.\n",
            "Converting JSON to Parquet...\n",
            "JSON data successfully converted to Parquet in 'json_to_parquet/'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 14\n",
        "\n",
        "Read Parquet and write it back as CSV with header and delimiter ,"
      ],
      "metadata": {
        "id": "4ORW7TQsWhVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_parquet.write.mode(\"overwrite\").option(\"header\", \"true\").option(\"sep\", \",\").csv(\"parquet_to_csv/\")\n",
        "\n",
        "print(\"Parquet data successfully written back to CSV in 'parquet_to_csv/'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nht6w-08WlE-",
        "outputId": "e6871ea3-16c8-4a46-8f55-a86ca6b88cd3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parquet data successfully written back to CSV in 'parquet_to_csv/'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALYTICS THINKING QUESTIONS"
      ],
      "metadata": {
        "id": "B9JT5rxIWxvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 15\n",
        "\n",
        "Which city generates the highest total trip_fare?"
      ],
      "metadata": {
        "id": "cDwRIC-XXDwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "total_fare_by_city = df_read.groupBy(\"city\")\\\n",
        "                             .agg(sum(col(\"trip_fare\").cast(\"int\")).alias(\"total_trip_fare\"))\n",
        "\n",
        "total_fare_by_city.orderBy(col(\"total_trip_fare\").desc()).show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sh9gAjcW1S-",
        "outputId": "fdea0846-5569-400f-eff0-d095d79614ef"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+\n",
            "|  city|total_trip_fare|\n",
            "+------+---------------+\n",
            "|Mumbai|           4160|\n",
            "+------+---------------+\n",
            "only showing top 1 row\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 16\n",
        "\n",
        "Which vehicle_type has the highest average fare?"
      ],
      "metadata": {
        "id": "82XtzOlFXg95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, avg\n",
        "\n",
        "average_fare_by_vehicle = df_read.groupBy(\"vehicle_type\")\\\n",
        "                                    .agg(avg(col(\"trip_fare\").cast(\"double\")).alias(\"average_trip_fare\"))\n",
        "\n",
        "average_fare_by_vehicle.orderBy(col(\"average_trip_fare\").desc()).show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLh2H-ZrXkx_",
        "outputId": "bb33c584-0117-41f1-d389-36a81759dd99"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------------+\n",
            "|vehicle_type|average_trip_fare|\n",
            "+------------+-----------------+\n",
            "|         SUV|667.7777777777778|\n",
            "+------------+-----------------+\n",
            "only showing top 1 row\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 17\n",
        "\n",
        "Which driver has completed the most trips?"
      ],
      "metadata": {
        "id": "_Hc2uSpfXvR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count\n",
        "\n",
        "completed_trips_by_driver = df_read.filter(col(\"trip_status\") == \"Completed\")\\\n",
        "                                     .groupBy(\"driver_name\")\\\n",
        "                                     .agg(count(\"trip_id\").alias(\"completed_trips_count\"))\n",
        "\n",
        "completed_trips_by_driver.orderBy(col(\"completed_trips_count\").desc()).show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izhPKwADX37J",
        "outputId": "1f7894a9-82a6-4683-9268-39f2575e45cd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------------+\n",
            "|driver_name|completed_trips_count|\n",
            "+-----------+---------------------+\n",
            "|       Anil|                   11|\n",
            "+-----------+---------------------+\n",
            "only showing top 1 row\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 18\n",
        "\n",
        "Why is Parquet preferred for analytics dashboards and aggregations?"
      ],
      "metadata": {
        "id": "Iyu4G6G5YIp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parquet is preferred for analytics dashboards and aggregations for several key reasons, primarily stemming from its columnar storage format and optimized design for big data processing:\n",
        "\n",
        "Columnar Storage: Unlike row-oriented formats (like CSV or JSON), Parquet stores data column by column. This means that all values for a specific column are stored together. For analytical queries that often select only a subset of columns (e.g., SUM(trip_fare)), this is highly efficient as only the required columns need to be read from disk, significantly reducing I/O operations.\n",
        "\n",
        "Efficient Compression and Encoding:\n",
        "\n",
        "Better Compression: Columnar storage allows for more effective compression because data within a single column is typically of the same data type and often has similar values. This homogeneity leads to higher compression ratios, resulting in smaller file sizes on disk.\n",
        "Various Encoding Schemes: Parquet supports different encoding schemes (e.g., Run Length Encoding, Dictionary Encoding) that are chosen based on the data type and distribution within each column, further optimizing storage and retrieval.\n",
        "Predicate Pushdown (Filter Pushdown): Parquet files store metadata about each column, including statistics like min/max values. Query engines (like Spark) can use this metadata to skip reading entire blocks or files that do not contain data relevant to a query's filters. This significantly speeds up query execution, especially for large datasets.\n",
        "\n",
        "Schema Evolution: Parquet supports schema evolution, allowing users to add new columns or modify existing ones without rewriting the entire dataset. This flexibility is crucial in dynamic data environments.\n",
        "\n",
        "Optimized for Analytical Workloads: Aggregations (like SUM, AVG, COUNT) and scans are inherently faster on columnar data. When performing an aggregation on a column, the engine can quickly access all values for that column without having to read through irrelevant data from other columns.\n",
        "\n",
        "Binary Format: While not human-readable (unlike CSV or JSON), the binary nature of Parquet makes it highly efficient for machine processing and serialization/deserialization.\n",
        "\n",
        "Interoperability: Parquet is widely supported across various data processing frameworks and tools (e.g., Apache Spark, Apache Hive, Apache Impala), making it a versatile choice for a diverse ecosystem.\n",
        "\n",
        "In summary, Parquet's columnar nature, combined with advanced compression, encoding, and optimization techniques, makes it ideal for handling large-scale analytical workloads, enabling faster query performance and reduced storage costs compared to row-oriented or text-based formats.\n",
        "\n"
      ],
      "metadata": {
        "id": "JZO2PF-7Yczd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTIONAL CHALLENGE"
      ],
      "metadata": {
        "id": "I6Swf_QFY-Rf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge 1\n",
        "Repartition the dataset into 4 partitions and write to Parquet."
      ],
      "metadata": {
        "id": "JK-xR9qvZDMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.repartition(4).write.mode(\"overwrite\").parquet(\"repartitioned_trips_parquet/\")\n",
        "\n",
        "print(\"Dataset successfully repartitioned into 4 partitions and written to 'repartitioned_trips_parquet/'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5il5u9a5YKjE",
        "outputId": "8e75f06a-2c68-460f-ca89-edbf8b76b415"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset successfully repartitioned into 4 partitions and written to 'repartitioned_trips_parquet/'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge 2\n",
        "Create a summary dataset with:\n",
        "city\n",
        "total_trips\n",
        "total_revenue\n",
        "\n",
        "average_trip_duration\n",
        "Write it to Parquet."
      ],
      "metadata": {
        "id": "0oKwDIhLZRsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, sum, avg\n",
        "\n",
        "summary_df = df_read.groupBy(\"city\").agg(\n",
        "    count(\"trip_id\").alias(\"total_trips\"),\n",
        "    sum(col(\"trip_fare\").cast(\"double\")).alias(\"total_revenue\"),\n",
        "    avg(col(\"trip_duration_minutes\").cast(\"double\")).alias(\"average_trip_duration\")\n",
        ")\n",
        "\n",
        "summary_df.write.mode(\"overwrite\").parquet(\"summary_trips_parquet/\")\n",
        "\n",
        "print(\"Summary dataset successfully created and written to 'summary_trips_parquet/'.\")\n",
        "\n",
        "summary_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3Gqkz3bZTcN",
        "outputId": "443e0f49-5aa2-49e5-dbc0-0eb9f4e19ec4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary dataset successfully created and written to 'summary_trips_parquet/'.\n",
            "+---------+-----------+-------------+---------------------+\n",
            "|     city|total_trips|total_revenue|average_trip_duration|\n",
            "+---------+-----------+-------------+---------------------+\n",
            "|Bangalore|          9|       2590.0|    27.11111111111111|\n",
            "|  Chennai|          8|       2240.0|                 26.0|\n",
            "|   Mumbai|          9|       4160.0|   38.888888888888886|\n",
            "|    Delhi|         11|       3035.0|   26.636363636363637|\n",
            "|Hyderabad|          9|       3310.0|    32.22222222222222|\n",
            "+---------+-----------+-------------+---------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}