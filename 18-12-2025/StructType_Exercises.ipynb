{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PYSPARK SCHEMA RECOVERY"
      ],
      "metadata": {
        "id": "QBc2iEeD3zxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StructType • StructField • Complex Types • Data Fixing"
      ],
      "metadata": {
        "id": "JBK8xtB833t6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 1 — USER PROFILE API (CORRUPTED TYPES)"
      ],
      "metadata": {
        "id": "1Dhuzgnw3-ju"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw data (as received from API)"
      ],
      "metadata": {
        "id": "YK1gK2B-4Dbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "spark=SparkSession.builder.appName('Struct Type').getOrCreate()"
      ],
      "metadata": {
        "id": "OyNp76vz4Inq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N1nt2WSH3xoj"
      },
      "outputs": [],
      "source": [
        "raw_users = [\n",
        "(\"U001\",\"Amit\",\"29\",\"Hyderabad\",\"50000\"),\n",
        "(\"U002\",\"Neha\",\"Thirty Two\",\"Delhi\",\"62000\"),\n",
        "(\"U003\",\"Ravi\",None,\"Bangalore\",\"45k\"),\n",
        "(\"U004\",\"Pooja\",\"28\",\"Mumbai\",58000),\n",
        "(\"U005\",None,\"31\",\"Chennai\",\"\")\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises"
      ],
      "metadata": {
        "id": "Rg47P00X4VOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Design a StructType schema for this data"
      ],
      "metadata": {
        "id": "3VPhT0Em4YuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import(\n",
        "    StructType,\n",
        "    StructField,\n",
        "    StringType,\n",
        "    IntegerType,\n",
        "    LongType\n",
        ")"
      ],
      "metadata": {
        "id": "U4RTWust4PpG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_schema=StructType([\n",
        "    StructField('user_id',StringType(),nullable=False),\n",
        "    StructField('name',StringType(),nullable=True),\n",
        "    StructField('age',StringType(),nullable=True),\n",
        "    StructField('city',StringType(),nullable=True),\n",
        "    StructField('salary',StringType(),nullable=True)\n",
        "])"
      ],
      "metadata": {
        "id": "TrfiOng74cbL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load the data using the schema"
      ],
      "metadata": {
        "id": "tA67ATsU4pxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_users=spark.createDataFrame(data=raw_users,schema=user_schema)\n"
      ],
      "metadata": {
        "id": "_wWCaXo04lM_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_users.show()\n",
        "df_users.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwzPKM_j5edn",
        "outputId": "b3b69405-d8fb-403d-8cc9-c7306881d210"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+----------+---------+------+\n",
            "|user_id| name|       age|     city|salary|\n",
            "+-------+-----+----------+---------+------+\n",
            "|   U001| Amit|        29|Hyderabad| 50000|\n",
            "|   U002| Neha|Thirty Two|    Delhi| 62000|\n",
            "|   U003| Ravi|      NULL|Bangalore|   45k|\n",
            "|   U004|Pooja|        28|   Mumbai| 58000|\n",
            "|   U005| NULL|        31|  Chennai|      |\n",
            "+-------+-----+----------+---------+------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Identify records that fail type conversion"
      ],
      "metadata": {
        "id": "Yfk5HyIt5oCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_cast = df_users.withColumn(\n",
        "    \"age_int\",\n",
        "    when(col(\"age\").rlike(r\"^[0-9]+$\"), col(\"age\").cast(\"int\")).otherwise(None)\n",
        ").withColumn(\n",
        "    \"salary_str\", col(\"salary\").cast(\"string\")\n",
        ").withColumn(\n",
        "    \"salary_int\",\n",
        "    when(col(\"salary_str\").rlike(r\"^[0-9]+$\"), col(\"salary_str\").cast(\"int\")).otherwise(None)\n",
        ")\n",
        "\n",
        "df_fail = df_cast.filter(\n",
        "    (col(\"age\").isNotNull() & col(\"age_int\").isNull()) |\n",
        "    (col(\"salary\").isNotNull() & col(\"salary_int\").isNull())\n",
        ").select(\"user_id\", \"name\", \"age\", \"city\", \"salary\")\n",
        "\n",
        "df_fail.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q86FZVZx8Kva",
        "outputId": "0cdae4da-d1dd-4598-b09f-864522a3c4b5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+----------+---------+------+\n",
            "|user_id|name|age       |city     |salary|\n",
            "+-------+----+----------+---------+------+\n",
            "|U002   |Neha|Thirty Two|Delhi    |62000 |\n",
            "|U003   |Ravi|NULL      |Bangalore|45k   |\n",
            "|U005   |NULL|31        |Chennai  |      |\n",
            "+-------+----+----------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Convert age to integer safely"
      ],
      "metadata": {
        "id": "yzqnr-ol6m20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, trim\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_users_clean = df_users.withColumn(\n",
        "    \"age_clean\", trim(col(\"age\"))\n",
        ").withColumn(\n",
        "    \"age_int\",\n",
        "    when(\n",
        "        (col(\"age_clean\").isNotNull()) & (col(\"age_clean\") != '') & col(\"age_clean\").rlike(\"^[0-9]+$\"),\n",
        "        col(\"age_clean\").cast(IntegerType())\n",
        "    ).otherwise(None)\n",
        ").drop(\"age_clean\")\n",
        "\n",
        "print(\"DataFrame with safe integer age column:\")\n",
        "df_users_clean.select(\"user_id\", \"name\", \"age\", \"age_int\").show(truncate=False)\n",
        "df_users_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voFqLYPW-0_N",
        "outputId": "e9613a76-9c15-47a7-8992-f4b41da98b80"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with safe integer age column:\n",
            "+-------+-----+----------+-------+\n",
            "|user_id|name |age       |age_int|\n",
            "+-------+-----+----------+-------+\n",
            "|U001   |Amit |29        |29     |\n",
            "|U002   |Neha |Thirty Two|NULL   |\n",
            "|U003   |Ravi |NULL      |NULL   |\n",
            "|U004   |Pooja|28        |28     |\n",
            "|U005   |NULL |31        |31     |\n",
            "+-------+-----+----------+-------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: string (nullable = true)\n",
            " |-- age_int: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Normalize salary into integer (handle k )"
      ],
      "metadata": {
        "id": "e7xle_Z6BOWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, regexp_replace\n",
        "from pyspark.sql.types import LongType\n",
        "\n",
        "df_users_with_salary_int = df_users_clean.withColumn(\n",
        "    \"salary_int\",\n",
        "    when(\n",
        "        (col(\"salary\").isNotNull()) & (col(\"salary\") != '') & col(\"salary\").rlike(\"^[0-9]+$\"),\n",
        "        col(\"salary\").cast(LongType())\n",
        "    ).when(\n",
        "        (col(\"salary\").isNotNull()) & (col(\"salary\") != '') & col(\"salary\").rlike(\"^[0-9]+[kK]$\"),\n",
        "        regexp_replace(col(\"salary\"), \"[kK]\", \"000\").cast(LongType())\n",
        "    ).otherwise(None)\n",
        ")\n",
        "\n",
        "print(\"DataFrame with normalized salary column:\")\n",
        "df_users_with_salary_int.select(\"user_id\", \"salary\", \"salary_int\").show(truncate=False)\n",
        "df_users_with_salary_int.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6w4uzlQ_WMS",
        "outputId": "882efb07-8024-47b1-97d5-3076dd7f1a30"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with normalized salary column:\n",
            "+-------+------+----------+\n",
            "|user_id|salary|salary_int|\n",
            "+-------+------+----------+\n",
            "|U001   |50000 |50000     |\n",
            "|U002   |62000 |62000     |\n",
            "|U003   |45k   |45000     |\n",
            "|U004   |58000 |58000     |\n",
            "|U005   |      |NULL      |\n",
            "+-------+------+----------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: string (nullable = true)\n",
            " |-- age_int: integer (nullable = true)\n",
            " |-- salary_int: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Replace missing names with \"UNKNOWN\""
      ],
      "metadata": {
        "id": "QetSmKOgBn_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_users_with_unknown_names = df_users_with_salary_int.fillna(\"UNKNOWN\", subset=[\"name\"])\n",
        "\n",
        "print(\"DataFrame with missing names replaced by 'UNKNOWN':\")\n",
        "df_users_with_unknown_names.select(\"user_id\", \"name\", \"age_int\", \"salary_int\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWywf5UdBaF8",
        "outputId": "cfc38c32-273d-4816-90d5-dc2d8c1ab531"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with missing names replaced by 'UNKNOWN':\n",
            "+-------+-------+-------+----------+\n",
            "|user_id|name   |age_int|salary_int|\n",
            "+-------+-------+-------+----------+\n",
            "|U001   |Amit   |29     |50000     |\n",
            "|U002   |Neha   |NULL   |62000     |\n",
            "|U003   |Ravi   |NULL   |45000     |\n",
            "|U004   |Pooja  |28     |58000     |\n",
            "|U005   |UNKNOWN|31     |NULL      |\n",
            "+-------+-------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Drop records where age cannot be recovered"
      ],
      "metadata": {
        "id": "7MJp9RCZB9rZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_users_final = df_users_with_unknown_names.filter(col(\"age_int\").isNotNull())\n",
        "\n",
        "print(\"DataFrame after dropping records with unrecoverable age:\")\n",
        "df_users_final.select(\"user_id\", \"name\", \"age\", \"age_int\", \"salary\", \"salary_int\").show(truncate=False)\n",
        "df_users_final.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTBNtT1DBzZV",
        "outputId": "018d96ea-e748-4a15-8386-35de7d286efd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame after dropping records with unrecoverable age:\n",
            "+-------+-------+---+-------+------+----------+\n",
            "|user_id|name   |age|age_int|salary|salary_int|\n",
            "+-------+-------+---+-------+------+----------+\n",
            "|U001   |Amit   |29 |29     |50000 |50000     |\n",
            "|U004   |Pooja  |28 |28     |58000 |58000     |\n",
            "|U005   |UNKNOWN|31 |31     |      |NULL      |\n",
            "+-------+-------+---+-------+------+----------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- name: string (nullable = false)\n",
            " |-- age: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: string (nullable = true)\n",
            " |-- age_int: integer (nullable = true)\n",
            " |-- salary_int: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Produce a final clean DataFrame"
      ],
      "metadata": {
        "id": "Qup_HEzDCQPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_clean = df_users_final.select(\"user_id\", \"name\", \"age_int\", \"city\", \"salary_int\")\n",
        "\n",
        "print(\"Final Clean DataFrame:\")\n",
        "df_final_clean.show(truncate=False)\n",
        "df_final_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWAqQCnMCHF5",
        "outputId": "00a3eb54-410d-492c-a358-9c5ac830d55e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Clean DataFrame:\n",
            "+-------+-------+-------+---------+----------+\n",
            "|user_id|name   |age_int|city     |salary_int|\n",
            "+-------+-------+-------+---------+----------+\n",
            "|U001   |Amit   |29     |Hyderabad|50000     |\n",
            "|U004   |Pooja  |28     |Mumbai   |58000     |\n",
            "|U005   |UNKNOWN|31     |Chennai  |NULL      |\n",
            "+-------+-------+-------+---------+----------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- name: string (nullable = false)\n",
            " |-- age_int: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary_int: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 2 — E-COMMERCE ORDERS (ARRAY CORRUPTION)"
      ],
      "metadata": {
        "id": "o4b0T1kECbyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_orders = [\n",
        "(\"O001\",\"U001\",\"Laptop,Mobile,Tablet\",75000),\n",
        "(\"O002\",\"U002\",[\"Mobile\",\"Tablet\"],32000),\n",
        "(\"O003\",\"U003\",\"Laptop\",72000),\n",
        "(\"O004\",\"U004\",None,25000),\n",
        "(\"O005\",\"U005\",\"Laptop|Mobile\",68000)\n",
        "]"
      ],
      "metadata": {
        "id": "tJ3a1HHICYq9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import ArrayType"
      ],
      "metadata": {
        "id": "eUuyX27VCigo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Define a schema with ArrayType"
      ],
      "metadata": {
        "id": "vjRhQQ80DBOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import (\n",
        "    StructType,\n",
        "    StructField,\n",
        "    StringType,\n",
        "    IntegerType,\n",
        "    ArrayType\n",
        ")\n",
        "\n",
        "order_schema = StructType([\n",
        "    StructField('order_id', StringType(), False),\n",
        "    StructField('user_id', StringType(), True),\n",
        "    StructField('items', ArrayType(StringType()), True),\n",
        "    StructField('amount', IntegerType(), True)\n",
        "])\n",
        "\n",
        "print(\"Order Schema Defined:\")\n",
        "print(order_schema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8wCrvllC9Kd",
        "outputId": "8692374c-fa7a-4c56-ee8d-3df48154916f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Order Schema Defined:\n",
            "StructType([StructField('order_id', StringType(), False), StructField('user_id', StringType(), True), StructField('items', ArrayType(StringType(), True), True), StructField('amount', IntegerType(), True)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Normalize all item values into arrays"
      ],
      "metadata": {
        "id": "tuvgzl5uFgy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, split, array, lit, trim\n",
        "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, IntegerType\n",
        "\n",
        "processed_raw_orders = []\n",
        "for order in raw_orders:\n",
        "    order_id, user_id, items_data, amount = order\n",
        "    if isinstance(items_data, list):\n",
        "        items_data_str = \",\".join(items_data)\n",
        "    elif items_data is None:\n",
        "        items_data_str = None\n",
        "    else:\n",
        "        items_data_str = str(items_data)\n",
        "    processed_raw_orders.append((order_id, user_id, items_data_str, amount))\n",
        "intermediate_order_schema = StructType([\n",
        "    StructField('order_id', StringType(), False),\n",
        "    StructField('user_id', StringType(), True),\n",
        "    StructField('items_str', StringType(), True),\n",
        "    StructField('amount', IntegerType(), True)\n",
        "])\n",
        "\n",
        "df_orders_raw = spark.createDataFrame(data=processed_raw_orders, schema=intermediate_order_schema)\n",
        "\n",
        "df_orders_clean = df_orders_raw.withColumn(\n",
        "    \"items\",\n",
        "    when(col(\"items_str\").isNull(), array().cast(ArrayType(StringType())))\n",
        "    .when(trim(col(\"items_str\")) == lit(\"\"), array().cast(ArrayType(StringType())))\n",
        "    .when(col(\"items_str\").contains(\",\"), split(col(\"items_str\"), \",\"))\n",
        "    .when(col(\"items_str\").contains(\"|\"), split(col(\"items_str\"), \"\\\\|\"))\n",
        "    .otherwise(array(col(\"items_str\")))\n",
        ").drop(\"items_str\")\n",
        "\n",
        "print(\"Normalized Orders DataFrame:\")\n",
        "df_orders_clean.show(truncate=False)\n",
        "df_orders_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33oogM5CEn7T",
        "outputId": "eedcd2d3-f98d-44e2-c3d5-9db140401965"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Orders DataFrame:\n",
            "+--------+-------+------+------------------------+\n",
            "|order_id|user_id|amount|items                   |\n",
            "+--------+-------+------+------------------------+\n",
            "|O001    |U001   |75000 |[Laptop, Mobile, Tablet]|\n",
            "|O002    |U002   |32000 |[Mobile, Tablet]        |\n",
            "|O003    |U003   |72000 |[Laptop]                |\n",
            "|O004    |U004   |25000 |[]                      |\n",
            "|O005    |U005   |68000 |[Laptop, Mobile]        |\n",
            "+--------+-------+------+------------------------+\n",
            "\n",
            "root\n",
            " |-- order_id: string (nullable = false)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- items: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Handle multiple delimiters"
      ],
      "metadata": {
        "id": "FGAl8iluGsIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_orders_clean = df_orders_raw.withColumn(\n",
        "    \"items\",\n",
        "    when(col(\"items_str\").isNull(), array().cast(ArrayType(StringType())))\n",
        "    .when(trim(col(\"items_str\")) == lit(\"\"), array().cast(ArrayType(StringType())))\n",
        "    .when(col(\"items_str\").contains(\",\"), split(col(\"items_str\"), \",\"))\n",
        "    .when(col(\"items_str\").contains(\"|\"), split(col(\"items_str\"), \"\\\\|\"))\n",
        "    .otherwise(array(col(\"items_str\")))\n",
        ").drop(\"items_str\")\n",
        "\n",
        "print(\"Normalized Orders DataFrame:\")\n",
        "df_orders_clean.show(truncate=False)\n",
        "df_orders_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz6xkXhgGVwm",
        "outputId": "171732cb-c275-492c-e183-a7a9bbcb3515"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Orders DataFrame:\n",
            "+--------+-------+------+------------------------+\n",
            "|order_id|user_id|amount|items                   |\n",
            "+--------+-------+------+------------------------+\n",
            "|O001    |U001   |75000 |[Laptop, Mobile, Tablet]|\n",
            "|O002    |U002   |32000 |[Mobile, Tablet]        |\n",
            "|O003    |U003   |72000 |[Laptop]                |\n",
            "|O004    |U004   |25000 |[]                      |\n",
            "|O005    |U005   |68000 |[Laptop, Mobile]        |\n",
            "+--------+-------+------+------------------------+\n",
            "\n",
            "root\n",
            " |-- order_id: string (nullable = false)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- items: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Replace null items with empty arrays"
      ],
      "metadata": {
        "id": "mU32wsXVJHE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"df_orders_clean with null items already replaced by empty arrays:\")\n",
        "df_orders_clean.show(truncate=False)\n",
        "df_orders_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6jFylF8G7Rt",
        "outputId": "dbcbbb04-8c9f-43f3-bea7-959beb63411d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_orders_clean with null items already replaced by empty arrays:\n",
            "+--------+-------+------+------------------------+\n",
            "|order_id|user_id|amount|items                   |\n",
            "+--------+-------+------+------------------------+\n",
            "|O001    |U001   |75000 |[Laptop, Mobile, Tablet]|\n",
            "|O002    |U002   |32000 |[Mobile, Tablet]        |\n",
            "|O003    |U003   |72000 |[Laptop]                |\n",
            "|O004    |U004   |25000 |[]                      |\n",
            "|O005    |U005   |68000 |[Laptop, Mobile]        |\n",
            "+--------+-------+------+------------------------+\n",
            "\n",
            "root\n",
            " |-- order_id: string (nullable = false)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- items: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Explode items into one row per item"
      ],
      "metadata": {
        "id": "pNbu9MAAKFOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "\n",
        "df_orders_exploded = df_orders_clean.withColumn(\"item\", explode(col(\"items\")))\n",
        "\n",
        "print(\"Orders DataFrame after exploding 'items' column:\")\n",
        "df_orders_exploded.select(\"order_id\", \"user_id\", \"item\", \"amount\").show(truncate=False)\n",
        "df_orders_exploded.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGJmOGsXJ5JU",
        "outputId": "27ec8ff3-fb87-4dc7-d8ee-da68017c936e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orders DataFrame after exploding 'items' column:\n",
            "+--------+-------+------+------+\n",
            "|order_id|user_id|item  |amount|\n",
            "+--------+-------+------+------+\n",
            "|O001    |U001   |Laptop|75000 |\n",
            "|O001    |U001   |Mobile|75000 |\n",
            "|O001    |U001   |Tablet|75000 |\n",
            "|O002    |U002   |Mobile|32000 |\n",
            "|O002    |U002   |Tablet|32000 |\n",
            "|O003    |U003   |Laptop|72000 |\n",
            "|O005    |U005   |Laptop|68000 |\n",
            "|O005    |U005   |Mobile|68000 |\n",
            "+--------+-------+------+------+\n",
            "\n",
            "root\n",
            " |-- order_id: string (nullable = false)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- items: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- item: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Count frequency of each item"
      ],
      "metadata": {
        "id": "8E--HBBMKiXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_orders_frequency=df_orders_clean.withColumn(\"item\",explode(col(\"items\"))).groupBy(\"item\").count()\n",
        "print(\"Frequency of Each Item:\")\n",
        "df_orders_frequency.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCjzE0KtKWh2",
        "outputId": "6a428097-ebf4-4938-cbaa-5aec819f133c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency of Each Item:\n",
            "+------+-----+\n",
            "|  item|count|\n",
            "+------+-----+\n",
            "|Laptop|    3|\n",
            "|Mobile|    3|\n",
            "|Tablet|    2|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Identify orders with more than 2 items"
      ],
      "metadata": {
        "id": "VyHkAsXhLht1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, size\n",
        "\n",
        "df_orders_more_than_2_items = df_orders_clean.filter(size(col(\"items\")) > 2)\n",
        "\n",
        "print(\"Orders with more than 2 items:\")\n",
        "df_orders_more_than_2_items.select(\"order_id\", \"items\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui9sjAfGK1Ne",
        "outputId": "e9e26dce-7aab-487a-db0f-441d700ce54d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orders with more than 2 items:\n",
            "+--------+------------------------+\n",
            "|order_id|items                   |\n",
            "+--------+------------------------+\n",
            "|O001    |[Laptop, Mobile, Tablet]|\n",
            "+--------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 3 — DEVICE USAGE (MAP CORRUPTION)"
      ],
      "metadata": {
        "id": "_BtVmUEhLylx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_devices = [\n",
        "(\"U001\",{\"mobile\":120,\"laptop\":300}),\n",
        "(\"U002\",\"mobile:200,tablet:100\"),\n",
        "(\"U003\",{\"desktop\":\"400\",\"mobile\":\"150\"}),\n",
        "(\"U004\",None),\n",
        "(\"U005\",\"laptop-250\")\n",
        "]"
      ],
      "metadata": {
        "id": "Z_SVfzDlLsKI"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Design a MapType(StringType, IntegerType) schema"
      ],
      "metadata": {
        "id": "3z0YWv1UME_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import MapType"
      ],
      "metadata": {
        "id": "cmbecLjgL7H7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device_schema=StructType([\n",
        "    StructField('user_id',StringType(),nullable=False),\n",
        "    StructField('device_usage',MapType(StringType(),IntegerType()),nullable=True)\n",
        "])"
      ],
      "metadata": {
        "id": "duBxeCTlMNS4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Parse string maps into proper maps"
      ],
      "metadata": {
        "id": "hDjCDu8BMiDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, split, lit, array, create_map, regexp_replace, trim, map_from_entries, transform, struct\n",
        "from pyspark.sql.types import MapType, StringType, IntegerType, StructType, StructField\n",
        "\n",
        "processed_raw_devices = []\n",
        "for user_id, device_data in raw_devices:\n",
        "    device_data_str = None\n",
        "    if isinstance(device_data, dict):\n",
        "        parts = []\n",
        "        for k, v in device_data.items():\n",
        "            parts.append(f\"{k}:{v}\")\n",
        "        device_data_str = \",\".join(parts)\n",
        "    elif isinstance(device_data, str):\n",
        "        if '-' in device_data and ':' not in device_data:\n",
        "            device_data_str = device_data.replace('-', ':')\n",
        "        else:\n",
        "            device_data_str = device_data\n",
        "    processed_raw_devices.append((user_id, device_data_str))\n",
        "\n",
        "intermediate_device_schema = StructType([\n",
        "    StructField('user_id', StringType(), False),\n",
        "    StructField('device_usage_raw_str', StringType(), True)\n",
        "])\n",
        "\n",
        "df_devices_raw_str = spark.createDataFrame(data=processed_raw_devices, schema=intermediate_device_schema)\n",
        "\n",
        "df_devices_clean = df_devices_raw_str.withColumn(\n",
        "    \"device_usage\",\n",
        "    when(\n",
        "        col(\"device_usage_raw_str\").isNull() | (trim(col(\"device_usage_raw_str\")) == lit(\"\")),\n",
        "        create_map().cast(MapType(StringType(), IntegerType()))\n",
        "    ).otherwise(\n",
        "        map_from_entries(\n",
        "            transform(\n",
        "                split(col(\"device_usage_raw_str\"), \",\"),\n",
        "                lambda kv_pair:\n",
        "                    struct(\n",
        "                        split(kv_pair, \":\").getItem(0).alias(\"key\"),\n",
        "                        when(\n",
        "                            split(kv_pair, \":\").getItem(1).rlike(\"^[0-9]+$\"),\n",
        "                            split(kv_pair, \":\").getItem(1).cast(IntegerType())\n",
        "                        ).otherwise(lit(None)).alias(\"value\")\n",
        "                    )\n",
        "            )\n",
        "        )\n",
        "    )\n",
        ").drop(\"device_usage_raw_str\")\n",
        "\n",
        "print(\"Cleaned Device Usage DataFrame:\")\n",
        "df_devices_clean.show(truncate=False)\n",
        "df_devices_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAG5U58aMfaN",
        "outputId": "20b4d42c-cc9b-446b-8c48-f332e4e49b3d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Device Usage DataFrame:\n",
            "+-------+-------------------------------+\n",
            "|user_id|device_usage                   |\n",
            "+-------+-------------------------------+\n",
            "|U001   |{mobile -> 120, laptop -> 300} |\n",
            "|U002   |{mobile -> 200, tablet -> 100} |\n",
            "|U003   |{desktop -> 400, mobile -> 150}|\n",
            "|U004   |{}                             |\n",
            "|U005   |{laptop -> 250}                |\n",
            "+-------+-------------------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- device_usage: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: integer (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Convert all usage values to integers"
      ],
      "metadata": {
        "id": "p10_raZzNXlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"df_devices_clean already has usage values converted to integers:\")\n",
        "df_devices_clean.show(truncate=False)\n",
        "df_devices_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpPV0bZMM7WB",
        "outputId": "2091934f-6dc6-4029-ac85-2756bcbb2c87"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_devices_clean already has usage values converted to integers:\n",
            "+-------+-------------------------------+\n",
            "|user_id|device_usage                   |\n",
            "+-------+-------------------------------+\n",
            "|U001   |{mobile -> 120, laptop -> 300} |\n",
            "|U002   |{mobile -> 200, tablet -> 100} |\n",
            "|U003   |{desktop -> 400, mobile -> 150}|\n",
            "|U004   |{}                             |\n",
            "|U005   |{laptop -> 250}                |\n",
            "+-------+-------------------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- device_usage: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: integer (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Handle malformed key-value pairs"
      ],
      "metadata": {
        "id": "MCaJrKE0OqMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"df_devices_clean already handles malformed key-value pairs by setting malformed values to NULL:\")\n",
        "df_devices_clean.show(truncate=False)\n",
        "df_devices_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcUuX6jlN4kW",
        "outputId": "2b1bf225-7638-4515-a4e9-c40192328b2f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_devices_clean already handles malformed key-value pairs by setting malformed values to NULL:\n",
            "+-------+-------------------------------+\n",
            "|user_id|device_usage                   |\n",
            "+-------+-------------------------------+\n",
            "|U001   |{mobile -> 120, laptop -> 300} |\n",
            "|U002   |{mobile -> 200, tablet -> 100} |\n",
            "|U003   |{desktop -> 400, mobile -> 150}|\n",
            "|U004   |{}                             |\n",
            "|U005   |{laptop -> 250}                |\n",
            "+-------+-------------------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- device_usage: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: integer (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Replace missing maps with empty maps"
      ],
      "metadata": {
        "id": "diWc6rFvQhcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"df_devices_clean already has missing (NULL) maps replaced with empty maps:\")\n",
        "df_devices_clean.show(truncate=False)\n",
        "df_devices_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLtYnabHQcrI",
        "outputId": "2bd1c1f3-13b0-4883-9c62-bede492088fe"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_devices_clean already has missing (NULL) maps replaced with empty maps:\n",
            "+-------+-------------------------------+\n",
            "|user_id|device_usage                   |\n",
            "+-------+-------------------------------+\n",
            "|U001   |{mobile -> 120, laptop -> 300} |\n",
            "|U002   |{mobile -> 200, tablet -> 100} |\n",
            "|U003   |{desktop -> 400, mobile -> 150}|\n",
            "|U004   |{}                             |\n",
            "|U005   |{laptop -> 250}                |\n",
            "+-------+-------------------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- device_usage: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: integer (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Extract mobile usage safely"
      ],
      "metadata": {
        "id": "m6yKJdmdQ7tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_devices_with_mobile_usage = df_devices_clean.withColumn(\n",
        "    \"mobile_usage\", col(\"device_usage\")[\"mobile\"]\n",
        ")\n",
        "\n",
        "print(\"DataFrame with extracted mobile usage:\")\n",
        "df_devices_with_mobile_usage.select(\"user_id\", \"device_usage\", \"mobile_usage\").show(truncate=False)\n",
        "df_devices_with_mobile_usage.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5t1fE22Q3j3",
        "outputId": "7251604d-4803-4015-f100-1c5018210fc5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with extracted mobile usage:\n",
            "+-------+-------------------------------+------------+\n",
            "|user_id|device_usage                   |mobile_usage|\n",
            "+-------+-------------------------------+------------+\n",
            "|U001   |{mobile -> 120, laptop -> 300} |120         |\n",
            "|U002   |{mobile -> 200, tablet -> 100} |200         |\n",
            "|U003   |{desktop -> 400, mobile -> 150}|150         |\n",
            "|U004   |{}                             |NULL        |\n",
            "|U005   |{laptop -> 250}                |NULL        |\n",
            "+-------+-------------------------------+------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- device_usage: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: integer (valueContainsNull = true)\n",
            " |-- mobile_usage: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Identify users with usage above a threshold"
      ],
      "metadata": {
        "id": "GHOLKiLhRbgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "usage_threshold = 150\n",
        "\n",
        "df_users_above_threshold = df_devices_with_mobile_usage.filter(\n",
        "    col(\"mobile_usage\").isNotNull() & (col(\"mobile_usage\") > usage_threshold)\n",
        ")\n",
        "\n",
        "print(f\"Users with mobile usage above {usage_threshold}:\")\n",
        "df_users_above_threshold.select(\"user_id\", \"mobile_usage\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlgCllPTRUm4",
        "outputId": "50b46186-3b99-40ee-afdb-5525dd7c214d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Users with mobile usage above 150:\n",
            "+-------+------------+\n",
            "|user_id|mobile_usage|\n",
            "+-------+------------+\n",
            "|U002   |200         |\n",
            "+-------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 4 — NESTED ADDRESS JSON (BROKEN STRUCTS)"
      ],
      "metadata": {
        "id": "oRLYpZ0gRzLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_profiles = [\n",
        "(\"U001\",\"Hyderabad,Telangana,500081\"),\n",
        "(\"U002\",{\"city\":\"Delhi\",\"state\":\"Delhi\",\"pincode\":\"110001\"}),\n",
        "(\"U003\",(\"Bangalore\",\"Karnataka\",560001)),\n",
        "(\"U004\",\"Mumbai,MH\"),\n",
        "(\"U005\",None)\n",
        "]"
      ],
      "metadata": {
        "id": "6PGTDNhQRnKI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Design a nested StructType for address"
      ],
      "metadata": {
        "id": "n8KAFFFdR8sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "address_schema=StructType([\n",
        "    StructField('city',StringType(),nullable=True),\n",
        "    StructField('state',StringType(),nullable=True),\n",
        "    StructField('pincode',IntegerType(),nullable=True)\n",
        "])\n",
        "\n",
        "profile_schema=StructType([\n",
        "    StructField('user_id',StringType(),nullable=False),\n",
        "    StructField('address',address_schema,nullable=True)\n",
        "])"
      ],
      "metadata": {
        "id": "M2BJxEesR4DG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Normalize all address formats into struct"
      ],
      "metadata": {
        "id": "KZJGY6cxSk-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, split, lit, struct, array_contains, get\n",
        "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
        "\n",
        "processed_raw_profiles = []\n",
        "for user_id, address_data in raw_profiles:\n",
        "    address_str = None\n",
        "    if isinstance(address_data, str):\n",
        "        address_str = address_data\n",
        "    elif isinstance(address_data, dict):\n",
        "        city = address_data.get('city')\n",
        "        state = address_data.get('state')\n",
        "        pincode = address_data.get('pincode')\n",
        "        address_str = f\"{city or ''},{state or ''},{pincode or ''}\"\n",
        "    elif isinstance(address_data, tuple):\n",
        "        city, state, pincode = address_data\n",
        "        address_str = f\"{city or ''},{state or ''},{pincode or ''}\"\n",
        "    processed_raw_profiles.append((user_id, address_str))\n",
        "\n",
        "intermediate_profile_schema = StructType([\n",
        "    StructField('user_id', StringType(), False),\n",
        "    StructField('address_raw_str', StringType(), True)\n",
        "])\n",
        "\n",
        "df_profiles_raw_str = spark.createDataFrame(data=processed_raw_profiles, schema=intermediate_profile_schema)\n",
        "\n",
        "df_profiles_clean = df_profiles_raw_str.withColumn(\n",
        "    \"address\",\n",
        "    when(\n",
        "        col(\"address_raw_str\").isNull() | (col(\"address_raw_str\") == lit(\"\")),\n",
        "        lit(None).cast(address_schema)\n",
        "    ).otherwise(\n",
        "        struct(\n",
        "            get(split(col(\"address_raw_str\"), \",\"), 0).alias(\"city\"),\n",
        "            get(split(col(\"address_raw_str\"), \",\"), 1).alias(\"state\"),\n",
        "            when(\n",
        "                get(split(col(\"address_raw_str\"), \",\"), 2).rlike(\"^[0-9]+$\"),\n",
        "                get(split(col(\"address_raw_str\"), \",\"), 2).cast(IntegerType())\n",
        "            ).otherwise(lit(None)).alias(\"pincode\")\n",
        "        )\n",
        "    )\n",
        ").drop(\"address_raw_str\")\n",
        "\n",
        "print(\"Cleaned Profiles DataFrame with normalized address:\")\n",
        "df_profiles_clean.show(truncate=False)\n",
        "df_profiles_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqu4Zm-ySihO",
        "outputId": "8928d5be-ca36-42ce-9ae3-d4e21da43f2b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Profiles DataFrame with normalized address:\n",
            "+-------+------------------------------+\n",
            "|user_id|address                       |\n",
            "+-------+------------------------------+\n",
            "|U001   |{Hyderabad, Telangana, 500081}|\n",
            "|U002   |{Delhi, Delhi, 110001}        |\n",
            "|U003   |{Bangalore, Karnataka, 560001}|\n",
            "|U004   |{Mumbai, MH, NULL}            |\n",
            "|U005   |NULL                          |\n",
            "+-------+------------------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- state: string (nullable = true)\n",
            " |    |-- pincode: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Extract city, state, pincode safely"
      ],
      "metadata": {
        "id": "DsrGW129k4FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"City, State, and Pincode have already been safely extracted and flattened:\")\n",
        "df_profiles_flat.show(truncate=False)\n",
        "df_profiles_flat.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NB_2W2Jk3rf",
        "outputId": "27bcf524-0f7d-4d53-8f33-f28097e500c4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "City, State, and Pincode have already been safely extracted and flattened:\n",
            "+-------+---------+---------+-------+\n",
            "|user_id|city     |state    |pincode|\n",
            "+-------+---------+---------+-------+\n",
            "|U001   |Hyderabad|Telangana|500081 |\n",
            "|U002   |Delhi    |Delhi    |110001 |\n",
            "|U003   |Bangalore|Karnataka|560001 |\n",
            "|U004   |Mumbai   |MH       |0      |\n",
            "+-------+---------+---------+-------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- city: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- pincode: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Set default pincode when missing"
      ],
      "metadata": {
        "id": "kTIU0iybUtz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, lit, struct\n",
        "\n",
        "default_pincode = 0\n",
        "\n",
        "df_profiles_with_default_pincode = df_profiles_clean.withColumn(\n",
        "    \"address\",\n",
        "    when(\n",
        "        col(\"address\").isNotNull(),\n",
        "        struct(\n",
        "            col(\"address.city\").alias(\"city\"),\n",
        "            col(\"address.state\").alias(\"state\"),\n",
        "            when(col(\"address.pincode\").isNull(), lit(default_pincode).cast(IntegerType()))\n",
        "            .otherwise(col(\"address.pincode\"))\n",
        "            .alias(\"pincode\")\n",
        "        )\n",
        "    ).otherwise(lit(None).cast(address_schema))\n",
        ")\n",
        "\n",
        "print(\"Profiles DataFrame with default pincode:\")\n",
        "df_profiles_with_default_pincode.show(truncate=False)\n",
        "df_profiles_with_default_pincode.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqk68dAsTVzc",
        "outputId": "becd3c1b-9f84-4c9a-9075-48ac4c82d05a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profiles DataFrame with default pincode:\n",
            "+-------+------------------------------+\n",
            "|user_id|address                       |\n",
            "+-------+------------------------------+\n",
            "|U001   |{Hyderabad, Telangana, 500081}|\n",
            "|U002   |{Delhi, Delhi, 110001}        |\n",
            "|U003   |{Bangalore, Karnataka, 560001}|\n",
            "|U004   |{Mumbai, MH, 0}               |\n",
            "|U005   |NULL                          |\n",
            "+-------+------------------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- state: string (nullable = true)\n",
            " |    |-- pincode: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Drop irrecoverable records"
      ],
      "metadata": {
        "id": "J1EWw2HjVImz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_profiles_final = df_profiles_with_default_pincode.filter(col(\"address\").isNotNull())\n",
        "\n",
        "print(\"Profiles DataFrame after dropping irrecoverable address records:\")\n",
        "df_profiles_final.show(truncate=False)\n",
        "df_profiles_final.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yagZ1SqkU6sx",
        "outputId": "18d7f41f-c5e1-480b-cd4a-17a513f3d719"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profiles DataFrame after dropping irrecoverable address records:\n",
            "+-------+------------------------------+\n",
            "|user_id|address                       |\n",
            "+-------+------------------------------+\n",
            "|U001   |{Hyderabad, Telangana, 500081}|\n",
            "|U002   |{Delhi, Delhi, 110001}        |\n",
            "|U003   |{Bangalore, Karnataka, 560001}|\n",
            "|U004   |{Mumbai, MH, 0}               |\n",
            "+-------+------------------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- state: string (nullable = true)\n",
            " |    |-- pincode: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Flatten the struct into columns"
      ],
      "metadata": {
        "id": "aus4VbA4Veg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_profiles_flat = df_profiles_final.select(\"user_id\", col(\"address.city\"), col(\"address.state\"), col(\"address.pincode\"))\n",
        "\n",
        "print(\"Profiles DataFrame with address flattened into columns:\")\n",
        "df_profiles_flat.show(truncate=False)\n",
        "df_profiles_flat.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS43mefKVb4L",
        "outputId": "52b4474a-e1d4-45f9-d9d2-8e9d930335c4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profiles DataFrame with address flattened into columns:\n",
            "+-------+---------+---------+-------+\n",
            "|user_id|city     |state    |pincode|\n",
            "+-------+---------+---------+-------+\n",
            "|U001   |Hyderabad|Telangana|500081 |\n",
            "|U002   |Delhi    |Delhi    |110001 |\n",
            "|U003   |Bangalore|Karnataka|560001 |\n",
            "|U004   |Mumbai   |MH       |0      |\n",
            "+-------+---------+---------+-------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = false)\n",
            " |-- city: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- pincode: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 5 — TRANSACTION LOGS (MIXED DATES & NUMBERS)"
      ],
      "metadata": {
        "id": "zcGw48eLVvX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_transactions = [\n",
        "(\"T001\",\"2024-01-05\",\"45000\"),\n",
        "(\"T002\",\"05/01/2024\",52000),\n",
        "(\"T003\",\"Jan 06 2024\",\"Thirty Thousand\"),\n",
        "(\"T004\",None,38000),\n",
        "(\"T005\",\"2024/01/07\",\"42000\")\n",
        "]"
      ],
      "metadata": {
        "id": "8Q017qu2VoJU"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Design schema using StructType"
      ],
      "metadata": {
        "id": "2fUugRrHV7lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions_schema= StructType([\n",
        "    StructField('transaction_id',StringType(),nullable=False),\n",
        "    StructField('transaction_date',StringType(),nullable=True),\n",
        "    StructField('transaction_amount',StringType(),nullable=True)\n",
        "])"
      ],
      "metadata": {
        "id": "mm4k-3wNV3SI"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Normalize all dates into DateType"
      ],
      "metadata": {
        "id": "u_QrtoMmWra-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_date, when\n",
        "from pyspark.sql.types import DateType\n",
        "\n",
        "df_transactions = spark.createDataFrame(data=raw_transactions, schema=transactions_schema)\n",
        "\n",
        "df_transactions_clean = df_transactions.withColumn(\n",
        "    \"transaction_date_clean\",\n",
        "    when(col(\"transaction_date\").rlike(\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\"), to_date(col(\"transaction_date\"), \"yyyy-MM-dd\"))\n",
        "    .when(col(\"transaction_date\").rlike(\"^\\\\d{2}/\\\\d{2}/\\\\d{4}$\"), to_date(col(\"transaction_date\"), \"dd/MM/yyyy\"))\n",
        "    .when(col(\"transaction_date\").rlike(\"^[A-Za-z]{3} \\\\d{2} \\\\d{4}$\"), to_date(col(\"transaction_date\"), \"MMM dd yyyy\"))\n",
        "    .when(col(\"transaction_date\").rlike(\"^\\\\d{4}/\\\\d{2}/\\\\d{2}$\"), to_date(col(\"transaction_date\"), \"yyyy/MM/dd\"))\n",
        "    .otherwise(None)\n",
        ")\n",
        "\n",
        "print(\"Transactions DataFrame with normalized dates:\")\n",
        "df_transactions_clean.select(\"transaction_id\", \"transaction_date\", \"transaction_date_clean\", \"transaction_amount\").show(truncate=False)\n",
        "df_transactions_clean.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoX7QjNCWW1Z",
        "outputId": "64c5c61e-35bb-49a2-990c-a76592b60d10"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transactions DataFrame with normalized dates:\n",
            "+--------------+----------------+----------------------+------------------+\n",
            "|transaction_id|transaction_date|transaction_date_clean|transaction_amount|\n",
            "+--------------+----------------+----------------------+------------------+\n",
            "|T001          |2024-01-05      |2024-01-05            |45000             |\n",
            "|T002          |05/01/2024      |2024-01-05            |52000             |\n",
            "|T003          |Jan 06 2024     |2024-01-06            |Thirty Thousand   |\n",
            "|T004          |NULL            |NULL                  |38000             |\n",
            "|T005          |2024/01/07      |2024-01-07            |42000             |\n",
            "+--------------+----------------+----------------------+------------------+\n",
            "\n",
            "root\n",
            " |-- transaction_id: string (nullable = false)\n",
            " |-- transaction_date: string (nullable = true)\n",
            " |-- transaction_amount: string (nullable = true)\n",
            " |-- transaction_date_clean: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Convert amount into integer"
      ],
      "metadata": {
        "id": "BV9fkIubXXXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_amount = df_transactions_clean.withColumn(\n",
        "    \"transaction_amount_int\",\n",
        "    when(\n",
        "        (col(\"transaction_amount\").isNotNull()) & (col(\"transaction_amount\") != '') & col(\"transaction_amount\").rlike(\"^[0-9]+$\"),\n",
        "        col(\"transaction_amount\").cast(IntegerType())\n",
        "    ).otherwise(None)\n",
        ")\n",
        "\n",
        "print(\"Transactions DataFrame with normalized amounts:\")\n",
        "df_amount.select(\"transaction_id\", \"transaction_amount\", \"transaction_amount_int\").show(truncate=False)\n",
        "df_amount.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68NPyZEDXFdz",
        "outputId": "0e450370-e60c-4c8c-9bc5-902a399aff0c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transactions DataFrame with normalized amounts:\n",
            "+--------------+------------------+----------------------+\n",
            "|transaction_id|transaction_amount|transaction_amount_int|\n",
            "+--------------+------------------+----------------------+\n",
            "|T001          |45000             |45000                 |\n",
            "|T002          |52000             |52000                 |\n",
            "|T003          |Thirty Thousand   |NULL                  |\n",
            "|T004          |38000             |38000                 |\n",
            "|T005          |42000             |42000                 |\n",
            "+--------------+------------------+----------------------+\n",
            "\n",
            "root\n",
            " |-- transaction_id: string (nullable = false)\n",
            " |-- transaction_date: string (nullable = true)\n",
            " |-- transaction_amount: string (nullable = true)\n",
            " |-- transaction_date_clean: date (nullable = true)\n",
            " |-- transaction_amount_int: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Identify unrecoverable records"
      ],
      "metadata": {
        "id": "VuPQkmWOYMt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "failed_date_conversion = df_amount.filter(\n",
        "    (col(\"transaction_date_clean\").isNull()) &\n",
        "    (col(\"transaction_date\").isNotNull()) &\n",
        "    (col(\"transaction_date\") != '')\n",
        ")\n",
        "\n",
        "failed_amount_conversion = df_amount.filter(\n",
        "    (col(\"transaction_amount_int\").isNull()) &\n",
        "    (col(\"transaction_amount\").isNotNull()) &\n",
        "    (col(\"transaction_amount\") != '')\n",
        ")\n",
        "\n",
        "print(\"Records with unrecoverable transaction dates:\")\n",
        "failed_date_conversion.select(\"transaction_id\", \"transaction_date\", \"transaction_date_clean\").show(truncate=False)\n",
        "\n",
        "print(\"Records with unrecoverable transaction amounts:\")\n",
        "failed_amount_conversion.select(\"transaction_id\", \"transaction_amount\", \"transaction_amount_int\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWO5fTBhXxu2",
        "outputId": "f1f65188-87eb-43e1-d874-49722ac5070c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Records with unrecoverable transaction dates:\n",
            "+--------------+----------------+----------------------+\n",
            "|transaction_id|transaction_date|transaction_date_clean|\n",
            "+--------------+----------------+----------------------+\n",
            "+--------------+----------------+----------------------+\n",
            "\n",
            "Records with unrecoverable transaction amounts:\n",
            "+--------------+------------------+----------------------+\n",
            "|transaction_id|transaction_amount|transaction_amount_int|\n",
            "+--------------+------------------+----------------------+\n",
            "|T003          |Thirty Thousand   |NULL                  |\n",
            "+--------------+------------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Separate valid vs invalid transactions"
      ],
      "metadata": {
        "id": "l4FLS6jqYTcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_valid_transactions = df_amount.filter(\n",
        "    col(\"transaction_date_clean\").isNotNull() & col(\"transaction_amount_int\").isNotNull()\n",
        ")\n",
        "\n",
        "df_invalid_transactions = df_amount.filter(\n",
        "    (col(\"transaction_date_clean\").isNull() & col(\"transaction_date\").isNotNull() & (col(\"transaction_date\") != '')) |\n",
        "    (col(\"transaction_amount_int\").isNull() & col(\"transaction_amount\").isNotNull() & (col(\"transaction_amount\") != ''))\n",
        ")\n",
        "\n",
        "print(\"Valid Transactions:\")\n",
        "df_valid_transactions.show(truncate=False)\n",
        "\n",
        "print(\"Invalid Transactions:\")\n",
        "df_invalid_transactions.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-VZntcnYUVP",
        "outputId": "40304b8c-e52e-4081-bbe6-24b8ab8109de"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Transactions:\n",
            "+--------------+----------------+------------------+----------------------+----------------------+\n",
            "|transaction_id|transaction_date|transaction_amount|transaction_date_clean|transaction_amount_int|\n",
            "+--------------+----------------+------------------+----------------------+----------------------+\n",
            "|T001          |2024-01-05      |45000             |2024-01-05            |45000                 |\n",
            "|T002          |05/01/2024      |52000             |2024-01-05            |52000                 |\n",
            "|T005          |2024/01/07      |42000             |2024-01-07            |42000                 |\n",
            "+--------------+----------------+------------------+----------------------+----------------------+\n",
            "\n",
            "Invalid Transactions:\n",
            "+--------------+----------------+------------------+----------------------+----------------------+\n",
            "|transaction_id|transaction_date|transaction_amount|transaction_date_clean|transaction_amount_int|\n",
            "+--------------+----------------+------------------+----------------------+----------------------+\n",
            "|T003          |Jan 06 2024     |Thirty Thousand   |2024-01-06            |NULL                  |\n",
            "+--------------+----------------+------------------+----------------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Produce a clean transactions DataFrame"
      ],
      "metadata": {
        "id": "2ZSKjkcfYhgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_clean_transactions = df_valid_transactions.select(\"transaction_id\", col(\"transaction_date_clean\").alias(\"transaction_date\"), col(\"transaction_amount_int\").alias(\"transaction_amount\"))\n",
        "\n",
        "print(\"Final Clean Transactions DataFrame:\")\n",
        "df_final_clean_transactions.show(truncate=False)\n",
        "df_final_clean_transactions.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRg0NUEMYiV-",
        "outputId": "47c6ffee-fb4b-4115-bf3d-e936f4b2b736"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Clean Transactions DataFrame:\n",
            "+--------------+----------------+------------------+\n",
            "|transaction_id|transaction_date|transaction_amount|\n",
            "+--------------+----------------+------------------+\n",
            "|T001          |2024-01-05      |45000             |\n",
            "|T002          |2024-01-05      |52000             |\n",
            "|T005          |2024-01-07      |42000             |\n",
            "+--------------+----------------+------------------+\n",
            "\n",
            "root\n",
            " |-- transaction_id: string (nullable = false)\n",
            " |-- transaction_date: date (nullable = true)\n",
            " |-- transaction_amount: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}