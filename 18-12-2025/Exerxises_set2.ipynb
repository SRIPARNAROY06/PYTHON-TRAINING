{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 1 — USER REGISTRATION (CORRUPTED SCHEMA)"
      ],
      "metadata": {
        "id": "A_uHqfYwxRdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "spark=SparkSession.builder.appName('Struct Type').getOrCreate()"
      ],
      "metadata": {
        "id": "UbNgm_d6yMzR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D4IjGM_7xQhQ"
      },
      "outputs": [],
      "source": [
        "raw_users = [\n",
        "(\"U001\",\"Amit\",\"28\",\"Hyderabad\",\"['AI','ML','Cloud']\"),\n",
        "(\"U002\",\"Neha\",\"Thirty\",\"Delhi\",\"AI,Testing\"),\n",
        "(\"U003\",\"Ravi\",None,\"Bangalore\",[\"Data\",\"Spark\"]),\n",
        "(\"U004\",\"Pooja\",\"29\",\"Mumbai\",None),\n",
        "(\"U005\",\"\", \"31\",\"Chennai\",\"['DevOps']\")\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Design an explicit schema using StructType"
      ],
      "metadata": {
        "id": "D9Xey3qxx5Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import(\n",
        "    StructType,\n",
        "    StructField,\n",
        "    StringType,\n",
        "    IntegerType,\n",
        "    LongType\n",
        ")"
      ],
      "metadata": {
        "id": "porAj5cSyGu7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_schema=StructType([\n",
        "    StructField(\"user_id\",StringType(),True),\n",
        "    StructField(\"name\",StringType(),True),\n",
        "    StructField(\"age\",StringType(),True),\n",
        "    StructField(\"city\",StringType(),True),\n",
        "    StructField(\"interests\",StringType(),True)\n",
        "])"
      ],
      "metadata": {
        "id": "uEgwzsHUxbF3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Normalize age into IntegerType"
      ],
      "metadata": {
        "id": "ZDrZeXWYyWTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,when,regexp_replace\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_users=spark.createDataFrame(raw_users,users_schema)\n",
        "df_users.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH7AepRjyRc-",
        "outputId": "912d2123-b7c1-4983-cdef-98f11e2248e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+------+---------+-------------------+\n",
            "|user_id| name|   age|     city|          interests|\n",
            "+-------+-----+------+---------+-------------------+\n",
            "|   U001| Amit|    28|Hyderabad|['AI','ML','Cloud']|\n",
            "|   U002| Neha|Thirty|    Delhi|         AI,Testing|\n",
            "|   U003| Ravi|  NULL|Bangalore|      [Data, Spark]|\n",
            "|   U004|Pooja|    29|   Mumbai|               NULL|\n",
            "|   U005|     |    31|  Chennai|         ['DevOps']|\n",
            "+-------+-----+------+---------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_users_age_normalized = df_users.withColumn(\n",
        "    \"age\",\n",
        "    when(col(\"age\").rlike(\"^[0-9]+$\"), col(\"age\").cast(IntegerType()))\n",
        "    .otherwise(None)\n",
        ")\n",
        "\n",
        "df_users_age_normalized.show()\n",
        "df_users_age_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyX2WpgpzHAF",
        "outputId": "deeea432-38eb-486c-bf4a-9d07133cd90c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+----+---------+-------------------+\n",
            "|user_id| name| age|     city|          interests|\n",
            "+-------+-----+----+---------+-------------------+\n",
            "|   U001| Amit|  28|Hyderabad|['AI','ML','Cloud']|\n",
            "|   U002| Neha|NULL|    Delhi|         AI,Testing|\n",
            "|   U003| Ravi|NULL|Bangalore|      [Data, Spark]|\n",
            "|   U004|Pooja|  29|   Mumbai|               NULL|\n",
            "|   U005|     |  31|  Chennai|         ['DevOps']|\n",
            "+-------+-----+----+---------+-------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- interests: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Normalize skills into ArrayType"
      ],
      "metadata": {
        "id": "97tqV7LM0FPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, split, regexp_replace, when, trim, expr\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "df_users_skills_normalized = df_users_age_normalized.withColumn(\n",
        "    \"cleaned_interests_str\",\n",
        "    when(\n",
        "        col(\"interests\").isNotNull(),\n",
        "        regexp_replace(\n",
        "            regexp_replace(\n",
        "                col(\"interests\"),\n",
        "                \"^\\[|\\]$\", \"\"\n",
        "            ),\n",
        "            \"'\", \"\"\n",
        "        )\n",
        "    ).otherwise(None)\n",
        ").withColumn(\n",
        "    \"interests\",\n",
        "    when(\n",
        "        col(\"cleaned_interests_str\").isNotNull(),\n",
        "        split(col(\"cleaned_interests_str\"), \",\")\n",
        "    ).otherwise(None)\n",
        ").withColumn(\n",
        "    \"interests\",\n",
        "    when(\n",
        "        col(\"interests\").isNotNull(),\n",
        "\n",
        "        expr(\"filter(transform(interests, x -> trim(x)), x -> x != '')\")\n",
        "    ).otherwise(None)\n",
        ").drop(\"cleaned_interests_str\")\n",
        "\n",
        "df_users_skills_normalized.show()\n",
        "df_users_skills_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZx7tXL0z_el",
        "outputId": "9be26b9d-58eb-416a-8751-60ea3f3fd41e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:11: SyntaxWarning: invalid escape sequence '\\['\n",
            "<>:11: SyntaxWarning: invalid escape sequence '\\['\n",
            "/tmp/ipython-input-345841350.py:11: SyntaxWarning: invalid escape sequence '\\['\n",
            "  \"^\\[|\\]$\", \"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+----+---------+---------------+\n",
            "|user_id| name| age|     city|      interests|\n",
            "+-------+-----+----+---------+---------------+\n",
            "|   U001| Amit|  28|Hyderabad|[AI, ML, Cloud]|\n",
            "|   U002| Neha|NULL|    Delhi|  [AI, Testing]|\n",
            "|   U003| Ravi|NULL|Bangalore|  [Data, Spark]|\n",
            "|   U004|Pooja|  29|   Mumbai|           NULL|\n",
            "|   U005|     |  31|  Chennai|       [DevOps]|\n",
            "+-------+-----+----+---------+---------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- interests: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Handle empty or missing names"
      ],
      "metadata": {
        "id": "yLG5Vjg70_Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "df_users_names_handled = df_users_skills_normalized.withColumn(\n",
        "    \"name\",\n",
        "    when(col(\"name\").isNull() | (col(\"name\") == \"\"), \"Unknown\")\n",
        "    .otherwise(col(\"name\"))\n",
        ")\n",
        "\n",
        "df_users_names_handled.show()\n",
        "df_users_names_handled.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_-8DJ-T00GZ",
        "outputId": "0321bb1c-0883-488b-f391-38dd83a6a7fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----+---------+---------------+\n",
            "|user_id|   name| age|     city|      interests|\n",
            "+-------+-------+----+---------+---------------+\n",
            "|   U001|   Amit|  28|Hyderabad|[AI, ML, Cloud]|\n",
            "|   U002|   Neha|NULL|    Delhi|  [AI, Testing]|\n",
            "|   U003|   Ravi|NULL|Bangalore|  [Data, Spark]|\n",
            "|   U004|  Pooja|  29|   Mumbai|           NULL|\n",
            "|   U005|Unknown|  31|  Chennai|       [DevOps]|\n",
            "+-------+-------+----+---------+---------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- interests: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Produce a clean users_df"
      ],
      "metadata": {
        "id": "fWH8Wck517Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_df = df_users_names_handled\n",
        "\n",
        "users_df.show()\n",
        "users_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1WK1yuu1XEv",
        "outputId": "26ed5742-8b54-455b-eb2d-bd0e8b2697db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----+---------+---------------+\n",
            "|user_id|   name| age|     city|      interests|\n",
            "+-------+-------+----+---------+---------------+\n",
            "|   U001|   Amit|  28|Hyderabad|[AI, ML, Cloud]|\n",
            "|   U002|   Neha|NULL|    Delhi|  [AI, Testing]|\n",
            "|   U003|   Ravi|NULL|Bangalore|  [Data, Spark]|\n",
            "|   U004|  Pooja|  29|   Mumbai|           NULL|\n",
            "|   U005|Unknown|  31|  Chennai|       [DevOps]|\n",
            "+-------+-------+----+---------+---------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- interests: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 2 — COURSE CATALOG (NESTED STRUCT)"
      ],
      "metadata": {
        "id": "zOxKPnEs2Xck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_courses = [\n",
        "(\"C001\",\"PySpark Mastery\",(\"Data Engineering\",\"Advanced\"),\"₹9999\"),\n",
        "(\"C002\",\"AI for Testers\",{\"domain\":\"QA\",\"level\":\"Beginner\"},\"8999\"),\n",
        "(\"C003\",\"ML Foundations\",(\"AI\",\"Intermediate\"),None),\n",
        "(\"C004\",\"Data Engineering Bootcamp\",\"Data|Advanced\",\"₹14999\")\n",
        "]"
      ],
      "metadata": {
        "id": "Uj1S8rUL2To3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Create nested StructType for course metadata"
      ],
      "metadata": {
        "id": "99MDeLXS29KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "course_metadata_schema = StructType([\n",
        "    StructField(\"domain\", StringType(), True),\n",
        "    StructField(\"level\", StringType(), True)\n",
        "])\n",
        "\n",
        "course_schema = StructType([\n",
        "    StructField(\"course_id\", StringType(), True),\n",
        "    StructField(\"title\", StringType(), True),\n",
        "    StructField(\"metadata\", course_metadata_schema, True),\n",
        "    StructField(\"price\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "9CRch1aP254q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Normalize domain and level"
      ],
      "metadata": {
        "id": "olaa1duq3a1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, split, struct, lit\n",
        "from pyspark.sql.types import StringType, StructType, StructField, Row\n",
        "\n",
        "# Preprocess raw_courses to explicitly create Row objects for the nested metadata struct\n",
        "processed_raw_courses = []\n",
        "for course_id, title, metadata_raw, price in raw_courses:\n",
        "    domain = None\n",
        "    level = None\n",
        "\n",
        "    if isinstance(metadata_raw, tuple) and len(metadata_raw) == 2:\n",
        "        # Handle tuple format: (\"Data Engineering\",\"Advanced\")\n",
        "        domain = metadata_raw[0]\n",
        "        level = metadata_raw[1]\n",
        "    elif isinstance(metadata_raw, dict):\n",
        "        # Handle dictionary format: {\"domain\":\"QA\",\"level\":\"Beginner\"}\n",
        "        domain = metadata_raw.get(\"domain\")\n",
        "        level = metadata_raw.get(\"level\")\n",
        "    elif isinstance(metadata_raw, str):\n",
        "        # Handle pipe-separated string format: \"Data|Advanced\"\n",
        "        parts = metadata_raw.split('|')\n",
        "        if len(parts) == 2:\n",
        "            domain = parts[0]\n",
        "            level = parts[1]\n",
        "\n",
        "    # Create a Row for the metadata field, or None if no valid metadata was found\n",
        "    metadata_struct_row = Row(domain=domain, level=level) if (domain is not None or level is not None) else None\n",
        "\n",
        "    # Append the processed Row for the entire course entry\n",
        "    processed_raw_courses.append(Row(course_id=course_id, title=title, metadata=metadata_struct_row, price=price))\n",
        "\n",
        "# Create the DataFrame using the preprocessed list and the defined course_schema\n",
        "df_courses_normalized = spark.createDataFrame(processed_raw_courses, course_schema)\n",
        "\n",
        "df_courses_normalized.show(truncate=False)\n",
        "df_courses_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3TscAUv3YMd",
        "outputId": "4db6baa1-02fc-4932-b4b5-503bc5b9483c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+----------------------------+------+\n",
            "|course_id|title                    |metadata                    |price |\n",
            "+---------+-------------------------+----------------------------+------+\n",
            "|C001     |PySpark Mastery          |{Data Engineering, Advanced}|₹9999 |\n",
            "|C002     |AI for Testers           |{QA, Beginner}              |8999  |\n",
            "|C003     |ML Foundations           |{AI, Intermediate}          |NULL  |\n",
            "|C004     |Data Engineering Bootcamp|{Data, Advanced}            |₹14999|\n",
            "+---------+-------------------------+----------------------------+------+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- metadata: struct (nullable = true)\n",
            " |    |-- domain: string (nullable = true)\n",
            " |    |-- level: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Convert price to IntegerType"
      ],
      "metadata": {
        "id": "Gt5yUqwL7Yet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, regexp_replace, when\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Convert price to IntegerType by removing currency symbols and casting\n",
        "df_courses_final = df_courses_normalized.withColumn(\n",
        "    \"price\",\n",
        "    when(col(\"price\").isNotNull(),\n",
        "         regexp_replace(col(\"price\"), \"₹\", \"\").cast(IntegerType())\n",
        "    ).otherwise(None)\n",
        ")\n",
        "\n",
        "df_courses_final.show()\n",
        "df_courses_final.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7VvN1TO7SOU",
        "outputId": "09d81759-67e5-4001-d3d2-7e97e7c28494"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+--------------------+-----+\n",
            "|course_id|               title|            metadata|price|\n",
            "+---------+--------------------+--------------------+-----+\n",
            "|     C001|     PySpark Mastery|{Data Engineering...| 9999|\n",
            "|     C002|      AI for Testers|      {QA, Beginner}| 8999|\n",
            "|     C003|      ML Foundations|  {AI, Intermediate}| NULL|\n",
            "|     C004|Data Engineering ...|    {Data, Advanced}|14999|\n",
            "+---------+--------------------+--------------------+-----+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- metadata: struct (nullable = true)\n",
            " |    |-- domain: string (nullable = true)\n",
            " |    |-- level: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Handle missing prices"
      ],
      "metadata": {
        "id": "U7FYr5ZW7oqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Fill missing prices (NULL values) with 0\n",
        "df_courses_final = df_courses_final.withColumn(\n",
        "    \"price\",\n",
        "    when(col(\"price\").isNull(), 0).otherwise(col(\"price\"))\n",
        ")\n",
        "\n",
        "# Alternatively, a more concise way using fillna:\n",
        "# df_courses_final = df_courses_final.fillna(0, subset=[\"price\"])\n",
        "\n",
        "df_courses_final.show()\n",
        "df_courses_final.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjSgMg-v7iKZ",
        "outputId": "ac526555-ffa0-4dcd-f5c7-e730d7c3e79e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+--------------------+-----+\n",
            "|course_id|               title|            metadata|price|\n",
            "+---------+--------------------+--------------------+-----+\n",
            "|     C001|     PySpark Mastery|{Data Engineering...| 9999|\n",
            "|     C002|      AI for Testers|      {QA, Beginner}| 8999|\n",
            "|     C003|      ML Foundations|  {AI, Intermediate}|    0|\n",
            "|     C004|Data Engineering ...|    {Data, Advanced}|14999|\n",
            "+---------+--------------------+--------------------+-----+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- metadata: struct (nullable = true)\n",
            " |    |-- domain: string (nullable = true)\n",
            " |    |-- level: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Produce courses_df"
      ],
      "metadata": {
        "id": "f4w3nRAF77W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "courses_df = df_courses_final\n",
        "\n",
        "courses_df.show()\n",
        "courses_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnNMc7cA7zL4",
        "outputId": "fd9d1088-51fa-4ba1-9fd5-00475cbc17ee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+--------------------+-----+\n",
            "|course_id|               title|            metadata|price|\n",
            "+---------+--------------------+--------------------+-----+\n",
            "|     C001|     PySpark Mastery|{Data Engineering...| 9999|\n",
            "|     C002|      AI for Testers|      {QA, Beginner}| 8999|\n",
            "|     C003|      ML Foundations|  {AI, Intermediate}|    0|\n",
            "|     C004|Data Engineering ...|    {Data, Advanced}|14999|\n",
            "+---------+--------------------+--------------------+-----+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- metadata: struct (nullable = true)\n",
            " |    |-- domain: string (nullable = true)\n",
            " |    |-- level: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 4 — USER ACTIVITY LOGS (ARRAY +MAP)"
      ],
      "metadata": {
        "id": "mtgpGEYN8T2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_activity = [\n",
        "(\"U001\",\"login,watch,logout\",\"{'device':'mobile','ip':'1.1.1.1'}\",120),\n",
        "(\"U002\",[\"login\",\"watch\"],\"device=laptop;ip=2.2.2.2\",90),\n",
        "(\"U003\",\"login|logout\",None,30),\n",
        "(\"U004\",None,\"{'device':'tablet'}\",60)\n",
        "]"
      ],
      "metadata": {
        "id": "ECSz8qag8WNB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Normalize actions into ArrayType"
      ],
      "metadata": {
        "id": "rdV3AWHe8sRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, when, split\n",
        "\n",
        "# 1. Define the schema for the activity data\n",
        "activity_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"actions\", ArrayType(StringType()), True), # Normalize actions to ArrayType\n",
        "    StructField(\"properties\", StringType(), True),\n",
        "    StructField(\"duration\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# 2. Preprocess raw_activity to normalize the 'actions' field\n",
        "processed_raw_activity = []\n",
        "for user_id, actions_raw, properties, duration in raw_activity:\n",
        "    normalized_actions = None\n",
        "    if actions_raw is None:\n",
        "        normalized_actions = None\n",
        "    elif isinstance(actions_raw, list):\n",
        "        normalized_actions = actions_raw\n",
        "    elif isinstance(actions_raw, str):\n",
        "        if ',' in actions_raw:\n",
        "            normalized_actions = [a.strip() for a in actions_raw.split(',')]\n",
        "        elif '|' in actions_raw:\n",
        "            normalized_actions = [a.strip() for a in actions_raw.split('|')]\n",
        "\n",
        "    processed_raw_activity.append(Row(user_id=user_id, actions=normalized_actions, properties=properties, duration=duration))\n",
        "\n",
        "# 3. Create the DataFrame\n",
        "df_activity = spark.createDataFrame(processed_raw_activity, activity_schema)\n",
        "\n",
        "# Display the DataFrame and its schema\n",
        "df_activity.show(truncate=False)\n",
        "df_activity.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1JIHKQg8fIK",
        "outputId": "f383a14a-5eeb-469d-9a5f-42807ec22f76"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+----------------------------------+--------+\n",
            "|user_id|actions               |properties                        |duration|\n",
            "+-------+----------------------+----------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{'device':'mobile','ip':'1.1.1.1'}|120     |\n",
            "|U002   |[login, watch]        |device=laptop;ip=2.2.2.2          |90      |\n",
            "|U003   |[login, logout]       |NULL                              |30      |\n",
            "|U004   |NULL                  |{'device':'tablet'}               |60      |\n",
            "+-------+----------------------+----------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Normalize metadata into MapType"
      ],
      "metadata": {
        "id": "d6FbC9bo9D6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, from_json, regexp_replace, udf\n",
        "from pyspark.sql.types import MapType, StringType\n",
        "\n",
        "# Define a UDF to parse custom key-value strings like \"device=laptop;ip=2.2.2.2\"\n",
        "def parse_custom_properties(s):\n",
        "    if s is None:\n",
        "        return None\n",
        "    try:\n",
        "        parts = s.split(';')\n",
        "        result_map = {}\n",
        "        for part in parts:\n",
        "            if '=' in part:\n",
        "                key, value = part.split('=', 1)\n",
        "                result_map[key.strip()] = value.strip()\n",
        "        return result_map\n",
        "    except Exception:\n",
        "        return None # Return None for malformed strings\n",
        "\n",
        "# Register the UDF\n",
        "parse_custom_properties_udf = udf(parse_custom_properties, MapType(StringType(), StringType()))\n",
        "\n",
        "df_activity_normalized = df_activity.withColumn(\n",
        "    \"properties\",\n",
        "    when(col(\"properties\").isNull(), None)\n",
        "    .when(col(\"properties\").startswith(\"{\"), # Check for JSON-like strings (start with '{')\n",
        "          from_json(regexp_replace(col(\"properties\"), \"'\", \"\\\"\"), MapType(StringType(), StringType())))\n",
        "    .otherwise(parse_custom_properties_udf(col(\"properties\"))) # Handle custom format for others\n",
        ")\n",
        "\n",
        "df_activity_normalized.show(truncate=False)\n",
        "df_activity_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkoTda7385gv",
        "outputId": "5a23c937-5e2a-4e02-bd0b-5603e78e92e3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |properties                       |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |NULL                             |30      |\n",
            "|U004   |NULL                  |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Handle missing actions safely"
      ],
      "metadata": {
        "id": "MDR66wjw9cDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, array\n",
        "\n",
        "# Handle missing actions safely by replacing NULL with empty array\n",
        "df_activity_normalized = df_activity_normalized.withColumn(\n",
        "    \"actions\",\n",
        "    when(col(\"actions\").isNull(), array()).otherwise(col(\"actions\"))\n",
        ")\n",
        "\n",
        "df_activity_normalized.show(truncate=False)\n",
        "df_activity_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66lHIfdk9QJT",
        "outputId": "eb7fcb47-09eb-4bf3-b6c7-1550c0aaa0f0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |properties                       |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |NULL                             |30      |\n",
            "|U004   |[]                    |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Explode actions and count frequency"
      ],
      "metadata": {
        "id": "EhQZTxEN9spp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col, count\n",
        "\n",
        "# Explode the 'actions' array to create a new row for each action\n",
        "df_exploded_actions = df_activity_normalized.select(col(\"user_id\"), explode(col(\"actions\")).alias(\"action\"))\n",
        "\n",
        "# Count the frequency of each action\n",
        "action_frequency = df_exploded_actions.groupBy(\"action\").agg(count(\"action\").alias(\"frequency\"))\n",
        "\n",
        "# Show the results, ordered by frequency\n",
        "action_frequency.orderBy(col(\"frequency\").desc()).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLT7QHjZ9lKg",
        "outputId": "b2b9a524-275d-43ba-f3b2-a076cc653b54"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+\n",
            "|action|frequency|\n",
            "+------+---------+\n",
            "| login|        3|\n",
            "| watch|        2|\n",
            "|logout|        2|\n",
            "+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Produce activity_df"
      ],
      "metadata": {
        "id": "gAcYfm2o-BfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activity_df = df_activity_normalized\n",
        "\n",
        "activity_df.show(truncate=False)\n",
        "activity_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a-Y9M9w99ta",
        "outputId": "00ead3e5-07a6-4bbe-939c-052f2daa8da1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |properties                       |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |NULL                             |30      |\n",
            "|U004   |[]                    |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 3 — USER COURSE ENROLLMENTS (JOIN + BROADCAST)"
      ],
      "metadata": {
        "id": "-rJI4_ZP-8N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_enrollments = [\n",
        "(\"U001\",\"C001\",\"2024-01-05\"),\n",
        "(\"U002\",\"C002\",\"05/01/2024\"),\n",
        "(\"U003\",\"C001\",\"2024/01/06\"),\n",
        "(\"U004\",\"C003\",\"invalid_date\"),\n",
        "(\"U001\",\"C004\",\"2024-01-10\")\n",
        "]"
      ],
      "metadata": {
        "id": "Hz8Zvu5K--Ys"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enroll_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"course_id\", StringType(), True),\n",
        "    StructField(\"enrollment_date_raw\", StringType(), True)\n",
        "])\n",
        "df_enrollments_raw = spark.createDataFrame(raw_enrollments, enroll_schema)\n",
        "df_enrollments_raw.printSchema()\n",
        "df_enrollments_raw.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnONqalL_DGO",
        "outputId": "7c83ce2b-b1df-4b06-eaa1-7bb2776befa3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- enrollment_date_raw: string (nullable = true)\n",
            "\n",
            "+-------+---------+-------------------+\n",
            "|user_id|course_id|enrollment_date_raw|\n",
            "+-------+---------+-------------------+\n",
            "|   U001|     C001|         2024-01-05|\n",
            "|   U002|     C002|         05/01/2024|\n",
            "|   U003|     C001|         2024/01/06|\n",
            "|   U004|     C003|       invalid_date|\n",
            "|   U001|     C004|         2024-01-10|\n",
            "+-------+---------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import coalesce, col, to_date, when\n",
        "\n",
        "df_enrollments_raw = df_enrollments_raw.withColumn(\n",
        "    \"enrollment_date\",\n",
        "    coalesce(\n",
        "        when(col(\"enrollment_date_raw\").rlike(\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$\"), to_date(col(\"enrollment_date_raw\"), \"yyyy-MM-dd\")),\n",
        "        when(col(\"enrollment_date_raw\").rlike(\"^\\\\d{2}/\\\\d{2}/\\\\d{4}$\"), to_date(col(\"enrollment_date_raw\"), \"dd/MM/yyyy\")),\n",
        "        when(col(\"enrollment_date_raw\").rlike(\"^\\\\d{4}/\\\\d{2}/\\\\d{2}$\"), to_date(col(\"enrollment_date_raw\"), \"yyyy/MM/dd\"))\n",
        "    )\n",
        ")\n",
        "df_enrollments_raw.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKNrHJoyBDws",
        "outputId": "7e84abfc-b962-4eac-ab1e-019686c47c3c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-------------------+---------------+\n",
            "|user_id|course_id|enrollment_date_raw|enrollment_date|\n",
            "+-------+---------+-------------------+---------------+\n",
            "|   U001|     C001|         2024-01-05|     2024-01-05|\n",
            "|   U002|     C002|         05/01/2024|     2024-01-05|\n",
            "|   U003|     C001|         2024/01/06|     2024-01-06|\n",
            "|   U004|     C003|       invalid_date|           NULL|\n",
            "|   U001|     C004|         2024-01-10|     2024-01-10|\n",
            "+-------+---------+-------------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast"
      ],
      "metadata": {
        "id": "7BYI8il7BVJP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_enrollments_processed = df_enrollments_raw.drop(\"enrollment_date_raw\")\n",
        "df_enriched = df_enrollments_processed.join(broadcast(courses_df), on=\"course_id\", how=\"left\")\n",
        "df_enriched.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXbjDJSYBGF-",
        "outputId": "583e69e5-036d-457f-b3e6-b021a1b0bc81"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+---------------+--------------------+--------------------+-----+\n",
            "|course_id|user_id|enrollment_date|               title|            metadata|price|\n",
            "+---------+-------+---------------+--------------------+--------------------+-----+\n",
            "|     C001|   U001|     2024-01-05|     PySpark Mastery|{Data Engineering...| 9999|\n",
            "|     C002|   U002|     2024-01-05|      AI for Testers|      {QA, Beginner}| 8999|\n",
            "|     C001|   U003|     2024-01-06|     PySpark Mastery|{Data Engineering...| 9999|\n",
            "|     C003|   U004|           NULL|      ML Foundations|  {AI, Intermediate}|    0|\n",
            "|     C004|   U001|     2024-01-10|Data Engineering ...|    {Data, Advanced}|14999|\n",
            "+---------+-------+---------------+--------------------+--------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_enriched.show(truncate=False)\n",
        "df_enriched.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mGbChueBGwr",
        "outputId": "81571934-cf98-4757-c032-8599f4f92bed"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+---------------+-------------------------+----------------------------+-----+\n",
            "|course_id|user_id|enrollment_date|title                    |metadata                    |price|\n",
            "+---------+-------+---------------+-------------------------+----------------------------+-----+\n",
            "|C001     |U001   |2024-01-05     |PySpark Mastery          |{Data Engineering, Advanced}|9999 |\n",
            "|C002     |U002   |2024-01-05     |AI for Testers           |{QA, Beginner}              |8999 |\n",
            "|C001     |U003   |2024-01-06     |PySpark Mastery          |{Data Engineering, Advanced}|9999 |\n",
            "|C003     |U004   |NULL           |ML Foundations           |{AI, Intermediate}          |0    |\n",
            "|C004     |U001   |2024-01-10     |Data Engineering Bootcamp|{Data, Advanced}            |14999|\n",
            "+---------+-------+---------------+-------------------------+----------------------------+-----+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- enrollment_date: date (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- metadata: struct (nullable = true)\n",
            " |    |-- domain: string (nullable = true)\n",
            " |    |-- level: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision: Broadcast df_courses_clean\n",
        "# Reasoning: The `df_courses_clean` (course catalog) is expected to be significantly smaller than `df_enrollments_processed` (user enrollments).\n",
        "# Broadcasting the smaller table to all worker nodes during a join optimizes performance by avoiding a shuffle of the larger DataFrame and reducing network I/O.\n",
        "# This was already implemented in the previous join: `df_enrollments_processed.join(broadcast(df_courses_clean), on=\"course_id\", how=\"left\")`"
      ],
      "metadata": {
        "id": "wmxuV3YiBKcT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_enriched.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7OeEfGaBMjh",
        "outputId": "f2629249-3512-4545-d0a9-f69aee61e999"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [course_id])\n",
            ":- Project [user_id#245, course_id#246, enrollment_date#258]\n",
            ":  +- Project [user_id#245, course_id#246, enrollment_date_raw#247, coalesce(CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#247, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#247, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#247, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#258]\n",
            ":     +- LogicalRDD [user_id#245, course_id#246, enrollment_date_raw#247], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "      +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) ELSE cast(null as int) END AS price#126]\n",
            "         +- LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, user_id: string, enrollment_date: date, title: string, metadata: struct<domain:string,level:string>, price: int\n",
            "Project [course_id#246, user_id#245, enrollment_date#258, title#110, metadata#111, price#140]\n",
            "+- Join LeftOuter, (course_id#246 = course_id#109)\n",
            "   :- Project [user_id#245, course_id#246, enrollment_date#258]\n",
            "   :  +- Project [user_id#245, course_id#246, enrollment_date_raw#247, coalesce(CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#247, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#247, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#247, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#258]\n",
            "   :     +- LogicalRDD [user_id#245, course_id#246, enrollment_date_raw#247], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "         +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) ELSE cast(null as int) END AS price#126]\n",
            "            +- LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [course_id#246, user_id#245, enrollment_date#258, title#110, metadata#111, price#140]\n",
            "+- Join LeftOuter, (course_id#246 = course_id#109), rightHint=(strategy=broadcast)\n",
            "   :- Project [user_id#245, course_id#246, coalesce(CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#247, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#247, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#247, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#258]\n",
            "   :  +- LogicalRDD [user_id#245, course_id#246, enrollment_date_raw#247], false\n",
            "   +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "      +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) END AS price#126]\n",
            "         +- Filter isnotnull(course_id#109)\n",
            "            +- LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [course_id#246, user_id#245, enrollment_date#258, title#110, metadata#111, price#140]\n",
            "   +- BroadcastHashJoin [course_id#246], [course_id#109], LeftOuter, BuildRight, false\n",
            "      :- Project [user_id#245, course_id#246, coalesce(CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#247, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#247, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#247, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#258]\n",
            "      :  +- Scan ExistingRDD[user_id#245,course_id#246,enrollment_date_raw#247]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=528]\n",
            "         +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "            +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) END AS price#126]\n",
            "               +- Filter isnotnull(course_id#109)\n",
            "                  +- Scan ExistingRDD[course_id#109,title#110,metadata#111,price#112]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 5 — PAYMENTS (WINDOW + AGGREGATES)"
      ],
      "metadata": {
        "id": "3uywbEwzBoHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= [\n",
        "(\"U001\",\"2024-01-05\",9999),\n",
        "(\"U001\",\"2024-01-10\",14999),\n",
        "(\"U002\",\"2024-01-06\",8999),\n",
        "(\"U003\",\"2024-01-07\",0),\n",
        "(\"U004\",\"2024-01-08\",7999),\n",
        "(\"U001\",\"2024-01-15\",1999)\n",
        "]"
      ],
      "metadata": {
        "id": "Ety9LvOeBkNy"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"user_id\",\"date\",\"amount\"]\n",
        "\n",
        "df=spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4-0gRXBBwMf",
        "outputId": "4a3f641d-5638-4e59-9fce-cc74623a2bad"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+\n",
            "|user_id|      date|amount|\n",
            "+-------+----------+------+\n",
            "|   U001|2024-01-05|  9999|\n",
            "|   U001|2024-01-10| 14999|\n",
            "|   U002|2024-01-06|  8999|\n",
            "|   U003|2024-01-07|     0|\n",
            "|   U004|2024-01-08|  7999|\n",
            "|   U001|2024-01-15|  1999|\n",
            "+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "df = df.withColumn(\"date\", to_date(df[\"date\"], \"yyyy-MM-dd\"))\n",
        "\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG3jjK1nB33J",
        "outputId": "4681e3f9-7d34-4f91-e77d-ae463ec3cabe"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            " |-- amount: long (nullable = true)\n",
            "\n",
            "+-------+----------+------+\n",
            "|user_id|      date|amount|\n",
            "+-------+----------+------+\n",
            "|   U001|2024-01-05|  9999|\n",
            "|   U001|2024-01-10| 14999|\n",
            "|   U002|2024-01-06|  8999|\n",
            "|   U003|2024-01-07|     0|\n",
            "|   U004|2024-01-08|  7999|\n",
            "|   U001|2024-01-15|  1999|\n",
            "+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_spend_per_user = df.groupBy(\"user_id\").sum(\"amount\")\n",
        "total_spend_per_user.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQgZklnqCASm",
        "outputId": "35f34759-e18d-4e4c-cafd-e055ab610a0f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|user_id|sum(amount)|\n",
            "+-------+-----------+\n",
            "|   U002|       8999|\n",
            "|   U001|      26997|\n",
            "|   U004|       7999|\n",
            "|   U003|          0|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum, col\n",
        "\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(\"date\")\n",
        "running_spend_per_user = df.withColumn(\"running_spend\", sum(col(\"amount\")).over(window_spec))\n",
        "running_spend_per_user.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycJgsGXoCC_1",
        "outputId": "c70185e0-26b2-4197-ef89-17953d016476"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+-------------+\n",
            "|user_id|      date|amount|running_spend|\n",
            "+-------+----------+------+-------------+\n",
            "|   U001|2024-01-05|  9999|         9999|\n",
            "|   U001|2024-01-10| 14999|        24998|\n",
            "|   U001|2024-01-15|  1999|        26997|\n",
            "|   U002|2024-01-06|  8999|         8999|\n",
            "|   U003|2024-01-07|     0|            0|\n",
            "|   U004|2024-01-08|  7999|         7999|\n",
            "+-------+----------+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rank, desc\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "window_spec_rank = Window.orderBy(desc(\"sum(amount)\"))\n",
        "\n",
        "ranked_users_by_total_spend = total_spend_per_user.withColumn(\"rank\", rank().over(window_spec_rank))\n",
        "\n",
        "ranked_users_by_total_spend.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzBQyAmNCFbO",
        "outputId": "4d2b07e7-86a4-400d-be9c-92ac26062912"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+----+\n",
            "|user_id|sum(amount)|rank|\n",
            "+-------+-----------+----+\n",
            "|   U001|      26997|   1|\n",
            "|   U002|       8999|   2|\n",
            "|   U004|       7999|   3|\n",
            "|   U003|          0|   4|\n",
            "+-------+-----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing `GroupBy` and `Window` Outputs\n",
        "\n",
        "Both `groupBy` and `Window` functions are powerful tools in PySpark for data aggregation and analysis, but they serve different purposes and produce distinct outputs.\n",
        "\n",
        "#### 1. `GroupBy` Output: `total_spend_per_user`\n",
        "\n",
        "- **Purpose**: `GroupBy` operations are used for **aggregation**, where you want to collapse multiple rows into a single summary row based on one or more grouping keys. It answers questions like 'What is the total spend for each user?' or 'How many items did each category sell?'\n",
        "\n",
        "- **Output Characteristics**: The output of a `groupBy` operation typically has fewer rows than the input DataFrame, as it aggregates data based on the unique values of the grouping keys. For each group, it provides a single aggregated value (e.g., sum, count, average).\n",
        "\n",
        "- **Example Output (`total_spend_per_user`):**\n",
        "\n",
        "```\n",
        "\n",
        "+-------+-----------+\n",
        "\n",
        "|user_id|sum(amount)|\n",
        "\n",
        "+-------+-----------+\n",
        "\n",
        "|   U002|       8999|\n",
        "\n",
        "|   U001|      26997|\n",
        "\n",
        "|   U004|       7999|\n",
        "\n",
        "|   U003|          0|\n",
        "\n",
        "+-------+-----------+\n",
        "\n",
        "```\n",
        "\n",
        "Here, you get one row per `user_id` showing their total spend.\n",
        "\n",
        "#### 2. `Window` Output: `running_spend_per_user` and `ranked_users_by_total_spend`\n",
        "\n",
        "- **Purpose**: `Window` functions perform calculations across a set of DataFrame rows that are related to the current row. Unlike `groupBy`, `Window` functions **do not collapse rows**. Instead, they add new columns to the DataFrame, providing context-sensitive calculations (like running totals, moving averages, or rankings) for each original row.\n",
        "\n",
        "- **Output Characteristics**: The output of a `Window` function typically has the **same number of rows** as the input DataFrame. It adds one or more new columns containing the results of the window calculation for each row.\n",
        "\n",
        "##### a. Running Spend per User (`running_spend_per_user`)\n",
        "\n",
        "- **Type**: Cumulative aggregate within a partition.\n",
        "\n",
        "- **Output Characteristics**: For each transaction, it shows the cumulative spend up to that point for that specific user.\n",
        "\n",
        "- **Example Output (`running_spend_per_user`):**\n",
        "\n",
        "```\n",
        "\n",
        "+-------+----------+------+-------------+\n",
        "\n",
        "|user_id|      date|amount|running_spend|\n",
        "\n",
        "+-------+----------+------+-------------+\n",
        "\n",
        "|   U001|2024-01-05|  9999|         9999|\n",
        "\n",
        "|   U001|2024-01-10| 14999|        24998|\n",
        "\n",
        "|   U001|2024-01-15|  1999|        26997|\n",
        "\n",
        "|   U002|2024-01-06|  8999|         8999|\n",
        "\n",
        "|   U003|2024-01-07|     0|            0|\n",
        "\n",
        "|   U004|2024-01-08|  7999|         7999|\n",
        "\n",
        "+-------+----------+------+-------------+\n",
        "\n",
        "```\n",
        "\n",
        "Here, each row of the original `df` is preserved, and a new column `running_spend` is added, showing the sum of `amount` up to that date for each user.\n",
        "\n",
        "##### b. Ranked Users by Total Spend (`ranked_users_by_total_spend`)\n",
        "\n",
        "- **Type**: Ranking function applied to an aggregated DataFrame.\n",
        "\n",
        "- **Output Characteristics**: It assigns a rank to each user based on their total spend. It operates on the `total_spend_per_user` (an aggregated DataFrame) and adds a rank column, keeping one row per user.\n",
        "\n",
        "- **Example Output (`ranked_users_by_total_spend`):**\n",
        "\n",
        "```\n",
        "\n",
        "+-------+-----------+----+\n",
        "\n",
        "|user_id|sum(amount)|rank|\n",
        "\n",
        "+-------+-----------+----+\n",
        "\n",
        "|   U001|      26997|   1|\n",
        "\n",
        "|   U002|       8999|   2|\n",
        "\n",
        "|   U004|       7999|   3|\n",
        "\n",
        "|   U003|          0|   4|\n",
        "\n",
        "+-------+-----------+----+\n",
        "\n",
        "```\n",
        "\n",
        "This output shows the ranking of users based on the total spend, which was initially computed using `groupBy`.\n",
        "\n",
        "#### Key Differences Summarized:\n",
        "\n",
        "| Feature         | `GroupBy`                               | `Window` (e.g., `running_spend`)                  | `Window` (e.g., `rank` on aggregated data)      |\n",
        "\n",
        "| :-------------- | :-------------------------------------- | :------------------------------------------------ | :---------------------------------------------- |\n",
        "\n",
        "| **Row Count**   | Reduces rows (aggregates)               | Preserves original rows                           | Preserves rows of the input (often aggregated)  |\n",
        "\n",
        "| **Output**      | Summarized data per group               | Adds new column(s) with contextual calculations   | Adds new column(s) with ranking                 |\n",
        "\n",
        "| **Use Case**    | Overall aggregates (total, count, avg)  | Running totals, moving averages, row comparisons  | Ranking within partitions or across the whole DF|\n",
        "\n",
        "In essence, `groupBy` is about summarization and reducing granularity, while `Window` functions are about enriching the existing data with new calculations that consider a defined 'window' of related rows.\n",
        ""
      ],
      "metadata": {
        "id": "c1gazaThChIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 6 — PARTITIONS & PERFORMANCE"
      ],
      "metadata": {
        "id": "h3dvYB_vDFCB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aff72d42",
        "outputId": "3065cf82-16f2-4405-9efe-0782c69b2d3f"
      },
      "source": [
        "dataframes_to_check = {\n",
        "    \"users_df\": users_df,\n",
        "    \"courses_df\": courses_df,\n",
        "    \"activity_df\": activity_df,\n",
        "    \"df_enrollments_processed\": df_enrollments_processed,\n",
        "    \"df_enriched\": df_enriched,\n",
        "    \"df\": df,\n",
        "    \"total_spend_per_user\": total_spend_per_user,\n",
        "    \"running_spend_per_user\": running_spend_per_user,\n",
        "    \"ranked_users_by_total_spend\": ranked_users_by_total_spend\n",
        "}\n",
        "\n",
        "print(\"--- Number of Partitions for DataFrames ---\")\n",
        "for df_name, df_obj in dataframes_to_check.items():\n",
        "    print(f\"DataFrame: {df_name}, Partitions: {df_obj.rdd.getNumPartitions()}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Number of Partitions for DataFrames ---\n",
            "DataFrame: users_df, Partitions: 2\n",
            "DataFrame: courses_df, Partitions: 2\n",
            "DataFrame: activity_df, Partitions: 2\n",
            "DataFrame: df_enrollments_processed, Partitions: 2\n",
            "DataFrame: df_enriched, Partitions: 2\n",
            "DataFrame: df, Partitions: 2\n",
            "DataFrame: total_spend_per_user, Partitions: 1\n",
            "DataFrame: running_spend_per_user, Partitions: 1\n",
            "DataFrame: ranked_users_by_total_spend, Partitions: 1\n",
            "-------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d37793c9",
        "outputId": "050acc55-a088-4401-a56e-1ee333a33948"
      },
      "source": [
        "df_enrollments_repartitioned = df_enrollments_processed.repartition('course_id')\n",
        "\n",
        "print(f\"Original df_enrollments_processed partitions: {df_enrollments_processed.rdd.getNumPartitions()}\")\n",
        "print(f\"Repartitioned df_enrollments_repartitioned partitions: {df_enrollments_repartitioned.rdd.getNumPartitions()}\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original df_enrollments_processed partitions: 2\n",
            "Repartitioned df_enrollments_repartitioned partitions: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30c8d568",
        "outputId": "b539e413-65a6-4f4d-b2db-3b1bacd9aead"
      },
      "source": [
        "df_enrollments_coalesced = df_enrollments_repartitioned.coalesce(1)\n",
        "\n",
        "print(f\"Coalesced df_enrollments_coalesced partitions: {df_enrollments_coalesced.rdd.getNumPartitions()}\")\n",
        "\n",
        "output_path = \"/tmp/enrollments_single_partition\"\n",
        "\n",
        "df_enrollments_coalesced.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"Coalesced DataFrame written to {output_path} in Parquet format.\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coalesced df_enrollments_coalesced partitions: 1\n",
            "Coalesced DataFrame written to /tmp/enrollments_single_partition in Parquet format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A repartition operation in Apache Spark causes a shuffle because it fundamentally changes how data is distributed across the partitions in your cluster. Here's a breakdown of why and its performance implications:\n",
        "\n",
        "# Why repartition causes a shuffle:\n",
        "\n",
        "# 1. Redistribution of Data: When you call repartition(), especially on a specific column (like repartition('course_id')), Spark needs to ensure that all rows with the same value for that column (e.g., all rows for 'C001') end up on the same partition. This often means data needs to be moved from its current location on one executor to a different executor and partition.\n",
        "# 2. Network Transfer: To achieve this redistribution, data must be serialized, sent over the network, and then deserialized on the destination executor. This cross-network data movement is the core of what a \"shuffle\" is.\n",
        "# 3. Intermediate Storage: During a shuffle, Spark might temporarily write data to local disk on the executors before it's sent to the final destination. This adds disk I/O to the process.\n",
        "\n",
        "# Performance Implications of a Shuffle:\n",
        "\n",
        "# * High Cost: Shuffles are generally the most expensive operations in Spark jobs. They are resource-intensive due to the significant network, disk I/O, and CPU usage.\n",
        "# * Network Bandwidth: Moving large amounts of data across the network can saturate network links, leading to slow performance.\n",
        "# * Disk I/O: If the data being shuffled is too large to fit in memory, Spark will spill it to disk, incurring additional disk read/write overhead.\n",
        "# * CPU Overhead: Data needs to be serialized before being sent over the network and deserialized upon arrival, consuming valuable CPU cycles.\n",
        "# * Memory Usage: Executors require memory to buffer shuffled data, and if this memory is insufficient, it can lead to out-of-memory errors or frequent spills to disk.\n",
        "# * Garbage Collection: Heavy shuffling can increase garbage collection activity on the JVMs of the executors, further impacting performance.\n",
        "# * Bottlenecks: Data skew (where a few partition keys have a disproportionately large amount of data) can exacerbate shuffle performance issues, creating bottlenecks at specific executors.\n",
        "\n",
        "# In your case, when you repartitioned df_enrollments_processed by 'course_id', Spark had to collect all rows, group them by course_id, and then distribute them to new partitions based on that grouping. Since the original partitions weren't necessarily organized by course_id, a shuffle was necessary to achieve the desired distribution, even resulting in a reduced partition count for the repartitioned DataFrame in this specific instance. This is why df_enriched.explain(True) showed BroadcastHashJoin, which avoids shuffling the smaller courses_df but still relies on proper partitioning for the larger df_enrollments_processed to optimize the join."
      ],
      "metadata": {
        "id": "IBvEGlfkESOX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 7 — DAG & OPTIMIZATION"
      ],
      "metadata": {
        "id": "V-y_HF1ZE5qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xv9DnB0cEyg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d211eb9",
        "outputId": "f4c3382b-8c9f-4a64-8753-ad1c4d59a6b7"
      },
      "source": [
        "print(\"\\n--- Execution Plan for df_users_age_normalized ---\")\n",
        "df_users_age_normalized.explain(True)\n",
        "\n",
        "print(\"\\n--- Execution Plan for df_users_skills_normalized ---\")\n",
        "df_users_skills_normalized.explain(True)\n",
        "\n",
        "print(\"\\n--- Execution Plan for users_df (Final) ---\")\n",
        "users_df.explain(True)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Execution Plan for df_users_age_normalized ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(age, CASE WHEN 'rlike('age, ^[0-9]+$) THEN cast('age as int) ELSE null END, None)]\n",
            "+- LogicalRDD [user_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, name: string, age: int, city: string, interests: string\n",
            "Project [user_id#0, name#1, CASE WHEN RLIKE(age#2, ^[0-9]+$) THEN cast(age#2 as int) ELSE cast(null as int) END AS age#38, city#3, interests#4]\n",
            "+- LogicalRDD [user_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#0, name#1, CASE WHEN RLIKE(age#2, ^[0-9]+$) THEN cast(age#2 as int) END AS age#38, city#3, interests#4]\n",
            "+- LogicalRDD [user_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [user_id#0, name#1, CASE WHEN RLIKE(age#2, ^[0-9]+$) THEN cast(age#2 as int) END AS age#38, city#3, interests#4]\n",
            "+- *(1) Scan ExistingRDD[user_id#0,name#1,age#2,city#3,interests#4]\n",
            "\n",
            "\n",
            "--- Execution Plan for df_users_skills_normalized ---\n",
            "== Parsed Logical Plan ==\n",
            "Project [user_id#0, name#1, age#38, city#3, interests#57]\n",
            "+- Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(interests#56) THEN filter(transform(interests#56, lambdafunction(trim(lambda x#58, None), lambda x#58, false)), lambdafunction(NOT (lambda x#59 = ), lambda x#59, false)) ELSE cast(null as array<string>) END AS interests#57, cleaned_interests_str#55]\n",
            "   +- Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(cleaned_interests_str#55) THEN split(cleaned_interests_str#55, ,, -1) ELSE cast(null as array<string>) END AS interests#56, cleaned_interests_str#55]\n",
            "      +- Project [user_id#0, name#1, age#38, city#3, interests#4, CASE WHEN isnotnull(interests#4) THEN regexp_replace(regexp_replace(interests#4, ^\\[|\\]$, , 1), ', , 1) ELSE cast(null as string) END AS cleaned_interests_str#55]\n",
            "         +- Project [user_id#0, name#1, CASE WHEN RLIKE(age#2, ^[0-9]+$) THEN cast(age#2 as int) ELSE cast(null as int) END AS age#38, city#3, interests#4]\n",
            "            +- LogicalRDD [user_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, name: string, age: int, city: string, interests: array<string>\n",
            "Project [user_id#0, name#1, age#38, city#3, interests#57]\n",
            "+- Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(interests#56) THEN filter(transform(interests#56, lambdafunction(trim(lambda x#58, None), lambda x#58, false)), lambdafunction(NOT (lambda x#59 = ), lambda x#59, false)) ELSE cast(null as array<string>) END AS interests#57, cleaned_interests_str#55]\n",
            "   +- Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(cleaned_interests_str#55) THEN split(cleaned_interests_str#55, ,, -1) ELSE cast(null as array<string>) END AS interests#56, cleaned_interests_str#55]\n",
            "      +- Project [user_id#0, name#1, age#38, city#3, interests#4, CASE WHEN isnotnull(interests#4) THEN regexp_replace(regexp_replace(interests#4, ^\\[|\\]$, , 1), ', , 1) ELSE cast(null as string) END AS cleaned_interests_str#55]\n",
            "         +- Project [user_id#0, name#1, CASE WHEN RLIKE(age#2, ^[0-9]+$) THEN cast(age#2 as int) ELSE cast(null as int) END AS age#38, city#3, interests#4]\n",
            "            +- LogicalRDD [user_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(interests#56) THEN filter(transform(interests#56, lambdafunction(trim(lambda x#58, None), lambda x#58, false)), lambdafunction(NOT (lambda x#59 = ), lambda x#59, false)) END AS interests#57]\n",
            "+- Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(cleaned_interests_str#55) THEN split(cleaned_interests_str#55, ,, -1) END AS interests#56]\n",
            "   +- Project [user_id#0, name#1, CASE WHEN RLIKE(age#2, ^[0-9]+$) THEN cast(age#2 as int) END AS age#38, city#3, CASE WHEN isnotnull(interests#4) THEN regexp_replace(regexp_replace(interests#4, ^\\[|\\]$, , 1), ', , 1) END AS cleaned_interests_str#55]\n",
            "      +- LogicalRDD [user_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Physical Plan ==\n",
            "Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(interests#56) THEN filter(transform(interests#56, lambdafunction(trim(lambda x#58, None), lambda x#58, false)), lambdafunction(NOT (lambda x#59 = ), lambda x#59, false)) END AS interests#57]\n",
            "+- *(1) Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(cleaned_interests_str#55) THEN split(cleaned_interests_str#55, ,, -1) END AS interests#56]\n",
            "   +- *(1) Project [user_id#0, name#1, CASE WHEN RLIKE(age#2, ^[0-9]+$) THEN cast(age#2 as int) END AS age#38, city#3, CASE WHEN isnotnull(interests#4) THEN regexp_replace(regexp_replace(interests#4, ^\\[|\\]$, , 1), ', , 1) END AS cleaned_interests_str#55]\n",
            "      +- *(1) Scan ExistingRDD[user_id#0,name#1,age#2,city#3,interests#4]\n",
            "\n",
            "\n",
            "--- Execution Plan for users_df (Final) ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(name, CASE WHEN 'or('isNull('name), '`=`('name, )) THEN Unknown ELSE 'name END, None)]\n",
            "+- Project [user_id#0, name#1, age#38, city#3, interests#57]\n",
            "   +- Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(interests#56) THEN filter(transform(interests#56, lambdafunction(trim(lambda x#58, None), lambda x#58, false)), lambdafunction(NOT (lambda x#59 = ), lambda x#59, false)) ELSE cast(null as array<string>) END AS interests#57, cleaned_interests_str#55]\n",
            "      +- Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(cleaned_interests_str#55) THEN split(cleaned_interests_str#55, ,, -1) ELSE cast(null as array<string>) END AS interests#56, cleaned_interests_str#55]\n",
            "         +- Project [user_id#0, name#1, age#38, city#3, interests#4, CASE WHEN isnotnull(interests#4) THEN regexp_replace(regexp_replace(interests#4, ^\\[|\\]$, , 1), ', , 1) ELSE cast(null as string) END AS cleaned_interests_str#55]\n",
            "            +- Project [user_id#0, name#1, CASE WHEN RLIKE(age#2, ^[0-9]+$) THEN cast(age#2 as int) ELSE cast(null as int) END AS age#38, city#3, interests#4]\n",
            "               +- LogicalRDD [user_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, name: string, age: int, city: string, interests: array<string>\n",
            "Project [user_id#0, CASE WHEN (isnull(name#1) OR (name#1 = )) THEN Unknown ELSE name#1 END AS name#76, age#38, city#3, interests#57]\n",
            "+- Project [user_id#0, name#1, age#38, city#3, interests#57]\n",
            "   +- Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(interests#56) THEN filter(transform(interests#56, lambdafunction(trim(lambda x#58, None), lambda x#58, false)), lambdafunction(NOT (lambda x#59 = ), lambda x#59, false)) ELSE cast(null as array<string>) END AS interests#57, cleaned_interests_str#55]\n",
            "      +- Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(cleaned_interests_str#55) THEN split(cleaned_interests_str#55, ,, -1) ELSE cast(null as array<string>) END AS interests#56, cleaned_interests_str#55]\n",
            "         +- Project [user_id#0, name#1, age#38, city#3, interests#4, CASE WHEN isnotnull(interests#4) THEN regexp_replace(regexp_replace(interests#4, ^\\[|\\]$, , 1), ', , 1) ELSE cast(null as string) END AS cleaned_interests_str#55]\n",
            "            +- Project [user_id#0, name#1, CASE WHEN RLIKE(age#2, ^[0-9]+$) THEN cast(age#2 as int) ELSE cast(null as int) END AS age#38, city#3, interests#4]\n",
            "               +- LogicalRDD [user_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#0, CASE WHEN (isnull(name#1) OR (name#1 = )) THEN Unknown ELSE name#1 END AS name#76, age#38, city#3, CASE WHEN isnotnull(interests#56) THEN filter(transform(interests#56, lambdafunction(trim(lambda x#58, None), lambda x#58, false)), lambdafunction(NOT (lambda x#59 = ), lambda x#59, false)) END AS interests#57]\n",
            "+- Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(cleaned_interests_str#55) THEN split(cleaned_interests_str#55, ,, -1) END AS interests#56]\n",
            "   +- Project [user_id#0, name#1, CASE WHEN RLIKE(age#2, ^[0-9]+$) THEN cast(age#2 as int) END AS age#38, city#3, CASE WHEN isnotnull(interests#4) THEN regexp_replace(regexp_replace(interests#4, ^\\[|\\]$, , 1), ', , 1) END AS cleaned_interests_str#55]\n",
            "      +- LogicalRDD [user_id#0, name#1, age#2, city#3, interests#4], false\n",
            "\n",
            "== Physical Plan ==\n",
            "Project [user_id#0, CASE WHEN (isnull(name#1) OR (name#1 = )) THEN Unknown ELSE name#1 END AS name#76, age#38, city#3, CASE WHEN isnotnull(interests#56) THEN filter(transform(interests#56, lambdafunction(trim(lambda x#58, None), lambda x#58, false)), lambdafunction(NOT (lambda x#59 = ), lambda x#59, false)) END AS interests#57]\n",
            "+- *(1) Project [user_id#0, name#1, age#38, city#3, CASE WHEN isnotnull(cleaned_interests_str#55) THEN split(cleaned_interests_str#55, ,, -1) END AS interests#56]\n",
            "   +- *(1) Project [user_id#0, name#1, CASE WHEN RLIKE(age#2, ^[0-9]+$) THEN cast(age#2 as int) END AS age#38, city#3, CASE WHEN isnotnull(interests#4) THEN regexp_replace(regexp_replace(interests#4, ^\\[|\\]$, , 1), ', , 1) END AS cleaned_interests_str#55]\n",
            "      +- *(1) Scan ExistingRDD[user_id#0,name#1,age#2,city#3,interests#4]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07264e62",
        "outputId": "6b538f2c-b51d-4488-d887-790e5443ca44"
      },
      "source": [
        "print(\"\\n--- Execution Plan for df_courses_normalized ---\")\n",
        "df_courses_normalized.explain(True)\n",
        "\n",
        "print(\"\\n--- Execution Plan for df_courses_final ---\")\n",
        "df_courses_final.explain(True)\n",
        "\n",
        "print(\"\\n--- Execution Plan for courses_df (Final) ---\")\n",
        "courses_df.explain(True)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Execution Plan for df_courses_normalized ---\n",
            "== Parsed Logical Plan ==\n",
            "LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, title: string, metadata: struct<domain:string,level:string>, price: string\n",
            "LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Scan ExistingRDD[course_id#109,title#110,metadata#111,price#112]\n",
            "\n",
            "\n",
            "--- Execution Plan for df_courses_final ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(price, CASE WHEN 'isNull('price) THEN 0 ELSE 'price END, None)]\n",
            "+- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) ELSE cast(null as int) END AS price#126]\n",
            "   +- LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, title: string, metadata: struct<domain:string,level:string>, price: int\n",
            "Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "+- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) ELSE cast(null as int) END AS price#126]\n",
            "   +- LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "+- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) END AS price#126]\n",
            "   +- LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "+- *(1) Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) END AS price#126]\n",
            "   +- *(1) Scan ExistingRDD[course_id#109,title#110,metadata#111,price#112]\n",
            "\n",
            "\n",
            "--- Execution Plan for courses_df (Final) ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(price, CASE WHEN 'isNull('price) THEN 0 ELSE 'price END, None)]\n",
            "+- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) ELSE cast(null as int) END AS price#126]\n",
            "   +- LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, title: string, metadata: struct<domain:string,level:string>, price: int\n",
            "Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "+- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) ELSE cast(null as int) END AS price#126]\n",
            "   +- LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "+- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) END AS price#126]\n",
            "   +- LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "+- *(1) Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) END AS price#126]\n",
            "   +- *(1) Scan ExistingRDD[course_id#109,title#110,metadata#111,price#112]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af4116cf",
        "outputId": "eafbb2c9-1bed-4d9a-8bea-84d8a4cf16d8"
      },
      "source": [
        "print(\"\\n--- Execution Plan for df_activity_normalized ---\")\n",
        "df_activity_normalized.explain(True)\n",
        "\n",
        "print(\"\\n--- Execution Plan for activity_df (Final) ---\")\n",
        "activity_df.explain(True)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Execution Plan for df_activity_normalized ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(actions, CASE WHEN 'isNull('actions) THEN 'array() ELSE 'actions END, None)]\n",
            "+- Project [user_id#167, actions#168, CASE WHEN isnull(properties#169) THEN cast(null as map<string,string>) WHEN StartsWith(properties#169, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#169, ', \", 1), Some(Etc/UTC), false) ELSE parse_custom_properties(properties#169)#184 END AS properties#185, duration#170]\n",
            "   +- LogicalRDD [user_id#167, actions#168, properties#169, duration#170], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, actions: array<string>, properties: map<string,string>, duration: int\n",
            "Project [user_id#167, CASE WHEN isnull(actions#168) THEN cast(array() as array<string>) ELSE actions#168 END AS actions#200, properties#185, duration#170]\n",
            "+- Project [user_id#167, actions#168, CASE WHEN isnull(properties#169) THEN cast(null as map<string,string>) WHEN StartsWith(properties#169, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#169, ', \", 1), Some(Etc/UTC), false) ELSE parse_custom_properties(properties#169)#184 END AS properties#185, duration#170]\n",
            "   +- LogicalRDD [user_id#167, actions#168, properties#169, duration#170], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#167, CASE WHEN isnull(actions#168) THEN [] ELSE actions#168 END AS actions#200, CASE WHEN isnull(properties#169) THEN null WHEN StartsWith(properties#169, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#169, ', \", 1), Some(Etc/UTC), false) ELSE pythonUDF0#385 END AS properties#185, duration#170]\n",
            "+- BatchEvalPython [parse_custom_properties(properties#169)#184], [pythonUDF0#385]\n",
            "   +- LogicalRDD [user_id#167, actions#168, properties#169, duration#170], false\n",
            "\n",
            "== Physical Plan ==\n",
            "Project [user_id#167, CASE WHEN isnull(actions#168) THEN [] ELSE actions#168 END AS actions#200, CASE WHEN isnull(properties#169) THEN null WHEN StartsWith(properties#169, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#169, ', \", 1), Some(Etc/UTC), false) ELSE pythonUDF0#385 END AS properties#185, duration#170]\n",
            "+- BatchEvalPython [parse_custom_properties(properties#169)#184], [pythonUDF0#385]\n",
            "   +- *(1) Scan ExistingRDD[user_id#167,actions#168,properties#169,duration#170]\n",
            "\n",
            "\n",
            "--- Execution Plan for activity_df (Final) ---\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(actions, CASE WHEN 'isNull('actions) THEN 'array() ELSE 'actions END, None)]\n",
            "+- Project [user_id#167, actions#168, CASE WHEN isnull(properties#169) THEN cast(null as map<string,string>) WHEN StartsWith(properties#169, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#169, ', \", 1), Some(Etc/UTC), false) ELSE parse_custom_properties(properties#169)#184 END AS properties#185, duration#170]\n",
            "   +- LogicalRDD [user_id#167, actions#168, properties#169, duration#170], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "user_id: string, actions: array<string>, properties: map<string,string>, duration: int\n",
            "Project [user_id#167, CASE WHEN isnull(actions#168) THEN cast(array() as array<string>) ELSE actions#168 END AS actions#200, properties#185, duration#170]\n",
            "+- Project [user_id#167, actions#168, CASE WHEN isnull(properties#169) THEN cast(null as map<string,string>) WHEN StartsWith(properties#169, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#169, ', \", 1), Some(Etc/UTC), false) ELSE parse_custom_properties(properties#169)#184 END AS properties#185, duration#170]\n",
            "   +- LogicalRDD [user_id#167, actions#168, properties#169, duration#170], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [user_id#167, CASE WHEN isnull(actions#168) THEN [] ELSE actions#168 END AS actions#200, CASE WHEN isnull(properties#169) THEN null WHEN StartsWith(properties#169, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#169, ', \", 1), Some(Etc/UTC), false) ELSE pythonUDF0#385 END AS properties#185, duration#170]\n",
            "+- BatchEvalPython [parse_custom_properties(properties#169)#184], [pythonUDF0#385]\n",
            "   +- LogicalRDD [user_id#167, actions#168, properties#169, duration#170], false\n",
            "\n",
            "== Physical Plan ==\n",
            "Project [user_id#167, CASE WHEN isnull(actions#168) THEN [] ELSE actions#168 END AS actions#200, CASE WHEN isnull(properties#169) THEN null WHEN StartsWith(properties#169, {) THEN from_json(MapType(StringType,StringType,true), regexp_replace(properties#169, ', \", 1), Some(Etc/UTC), false) ELSE pythonUDF0#385 END AS properties#185, duration#170]\n",
            "+- BatchEvalPython [parse_custom_properties(properties#169)#184], [pythonUDF0#385]\n",
            "   +- *(1) Scan ExistingRDD[user_id#167,actions#168,properties#169,duration#170]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b29f5e00",
        "outputId": "c6f64339-9403-4e32-f2c1-2063114da09d"
      },
      "source": [
        "print(\"\\n--- Execution Plan for df_enriched (Joined Enrollments and Courses) ---\")\n",
        "df_enriched.explain(True)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Execution Plan for df_enriched (Joined Enrollments and Courses) ---\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [course_id])\n",
            ":- Project [user_id#245, course_id#246, enrollment_date#258]\n",
            ":  +- Project [user_id#245, course_id#246, enrollment_date_raw#247, coalesce(CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#247, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#247, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#247, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#258]\n",
            ":     +- LogicalRDD [user_id#245, course_id#246, enrollment_date_raw#247], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "      +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) ELSE cast(null as int) END AS price#126]\n",
            "         +- LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, user_id: string, enrollment_date: date, title: string, metadata: struct<domain:string,level:string>, price: int\n",
            "Project [course_id#246, user_id#245, enrollment_date#258, title#110, metadata#111, price#140]\n",
            "+- Join LeftOuter, (course_id#246 = course_id#109)\n",
            "   :- Project [user_id#245, course_id#246, enrollment_date#258]\n",
            "   :  +- Project [user_id#245, course_id#246, enrollment_date_raw#247, coalesce(CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#247, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#247, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#247, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#258]\n",
            "   :     +- LogicalRDD [user_id#245, course_id#246, enrollment_date_raw#247], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "         +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) ELSE cast(null as int) END AS price#126]\n",
            "            +- LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [course_id#246, user_id#245, enrollment_date#258, title#110, metadata#111, price#140]\n",
            "+- Join LeftOuter, (course_id#246 = course_id#109), rightHint=(strategy=broadcast)\n",
            "   :- Project [user_id#245, course_id#246, coalesce(CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#247, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#247, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#247, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#258]\n",
            "   :  +- LogicalRDD [user_id#245, course_id#246, enrollment_date_raw#247], false\n",
            "   +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "      +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) END AS price#126]\n",
            "         +- Filter isnotnull(course_id#109)\n",
            "            +- LogicalRDD [course_id#109, title#110, metadata#111, price#112], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 1\n",
            "   +- *(2) Project [course_id#246, user_id#245, enrollment_date#258, title#110, metadata#111, price#140]\n",
            "      +- *(2) BroadcastHashJoin [course_id#246], [course_id#109], LeftOuter, BuildRight, false\n",
            "         :- *(2) Project [user_id#245, course_id#246, coalesce(CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#247, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#247, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#247, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#258]\n",
            "         :  +- *(2) Scan ExistingRDD[user_id#245,course_id#246,enrollment_date_raw#247]\n",
            "         +- BroadcastQueryStage 0\n",
            "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=803]\n",
            "               +- *(1) Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "                  +- *(1) Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) END AS price#126]\n",
            "                     +- *(1) Filter isnotnull(course_id#109)\n",
            "                        +- *(1) Scan ExistingRDD[course_id#109,title#110,metadata#111,price#112]\n",
            "+- == Initial Plan ==\n",
            "   Project [course_id#246, user_id#245, enrollment_date#258, title#110, metadata#111, price#140]\n",
            "   +- BroadcastHashJoin [course_id#246], [course_id#109], LeftOuter, BuildRight, false\n",
            "      :- Project [user_id#245, course_id#246, coalesce(CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#247, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#247, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#247, ^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#247, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#258]\n",
            "      :  +- Scan ExistingRDD[user_id#245,course_id#246,enrollment_date_raw#247]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=528]\n",
            "         +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnull(price#126) THEN 0 ELSE price#126 END AS price#140]\n",
            "            +- Project [course_id#109, title#110, metadata#111, CASE WHEN isnotnull(price#112) THEN cast(regexp_replace(price#112, ₹, , 1) as int) END AS price#126]\n",
            "               +- Filter isnotnull(course_id#109)\n",
            "                  +- Scan ExistingRDD[course_id#109,title#110,metadata#111,price#112]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 3. Bad DAG identified in `ranked_users_by_total_spend`:\n",
        "# The physical plan for `ranked_users_by_total_spend` includes an `Exchange SinglePartition` followed by a global `Sort`.\n",
        "# This means that after computing the total spend per user (which already involves a shuffle for aggregation),\n",
        "# Spark then gathers ALL the aggregated data into a single partition (`Exchange SinglePartition`) to perform a global sort (`Sort`).\n",
        "# This design choice, while correct for achieving a global rank, is highly inefficient and becomes a major bottleneck\n",
        "# for large datasets as it eliminates parallelism and forces all data processing onto a single executor."
      ],
      "metadata": {
        "id": "5XSVardWGWFP"
      }
    }
  ]
}