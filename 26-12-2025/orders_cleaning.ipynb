{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('Orders Cleaning').getOrCreate()\n"
      ],
      "metadata": {
        "id": "7OTORCvs4AJ6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 1 — INGESTION & FIRST INSPECTION"
      ],
      "metadata": {
        "id": "oyQWP_Wl4VV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Read the CSV file into a DataFrame"
      ],
      "metadata": {
        "id": "Pn5h_qz27DI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv=spark.read.option('header','true').csv('orders_large_bad.csv')"
      ],
      "metadata": {
        "id": "ND8qydN85fTf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Disable schema inference and read everything as string"
      ],
      "metadata": {
        "id": "3fG7l81N7dIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv_raw=spark.read.option('header','true').option('inferSchema','false').csv('orders_large_bad.csv')\n",
        "df_csv_raw.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fymKQ6pM4RzX",
        "outputId": "6a292eab-9425-4ac7-ffe4-c1df6d31abe1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "|   order_id|customer_id|       city|   category|    product| amount|order_date|   status|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "|ORD00000000|    C000000| hyderabad |   grocery |       Oil |invalid|01/01/2024|Cancelled|\n",
            "|ORD00000001|    C000001|       Pune|    Grocery|      Sugar|  35430|2024-01-02|Completed|\n",
            "|ORD00000002|    C000002|       Pune|Electronics|     Mobile|  65358|2024-01-03|Completed|\n",
            "|ORD00000003|    C000003|  Bangalore|Electronics|     Laptop|   5558|2024-01-04|Completed|\n",
            "|ORD00000004|    C000004|       Pune|       Home|AirPurifier|  33659|2024-01-05|Completed|\n",
            "|ORD00000005|    C000005|      Delhi|    Fashion|      Jeans|   8521|2024-01-06|Completed|\n",
            "|ORD00000006|    C000006|      Delhi|    Grocery|      Sugar|  42383|2024-01-07|Completed|\n",
            "|ORD00000007|    C000007|       Pune|    Grocery|       Rice|  45362|2024-01-08|Completed|\n",
            "|ORD00000008|    C000008|  Bangalore|    Fashion|      Jeans|  10563|2024-01-09|Completed|\n",
            "|ORD00000009|    C000009|    Kolkata|Electronics|     Laptop|  63715|2024-01-10|Completed|\n",
            "|ORD00000010|    C000010|  Bangalore|    Grocery|      Sugar|  66576|2024-01-11|Completed|\n",
            "|ORD00000011|    C000011|    Kolkata|Electronics|     Tablet|  50318|12/01/2024|Completed|\n",
            "|ORD00000012|    C000012|  Bangalore|    Grocery|      Sugar|  84768|2024-01-13|Completed|\n",
            "|ORD00000013|    C000013|       Pune|    Fashion|     TShirt|  79121|2024/01/14|Completed|\n",
            "|ORD00000014|    C000014|     Mumbai|Electronics|     Tablet|  79469|2024-01-15|Completed|\n",
            "|ORD00000015|    C000015|       Pune|Electronics|     Mobile|  81018|2024-01-16|Completed|\n",
            "|ORD00000016|    C000016|     Mumbai|       Home|      Mixer|  64225|2024-01-17|Completed|\n",
            "|ORD00000017|    C000017| bangalore |    Grocery|        Oil|  69582|2024-01-18|Completed|\n",
            "|ORD00000018|    C000018|    Kolkata|    Fashion|      Jeans|  50424|2024-01-19|Completed|\n",
            "|ORD00000019|    C000019|     Mumbai|Electronics|     Mobile|invalid|2024-01-20|Completed|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Print schema and record count"
      ],
      "metadata": {
        "id": "WAx0eCtW8Ilo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv_raw.printSchema()\n",
        "df_csv_raw.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkfcmnR-8JF8",
        "outputId": "e34fcc21-d3ad-42ad-93a0-18ad63c1b966"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Display 20 random rows"
      ],
      "metadata": {
        "id": "_EZVUx-C8cIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv_raw.orderBy(\"order_id\").sample(0.01).show(20,False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXltDU-f8M3q",
        "outputId": "ef8c3229-b43c-4864-f3f6-140be5b2ff88"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+-----------+-------+-------+----------+---------+\n",
            "|order_id   |customer_id|city       |category   |product|amount |order_date|status   |\n",
            "+-----------+-----------+-----------+-----------+-------+-------+----------+---------+\n",
            "|ORD00000063|C000063    |Hyderabad  |Grocery    |Sugar  |23023  |2024-01-04|Completed|\n",
            "|ORD00000133|C000133    |Kolkata    |Home       |Vacuum |invalid|2024-01-14|Completed|\n",
            "|ORD00000169|C000169    |Mumbai     |Grocery    |Sugar  |52565  |2024/02/19|Completed|\n",
            "|ORD00000188|C000188    |Bangalore  |Electronics|Tablet |82780  |2024-01-09|Completed|\n",
            "|ORD00000282|C000282    |Pune       |Grocery    |Rice   |70336  |2024-02-12|Completed|\n",
            "|ORD00000381|C000381    |Delhi      |Fashion    |Jeans  |79839  |2024-01-22|Completed|\n",
            "|ORD00000417|C000417    |Delhi      |Home       |Mixer  |23287  |2024-02-27|Completed|\n",
            "|ORD00000423|C000423    |Chennai    |Home       |Mixer  |88763  |2024-01-04|Completed|\n",
            "|ORD00000466|C000466    |Hyderabad  |Grocery    |Sugar  |26207  |2024-02-16|Completed|\n",
            "|ORD00000566|C000566    |Mumbai     |Electronics|Mobile |52783  |2024-01-27|Completed|\n",
            "|ORD00000730|C000730    |Kolkata    |Fashion    |TShirt |34805  |2024-01-11|Completed|\n",
            "|ORD00000825|C000825    |Mumbai     |Grocery    |Sugar  |27206  |15/02/2024|Completed|\n",
            "|ORD00000934|C000934    |Hyderabad  |Home       |Vacuum |24848  |2024-02-04|Completed|\n",
            "|ORD00000935|C000935    | hyderabad |Fashion    |Shoes  |38991  |05/02/2024|Completed|\n",
            "|ORD00000983|C000983    |Chennai    |Fashion    |TShirt |30277  |2024-01-24|Completed|\n",
            "|ORD00001050|C001050    |Mumbai     |Electronics|Mobile |27620  |2024-01-31|Completed|\n",
            "|ORD00001077|C001077    |Delhi      |Home       |Vacuum |84180  |2024-02-27|Completed|\n",
            "|ORD00001084|C001084    |Hyderabad  |Home       |Mixer  |47382  |2024-01-05|Completed|\n",
            "|ORD00001085|C001085    |Kolkata    | home      |Vacuum |22659  |2024-01-06|Completed|\n",
            "|ORD00001562|C001562    |Chennai    |Electronics|Mobile |7354   |03/01/2024|Completed|\n",
            "+-----------+-----------+-----------+-----------+-------+-------+----------+---------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Identify at least 5 data quality issues by observation"
      ],
      "metadata": {
        "id": "ffSzIhpH85WR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Leading/trailing spaces in city, category, product.\n",
        "\n",
        "2. Mixed case values(Electronics,electronics)\n",
        "\n",
        "3. Invalid amount values(null,\"\",invalid, 12,000)\n",
        "\n",
        "4. Multiple date formats\n",
        "\n",
        "5. Duplicate order_id\n",
        "\n",
        "6. Inconsistent status values"
      ],
      "metadata": {
        "id": "ijFezj6P9C5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Read the JSON file and compare schema and row count with CSV"
      ],
      "metadata": {
        "id": "SkITPWb_9oXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_json=spark.read.option(\"multiline\", True).json(\"orders_large_bad.json\")\n",
        "df_json.printSchema()\n",
        "df_json.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4saAqpp8s22",
        "outputId": "03356a79-e808-4f25-d8bd-06c44ea13462"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- amount: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 2 — SCHEMA ENFORCEMENT & VALIDATION"
      ],
      "metadata": {
        "id": "V2k_pz5--Nqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Define an explicit schema using StructType"
      ],
      "metadata": {
        "id": "p9y-F08X-Xdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType"
      ],
      "metadata": {
        "id": "FFeaj5AH9_vC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema=StructType([\n",
        "    StructField(\"order_id\",StringType(),False),\n",
        "    StructField(\"customer_id\",StringType(),True),\n",
        "    StructField(\"city\",StringType(),True),\n",
        "    StructField(\"category\",StringType(),True),\n",
        "    StructField(\"product\",StringType(),True),\n",
        "    StructField(\"amount\",StringType(),True),\n",
        "    StructField(\"order_date\",StringType(),True),\n",
        "    StructField(\"status\",StringType(),True)\n",
        "])"
      ],
      "metadata": {
        "id": "uods2I-f-pbn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Re-read the CSV using the defined schema"
      ],
      "metadata": {
        "id": "yRmKXVWa_E6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=spark.read.option('header','True').schema(schema).csv('orders_large_bad.csv')\n"
      ],
      "metadata": {
        "id": "bWwOnqIq-3MX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Identify rows that fail schema expectations"
      ],
      "metadata": {
        "id": "hMafPkPZ_U-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.order_id.isNull()).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXQFOh5F_Vhv",
        "outputId": "4149ca5a-47ef-4544-cc01-1e8ec6fe3f36"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Explain why schema inference is dangerous at scale"
      ],
      "metadata": {
        "id": "V5B8Crv0_5cJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Different partitions infer different types.\n",
        "\n",
        "2. Breaks downstream jobs.\n",
        "\n",
        "3. Expensive full scans\n",
        "\n",
        "4. Non-deterministic schemas at scale"
      ],
      "metadata": {
        "id": "fTbTZIiG_9fA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 3 — STRING CLEANING & STANDARDIZATION"
      ],
      "metadata": {
        "id": "XOEbShb3AUxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Trim leading and trailing spaces from all string columns"
      ],
      "metadata": {
        "id": "N2TqiLuDAgJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,trim\n",
        "\n",
        "for c in df.columns:\n",
        "  df=df.withColumn(c,trim(col(c)))"
      ],
      "metadata": {
        "id": "3p8jTEpB_6LU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Standardize city , category , and product values"
      ],
      "metadata": {
        "id": "bEvpy8NLBPlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lower"
      ],
      "metadata": {
        "id": "2vXOPPKKA3lb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.withColumn(\"city\",lower(col(\"city\")))\\\n",
        ".withColumn(\"category\",lower(col(\"category\")))\\\n",
        ".withColumn(\"product\",lower(col(\"product\")))\n"
      ],
      "metadata": {
        "id": "9YgmoyDmBgMy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Convert all categorical columns to a consistent case"
      ],
      "metadata": {
        "id": "id5EN3-FB2LG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.withColumn(\"status\",lower(col(\"status\")))"
      ],
      "metadata": {
        "id": "CSkWBFR9B2uS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Identify how many distinct city values existed before vs after cleaning"
      ],
      "metadata": {
        "id": "USw94Q40COc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv_raw.select(\"city\").distinct().count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0EhSO_ICE7K",
        "outputId": "21626d11-c447-4c4c-be63-d2f1c40b7481"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"city\").distinct().count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGocAmfKCU8h",
        "outputId": "6f10bc4d-2cf2-4e3f-b39a-814839607a99"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 4 — AMOUNT CLEANING (CRITICAL)"
      ],
      "metadata": {
        "id": "sGsKy1a1DRg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Identify invalid values in the amount column"
      ],
      "metadata": {
        "id": "hkS81bjeDYJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(~col(\"amount\").rlike(\"^[0-9]+$\")).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D605WsnVDSWN",
        "outputId": "daa72db8-d707-40ab-ce09-abf7b4a61f3b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28147"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Remove commas from numeric strings"
      ],
      "metadata": {
        "id": "zuLTk8E9Dt1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "df=df.withColumn(\"amount_clean\",regexp_replace(col(\"amount\"),\",\",\"\"))\n"
      ],
      "metadata": {
        "id": "h0lmHtKjDpBT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Convert amount to IntegerType safely"
      ],
      "metadata": {
        "id": "s4o1AOy0EHTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import IntegerType\n",
        "df=df.withColumn(\"amount_int\",F.when(col(\"amount_clean\").rlike(\"^[0-9]+$\"),col(\"amount_clean\").cast(IntegerType())).otherwise(None)\n",
        "                 )"
      ],
      "metadata": {
        "id": "ZAY-glnPEAeD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Handle empty, null, and invalid values explicitly"
      ],
      "metadata": {
        "id": "aSkzBJIyEbJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.filter(col(\"amount_int\").isNotNull())"
      ],
      "metadata": {
        "id": "74U5CURYEYUk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Count how many records were affected during amount cleaning"
      ],
      "metadata": {
        "id": "M4tTUdkQEuNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_count = df_csv_raw.rdd.count()\n",
        "cleaned_count = df.count()\n",
        "print(initial_count - cleaned_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfLmI6x7EnPh",
        "outputId": "8ba0af28-445d-4bae-ee8d-4045d89ed9d0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 5 — DATE PARSING & NORMALIZATION"
      ],
      "metadata": {
        "id": "6_ubRZWtG5PB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Identify all date formats present in order_date"
      ],
      "metadata": {
        "id": "-wVh4TdmJKA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yyyy-MM-dd\n",
        "\n",
        "dd/MM/yyyy\n",
        "\n",
        "yyyy/MM/dd"
      ],
      "metadata": {
        "id": "z4sx-yQQJNtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Parse valid dates into DateType\n",
        "\n",
        "22. Handle invalid dates gracefully\n",
        "\n",
        "23. Create a clean order_date_clean column\n"
      ],
      "metadata": {
        "id": "VLtSvmFgJGeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import try_to_timestamp,lit,coalesce,col,date_format,to_date\n",
        "df=df.withColumn(\"order_date_clean\",\n",
        "                 coalesce(\n",
        "                      try_to_timestamp(col(\"order_date\"), lit(\"yyyy-MM-dd\")),\n",
        "                      try_to_timestamp(col(\"order_date\"), lit(\"dd-MM-yyyy\")),\n",
        "                      try_to_timestamp(col(\"order_date\"), lit(\"MM-dd-yyyy\")),\n",
        "                      try_to_timestamp(col(\"order_date\"), lit(\"dd/MM/yyyy\")),\n",
        "                      try_to_timestamp(col(\"order_date\"), lit(\"MM/dd/yyyy\")),\n",
        "                      try_to_timestamp(col(\"order_date\"), lit(\"yyyy/MM/dd\"))\n",
        "    ).cast(DateType()))\n",
        ""
      ],
      "metadata": {
        "id": "ttUy0JxsEz7D"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Count records with invalid dates"
      ],
      "metadata": {
        "id": "E1O9Mhq7JdSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col(\"order_date_clean\").isNull()).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKKo3pXJJd15",
        "outputId": "82339979-4035-45e6-c04f-2e4e76869f4e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2378"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 6 — BUSINESS FILTERING & DEDUPLICATION"
      ],
      "metadata": {
        "id": "7Y_nPAxSKMBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Identify duplicate order_id values"
      ],
      "metadata": {
        "id": "f0oU2uglKSyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"order_id\").count().filter(col(\"count\")>1).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bXwzW5fKOdV",
        "outputId": "9782eac7-e04a-40cc-dd85-c60530864760"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|order_id|count|\n",
            "+--------+-----+\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Remove duplicate orders safely"
      ],
      "metadata": {
        "id": "rK6v3z4tKlgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.dropDuplicates([\"order_id\"])"
      ],
      "metadata": {
        "id": "lpS5c3QGKiXb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Keep only records with status = Completed"
      ],
      "metadata": {
        "id": "Z2QWEDiPKtHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.filter(col(\"status\")==\"completed\")"
      ],
      "metadata": {
        "id": "wa_Y9loMKpuA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Validate record counts before and after filtering"
      ],
      "metadata": {
        "id": "Mxuuabk7K1Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMDfY6DAKxHR",
        "outputId": "2c1ddd30-e7dd-4074-c1d5-beb6515ae794"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "261095"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 7 — PERFORMANCE & PARTITION AWARENESS"
      ],
      "metadata": {
        "id": "Vkld0HbfLSK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Check the default number of partitions"
      ],
      "metadata": {
        "id": "26_8-xQoL45d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2OQv88nK3og",
        "outputId": "1509e5bc-208f-469f-e488-27fc1db12631"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Run a heavy groupBy and observe execution time\n",
        "\n",
        "31. Use explain(True) to identify shuffle stages\n",
        "\n",
        "33.Compare execution plans before and after repartition"
      ],
      "metadata": {
        "id": "hXKE8ff6L_kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"city\").sum(\"amount_int\").explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-cty4vCL8uL",
        "outputId": "bed6f917-fddb-462e-ffe7-6ed199bb71f1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['city], ['city, unresolvedalias('sum(amount_int#209))]\n",
            "+- Filter (status#182 = completed)\n",
            "   +- Deduplicate [order_id#171]\n",
            "      +- Project [order_id#171, customer_id#172, city#179, category#180, product#181, amount#176, order_date#177, status#182, amount_clean#208, amount_int#209, cast(coalesce(try_to_timestamp(order_date#177, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#177, Some(dd-MM-yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#177, Some(MM-dd-yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#177, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#177, Some(MM/dd/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#177, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) as date) AS order_date_clean#233]\n",
            "         +- Filter isnotnull(amount_int#209)\n",
            "            +- Project [order_id#171, customer_id#172, city#179, category#180, product#181, amount#176, order_date#177, status#182, amount_clean#208, CASE WHEN RLIKE(amount_clean#208, ^[0-9]+$) THEN cast(amount_clean#208 as int) ELSE cast(null as int) END AS amount_int#209]\n",
            "               +- Project [order_id#171, customer_id#172, city#179, category#180, product#181, amount#176, order_date#177, status#182, regexp_replace(amount#176, ,, , 1) AS amount_clean#208]\n",
            "                  +- Project [order_id#171, customer_id#172, city#179, category#180, product#181, amount#176, order_date#177, lower(status#178) AS status#182]\n",
            "                     +- Project [order_id#171, customer_id#172, city#179, category#180, lower(product#175) AS product#181, amount#176, order_date#177, status#178]\n",
            "                        +- Project [order_id#171, customer_id#172, city#179, lower(category#174) AS category#180, product#175, amount#176, order_date#177, status#178]\n",
            "                           +- Project [order_id#171, customer_id#172, lower(city#173) AS city#179, category#174, product#175, amount#176, order_date#177, status#178]\n",
            "                              +- Project [order_id#171, customer_id#172, city#173, category#174, product#175, amount#176, order_date#177, trim(status#156, None) AS status#178]\n",
            "                                 +- Project [order_id#171, customer_id#172, city#173, category#174, product#175, amount#176, trim(order_date#155, None) AS order_date#177, status#156]\n",
            "                                    +- Project [order_id#171, customer_id#172, city#173, category#174, product#175, trim(amount#154, None) AS amount#176, order_date#155, status#156]\n",
            "                                       +- Project [order_id#171, customer_id#172, city#173, category#174, trim(product#153, None) AS product#175, amount#154, order_date#155, status#156]\n",
            "                                          +- Project [order_id#171, customer_id#172, city#173, trim(category#152, None) AS category#174, product#153, amount#154, order_date#155, status#156]\n",
            "                                             +- Project [order_id#171, customer_id#172, trim(city#151, None) AS city#173, category#152, product#153, amount#154, order_date#155, status#156]\n",
            "                                                +- Project [order_id#171, trim(customer_id#150, None) AS customer_id#172, city#151, category#152, product#153, amount#154, order_date#155, status#156]\n",
            "                                                   +- Project [trim(order_id#149, None) AS order_id#171, customer_id#150, city#151, category#152, product#153, amount#154, order_date#155, status#156]\n",
            "                                                      +- Relation [order_id#149,customer_id#150,city#151,category#152,product#153,amount#154,order_date#155,status#156] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, sum(amount_int): bigint\n",
            "Aggregate [city#179], [city#179, sum(amount_int#209) AS sum(amount_int)#561L]\n",
            "+- Filter (status#182 = completed)\n",
            "   +- Deduplicate [order_id#171]\n",
            "      +- Project [order_id#171, customer_id#172, city#179, category#180, product#181, amount#176, order_date#177, status#182, amount_clean#208, amount_int#209, cast(coalesce(try_to_timestamp(order_date#177, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#177, Some(dd-MM-yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#177, Some(MM-dd-yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#177, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#177, Some(MM/dd/yyyy), TimestampType, Some(Etc/UTC), false), try_to_timestamp(order_date#177, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false)) as date) AS order_date_clean#233]\n",
            "         +- Filter isnotnull(amount_int#209)\n",
            "            +- Project [order_id#171, customer_id#172, city#179, category#180, product#181, amount#176, order_date#177, status#182, amount_clean#208, CASE WHEN RLIKE(amount_clean#208, ^[0-9]+$) THEN cast(amount_clean#208 as int) ELSE cast(null as int) END AS amount_int#209]\n",
            "               +- Project [order_id#171, customer_id#172, city#179, category#180, product#181, amount#176, order_date#177, status#182, regexp_replace(amount#176, ,, , 1) AS amount_clean#208]\n",
            "                  +- Project [order_id#171, customer_id#172, city#179, category#180, product#181, amount#176, order_date#177, lower(status#178) AS status#182]\n",
            "                     +- Project [order_id#171, customer_id#172, city#179, category#180, lower(product#175) AS product#181, amount#176, order_date#177, status#178]\n",
            "                        +- Project [order_id#171, customer_id#172, city#179, lower(category#174) AS category#180, product#175, amount#176, order_date#177, status#178]\n",
            "                           +- Project [order_id#171, customer_id#172, lower(city#173) AS city#179, category#174, product#175, amount#176, order_date#177, status#178]\n",
            "                              +- Project [order_id#171, customer_id#172, city#173, category#174, product#175, amount#176, order_date#177, trim(status#156, None) AS status#178]\n",
            "                                 +- Project [order_id#171, customer_id#172, city#173, category#174, product#175, amount#176, trim(order_date#155, None) AS order_date#177, status#156]\n",
            "                                    +- Project [order_id#171, customer_id#172, city#173, category#174, product#175, trim(amount#154, None) AS amount#176, order_date#155, status#156]\n",
            "                                       +- Project [order_id#171, customer_id#172, city#173, category#174, trim(product#153, None) AS product#175, amount#154, order_date#155, status#156]\n",
            "                                          +- Project [order_id#171, customer_id#172, city#173, trim(category#152, None) AS category#174, product#153, amount#154, order_date#155, status#156]\n",
            "                                             +- Project [order_id#171, customer_id#172, trim(city#151, None) AS city#173, category#152, product#153, amount#154, order_date#155, status#156]\n",
            "                                                +- Project [order_id#171, trim(customer_id#150, None) AS customer_id#172, city#151, category#152, product#153, amount#154, order_date#155, status#156]\n",
            "                                                   +- Project [trim(order_id#149, None) AS order_id#171, customer_id#150, city#151, category#152, product#153, amount#154, order_date#155, status#156]\n",
            "                                                      +- Relation [order_id#149,customer_id#150,city#151,category#152,product#153,amount#154,order_date#155,status#156] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [city#565], [city#565, sum(amount_int#579) AS sum(amount_int)#561L]\n",
            "+- Project [city#565, amount_int#579]\n",
            "   +- Filter (isnotnull(status#575) AND (status#575 = completed))\n",
            "      +- Aggregate [order_id#171], [first(city#179, false) AS city#565, first(status#182, false) AS status#575, first(amount_int#209, false) AS amount_int#579]\n",
            "         +- Project [order_id#171, city#179, status#182, CASE WHEN RLIKE(amount_clean#208, ^[0-9]+$) THEN cast(amount_clean#208 as int) END AS amount_int#209]\n",
            "            +- Project [trim(order_id#149, None) AS order_id#171, lower(trim(city#151, None)) AS city#179, lower(trim(status#156, None)) AS status#182, regexp_replace(trim(amount#154, None), ,, , 1) AS amount_clean#208]\n",
            "               +- Filter CASE WHEN RLIKE(regexp_replace(trim(amount#154, None), ,, , 1), ^[0-9]+$) THEN isnotnull(cast(regexp_replace(trim(amount#154, None), ,, , 1) as int)) ELSE false END\n",
            "                  +- Relation [order_id#149,customer_id#150,city#151,category#152,product#153,amount#154,order_date#155,status#156] csv\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[city#565], functions=[sum(amount_int#579)], output=[city#565, sum(amount_int)#561L])\n",
            "   +- Exchange hashpartitioning(city#565, 200), ENSURE_REQUIREMENTS, [plan_id=1005]\n",
            "      +- HashAggregate(keys=[city#565], functions=[partial_sum(amount_int#579)], output=[city#565, sum#583L])\n",
            "         +- Project [city#565, amount_int#579]\n",
            "            +- Filter (isnotnull(status#575) AND (status#575 = completed))\n",
            "               +- SortAggregate(key=[order_id#171], functions=[first(city#179, false), first(status#182, false), first(amount_int#209, false)], output=[city#565, status#575, amount_int#579])\n",
            "                  +- Sort [order_id#171 ASC NULLS FIRST], false, 0\n",
            "                     +- Exchange hashpartitioning(order_id#171, 200), ENSURE_REQUIREMENTS, [plan_id=998]\n",
            "                        +- SortAggregate(key=[order_id#171], functions=[partial_first(city#179, false), partial_first(status#182, false), partial_first(amount_int#209, false)], output=[order_id#171, first#590, valueSet#591, first#592, valueSet#593, first#594, valueSet#595])\n",
            "                           +- Sort [order_id#171 ASC NULLS FIRST], false, 0\n",
            "                              +- Project [order_id#171, city#179, status#182, CASE WHEN RLIKE(amount_clean#208, ^[0-9]+$) THEN cast(amount_clean#208 as int) END AS amount_int#209]\n",
            "                                 +- Project [trim(order_id#149, None) AS order_id#171, lower(trim(city#151, None)) AS city#179, lower(trim(status#156, None)) AS status#182, regexp_replace(trim(amount#154, None), ,, , 1) AS amount_clean#208]\n",
            "                                    +- Filter CASE WHEN RLIKE(regexp_replace(trim(amount#154, None), ,, , 1), ^[0-9]+$) THEN isnotnull(cast(regexp_replace(trim(amount#154, None), ,, , 1) as int)) ELSE false END\n",
            "                                       +- FileScan csv [order_id#149,city#151,amount#154,status#156] Batched: false, DataFilters: [CASE WHEN RLIKE(regexp_replace(trim(amount#154, None), ,, , 1), ^[0-9]+$) THEN isnotnull(cast(re..., Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders_large_bad.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<order_id:string,city:string,amount:string,status:string>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Repartition the DataFrame by city"
      ],
      "metadata": {
        "id": "T4jp_SGEMTvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.repartition(\"city\")"
      ],
      "metadata": {
        "id": "CaxQcrKaMIDY"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 8 — ANALYTICS ON LARGE DATA"
      ],
      "metadata": {
        "id": "PITDjUYJNXcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Calculate total revenue per city"
      ],
      "metadata": {
        "id": "grZp0RyiNsN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum,avg"
      ],
      "metadata": {
        "id": "A4DuRi8BMiGR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "rev_city=df.groupBy(\"city\").agg(sum(\"amount_int\").alias(\"total revenue\"))\n",
        "rev_city.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI_atYdxNyGc",
        "outputId": "625b1349-3150-49bf-9f3d-30d2f505ce4f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "|     city|total revenue|\n",
            "+---------+-------------+\n",
            "|  chennai|   1629865247|\n",
            "|    delhi|   1639639916|\n",
            "|bangalore|   1628527093|\n",
            "|hyderabad|   1642443340|\n",
            "|  kolkata|   1624300497|\n",
            "|   mumbai|   1625518096|\n",
            "|     pune|   1646196535|\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Calculate total revenue per category"
      ],
      "metadata": {
        "id": "AO1lzeSlOBw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rev_category=df.groupBy(\"category\").agg(sum(\"amount_int\").alias(\"total revenue\"))\n",
        "rev_category.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZQxv2ToOF_g",
        "outputId": "7118e292-96c9-42d7-e182-1d1b01bf0d3f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|   category|total revenue|\n",
            "+-----------+-------------+\n",
            "|    grocery|   2866272106|\n",
            "|electronics|   2867568870|\n",
            "|       home|   2868467576|\n",
            "|    fashion|   2834182172|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Calculate total revenue per product"
      ],
      "metadata": {
        "id": "gU4DoJwcO959"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rev_product=df.groupBy(\"product\").agg(sum(\"amount_int\").alias(\"total revenue\"))\n",
        "rev_product.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fLmrtvpO-cp",
        "outputId": "a07f6cee-dbec-4dbe-ca22-03b142e09682"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|    product|total revenue|\n",
            "+-----------+-------------+\n",
            "|      shoes|    946799102|\n",
            "|     vacuum|    959149427|\n",
            "|airpurifier|    952178123|\n",
            "|     mobile|    944352576|\n",
            "|     tablet|    960719999|\n",
            "|      sugar|    948205000|\n",
            "|     laptop|    962496295|\n",
            "|      mixer|    957140026|\n",
            "|      jeans|    951286127|\n",
            "|       rice|    954494237|\n",
            "|     tshirt|    936096943|\n",
            "|        oil|    963572869|\n",
            "+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Identify top 10 products by revenue"
      ],
      "metadata": {
        "id": "8dEfwtQmPKMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rev_product.orderBy(col(\"total revenue\").desc()).show(10, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99BPECRwPF5t",
        "outputId": "00932b75-b0f4-4d48-e3b6-8c0d96094472"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+\n",
            "|product    |total revenue|\n",
            "+-----------+-------------+\n",
            "|oil        |963572869    |\n",
            "|laptop     |962496295    |\n",
            "|tablet     |960719999    |\n",
            "|vacuum     |959149427    |\n",
            "|mixer      |957140026    |\n",
            "|rice       |954494237    |\n",
            "|airpurifier|952178123    |\n",
            "|jeans      |951286127    |\n",
            "|sugar      |948205000    |\n",
            "|shoes      |946799102    |\n",
            "+-----------+-------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Calculate average order value per city"
      ],
      "metadata": {
        "id": "Q27zaRTkPZRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_order_value_city=df.groupBy(\"city\").agg(avg(\"amount_int\").alias(\"average order value\"))\n",
        "avg_order_value_city.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azwax_YDPZzk",
        "outputId": "086332e5-b253-4e1d-e799-63415c3d74de"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------+\n",
            "|     city|average order value|\n",
            "+---------+-------------------+\n",
            "|  chennai|  43628.27900315863|\n",
            "|    delhi|  43817.20780331374|\n",
            "|bangalore| 44098.867908689645|\n",
            "|hyderabad|  43708.74045293664|\n",
            "|  kolkata| 43709.816662630175|\n",
            "|   mumbai|  43723.75651612556|\n",
            "|     pune| 43930.204013556424|\n",
            "+---------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 9 — WINDOW FUNCTIONS (BIG DATA SAFE)"
      ],
      "metadata": {
        "id": "EEGs_-NsQA5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Rank cities by total revenue"
      ],
      "metadata": {
        "id": "TKEV2r32QHw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number,rank,dense_rank"
      ],
      "metadata": {
        "id": "ZZ0qY2b6QDIW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w=Window.orderBy(col(\"total revenue\").desc())\n",
        "rev_city.withColumn(\"rank\",rank().over(w)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQPnwVzOQX-E",
        "outputId": "3ba4b3bc-b2a3-4d04-d5b2-94d39c7839c1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+----+\n",
            "|     city|total revenue|rank|\n",
            "+---------+-------------+----+\n",
            "|     pune|   1646196535|   1|\n",
            "|hyderabad|   1642443340|   2|\n",
            "|    delhi|   1639639916|   3|\n",
            "|  chennai|   1629865247|   4|\n",
            "|bangalore|   1628527093|   5|\n",
            "|   mumbai|   1625518096|   6|\n",
            "|  kolkata|   1624300497|   7|\n",
            "+---------+-------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Rank products within each category by revenue"
      ],
      "metadata": {
        "id": "CysmNzCkQ13i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "product_revenue_by_category = df.groupBy(\"category\", \"product\").agg(sum(\"amount_int\").alias(\"total_revenue\"))\n",
        "\n",
        "window_spec_category = Window.partitionBy(\"category\").orderBy(col(\"total_revenue\").desc())\n",
        "\n",
        "ranked_products_by_category = product_revenue_by_category.withColumn(\"rank\", rank().over(window_spec_category))\n",
        "ranked_products_by_category.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1H7MCcVQhq-",
        "outputId": "5d24a599-62e7-4d93-f164-a2f8270b9543"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-------------+----+\n",
            "|   category|    product|total_revenue|rank|\n",
            "+-----------+-----------+-------------+----+\n",
            "|electronics|     laptop|    962496295|   1|\n",
            "|electronics|     tablet|    960719999|   2|\n",
            "|electronics|     mobile|    944352576|   3|\n",
            "|    fashion|      jeans|    951286127|   1|\n",
            "|    fashion|      shoes|    946799102|   2|\n",
            "|    fashion|     tshirt|    936096943|   3|\n",
            "|    grocery|        oil|    963572869|   1|\n",
            "|    grocery|       rice|    954494237|   2|\n",
            "|    grocery|      sugar|    948205000|   3|\n",
            "|       home|     vacuum|    959149427|   1|\n",
            "|       home|      mixer|    957140026|   2|\n",
            "|       home|airpurifier|    952178123|   3|\n",
            "+-----------+-----------+-------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Identify the top product per category"
      ],
      "metadata": {
        "id": "ZUDBreM4RNd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_product_per_category = ranked_products_by_category.filter(col(\"rank\") == 1)\n",
        "top_product_per_category.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPDPa1ZBRN6z",
        "outputId": "ba5cdd99-8644-4e76-d065-fde9cadcb4cf"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+-------------+----+\n",
            "|   category|product|total_revenue|rank|\n",
            "+-----------+-------+-------------+----+\n",
            "|electronics| laptop|    962496295|   1|\n",
            "|    fashion|  jeans|    951286127|   1|\n",
            "|    grocery|    oil|    963572869|   1|\n",
            "|       home| vacuum|    959149427|   1|\n",
            "+-----------+-------+-------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Identify top 3 cities using window functions"
      ],
      "metadata": {
        "id": "z81Ag3BJRb5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w=Window.orderBy(col(\"total revenue\").desc())\n",
        "top_3_cities = rev_city.withColumn(\"rank\",rank().over(w)).filter(col(\"rank\")<=3)\n",
        "top_3_cities.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ey0l_UFRcdd",
        "outputId": "42aacf50-bb72-4bc4-a7d4-7508239f25dc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+----+\n",
            "|     city|total revenue|rank|\n",
            "+---------+-------------+----+\n",
            "|     pune|   1646196535|   1|\n",
            "|hyderabad|   1642443340|   2|\n",
            "|    delhi|   1639639916|   3|\n",
            "+---------+-------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 10 — CACHING & REUSE"
      ],
      "metadata": {
        "id": "mETMtlz8Rngm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Identify DataFrames reused multiple times"
      ],
      "metadata": {
        "id": "GDaEkkzURxIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrames reused multiple times and good candidates for caching:\n",
        "# 1. `df`: This is the main DataFrame after extensive cleaning and transformations, used in almost all subsequent analysis steps.\n",
        "# 2. `df_csv_raw`: Used for initial inspection, schema comparison, and calculating the initial record count.\n",
        "# 3. `rev_city`: Used to calculate total revenue per city and then for ranking cities.\n",
        "# 4. `rev_product`: Used to calculate total revenue per product and then for identifying top products.\n",
        "# 5. `product_revenue_by_category`: Used as an intermediate result before ranking products within categories.\n",
        "# 6. `ranked_products_by_category`: Used for displaying ranked products and then for identifying the top product per category.\n",
        "\n",
        "# Example of how you would cache a DataFrame:\n",
        "# df.cache()\n",
        "# rev_city.cache()\n",
        "# etc."
      ],
      "metadata": {
        "id": "mQnUGJSfRpDR"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Apply caching strategically"
      ],
      "metadata": {
        "id": "mYJ5DISkSBDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRfl3xb9SBrL",
        "outputId": "95223a39-3ade-43c1-94f7-1c747817efc3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[order_id: string, customer_id: string, city: string, category: string, product: string, amount: string, order_date: string, status: string, amount_clean: string, amount_int: int, order_date_clean: date]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Re-run analytics and observe performance"
      ],
      "metadata": {
        "id": "V6AqWa0-SHkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Re-running analytics after caching 'df'...\")\n",
        "\n",
        "# 34. Calculate total revenue per city\n",
        "rev_city=df.groupBy(\"city\").agg(sum(\"amount_int\").alias(\"total revenue\"))\n",
        "print(\"\\nTotal revenue per city:\")\n",
        "rev_city.show()\n",
        "\n",
        "# 35. Calculate total revenue per category\n",
        "rev_category=df.groupBy(\"category\").agg(sum(\"amount_int\").alias(\"total revenue\"))\n",
        "print(\"\\nTotal revenue per category:\")\n",
        "rev_category.show()\n",
        "\n",
        "# 36. Calculate total revenue per product\n",
        "rev_product=df.groupBy(\"product\").agg(sum(\"amount_int\").alias(\"total revenue\"))\n",
        "print(\"\\nTotal revenue per product:\")\n",
        "rev_product.show()\n",
        "\n",
        "# 37. Identify top 10 products by revenue\n",
        "print(\"\\nTop 10 products by revenue:\")\n",
        "rev_product.orderBy(col(\"total revenue\").desc()).show(10, False)\n",
        "\n",
        "# 38. Calculate average order value per city\n",
        "avg_order_value_city=df.groupBy(\"city\").agg(avg(\"amount_int\").alias(\"average order value\"))\n",
        "print(\"\\nAverage order value per city:\")\n",
        "avg_order_value_city.show()\n",
        "\n",
        "# 39. Rank cities by total revenue\n",
        "w_city=Window.orderBy(col(\"total revenue\").desc())\n",
        "ranked_cities = rev_city.withColumn(\"rank\",rank().over(w_city))\n",
        "print(\"\\nRanked cities by total revenue:\")\n",
        "ranked_cities.show()\n",
        "\n",
        "# 40. Rank products within each category by revenue\n",
        "product_revenue_by_category = df.groupBy(\"category\", \"product\").agg(sum(\"amount_int\").alias(\"total_revenue\"))\n",
        "window_spec_category = Window.partitionBy(\"category\").orderBy(col(\"total_revenue\").desc())\n",
        "ranked_products_by_category = product_revenue_by_category.withColumn(\"rank\", rank().over(window_spec_category))\n",
        "print(\"\\nRanked products within each category by revenue:\")\n",
        "ranked_products_by_category.show()\n",
        "\n",
        "# 41. Identify the top product per category\n",
        "top_product_per_category = ranked_products_by_category.filter(col(\"rank\") == 1)\n",
        "print(\"\\nTop product per category:\")\n",
        "top_product_per_category.show()\n",
        "\n",
        "# 42. Identify top 3 cities using window functions\n",
        "w_top_3_cities=Window.orderBy(col(\"total revenue\").desc())\n",
        "top_3_cities = rev_city.withColumn(\"rank\",rank().over(w_top_3_cities)).filter(col(\"rank\")<=3)\n",
        "print(\"\\nTop 3 cities:\")\n",
        "top_3_cities.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSR_K-95SDTs",
        "outputId": "91cea22b-28e0-4f38-989a-ea46756bd2bf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-running analytics after caching 'df'...\n",
            "\n",
            "Total revenue per city:\n",
            "+---------+-------------+\n",
            "|     city|total revenue|\n",
            "+---------+-------------+\n",
            "|  chennai|   1629865247|\n",
            "|    delhi|   1639639916|\n",
            "|bangalore|   1628527093|\n",
            "|hyderabad|   1642443340|\n",
            "|  kolkata|   1624300497|\n",
            "|   mumbai|   1625518096|\n",
            "|     pune|   1646196535|\n",
            "+---------+-------------+\n",
            "\n",
            "\n",
            "Total revenue per category:\n",
            "+-----------+-------------+\n",
            "|   category|total revenue|\n",
            "+-----------+-------------+\n",
            "|    grocery|   2866272106|\n",
            "|electronics|   2867568870|\n",
            "|       home|   2868467576|\n",
            "|    fashion|   2834182172|\n",
            "+-----------+-------------+\n",
            "\n",
            "\n",
            "Total revenue per product:\n",
            "+-----------+-------------+\n",
            "|    product|total revenue|\n",
            "+-----------+-------------+\n",
            "|      shoes|    946799102|\n",
            "|     vacuum|    959149427|\n",
            "|airpurifier|    952178123|\n",
            "|     mobile|    944352576|\n",
            "|     tablet|    960719999|\n",
            "|      sugar|    948205000|\n",
            "|     laptop|    962496295|\n",
            "|      mixer|    957140026|\n",
            "|      jeans|    951286127|\n",
            "|       rice|    954494237|\n",
            "|     tshirt|    936096943|\n",
            "|        oil|    963572869|\n",
            "+-----------+-------------+\n",
            "\n",
            "\n",
            "Top 10 products by revenue:\n",
            "+-----------+-------------+\n",
            "|product    |total revenue|\n",
            "+-----------+-------------+\n",
            "|oil        |963572869    |\n",
            "|laptop     |962496295    |\n",
            "|tablet     |960719999    |\n",
            "|vacuum     |959149427    |\n",
            "|mixer      |957140026    |\n",
            "|rice       |954494237    |\n",
            "|airpurifier|952178123    |\n",
            "|jeans      |951286127    |\n",
            "|sugar      |948205000    |\n",
            "|shoes      |946799102    |\n",
            "+-----------+-------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Average order value per city:\n",
            "+---------+-------------------+\n",
            "|     city|average order value|\n",
            "+---------+-------------------+\n",
            "|  chennai|  43628.27900315863|\n",
            "|    delhi|  43817.20780331374|\n",
            "|bangalore| 44098.867908689645|\n",
            "|hyderabad|  43708.74045293664|\n",
            "|  kolkata| 43709.816662630175|\n",
            "|   mumbai|  43723.75651612556|\n",
            "|     pune| 43930.204013556424|\n",
            "+---------+-------------------+\n",
            "\n",
            "\n",
            "Ranked cities by total revenue:\n",
            "+---------+-------------+----+\n",
            "|     city|total revenue|rank|\n",
            "+---------+-------------+----+\n",
            "|     pune|   1646196535|   1|\n",
            "|hyderabad|   1642443340|   2|\n",
            "|    delhi|   1639639916|   3|\n",
            "|  chennai|   1629865247|   4|\n",
            "|bangalore|   1628527093|   5|\n",
            "|   mumbai|   1625518096|   6|\n",
            "|  kolkata|   1624300497|   7|\n",
            "+---------+-------------+----+\n",
            "\n",
            "\n",
            "Ranked products within each category by revenue:\n",
            "+-----------+-----------+-------------+----+\n",
            "|   category|    product|total_revenue|rank|\n",
            "+-----------+-----------+-------------+----+\n",
            "|electronics|     laptop|    962496295|   1|\n",
            "|electronics|     tablet|    960719999|   2|\n",
            "|electronics|     mobile|    944352576|   3|\n",
            "|    fashion|      jeans|    951286127|   1|\n",
            "|    fashion|      shoes|    946799102|   2|\n",
            "|    fashion|     tshirt|    936096943|   3|\n",
            "|    grocery|        oil|    963572869|   1|\n",
            "|    grocery|       rice|    954494237|   2|\n",
            "|    grocery|      sugar|    948205000|   3|\n",
            "|       home|     vacuum|    959149427|   1|\n",
            "|       home|      mixer|    957140026|   2|\n",
            "|       home|airpurifier|    952178123|   3|\n",
            "+-----------+-----------+-------------+----+\n",
            "\n",
            "\n",
            "Top product per category:\n",
            "+-----------+-------+-------------+----+\n",
            "|   category|product|total_revenue|rank|\n",
            "+-----------+-------+-------------+----+\n",
            "|electronics| laptop|    962496295|   1|\n",
            "|    fashion|  jeans|    951286127|   1|\n",
            "|    grocery|    oil|    963572869|   1|\n",
            "|       home| vacuum|    959149427|   1|\n",
            "+-----------+-------+-------------+----+\n",
            "\n",
            "\n",
            "Top 3 cities:\n",
            "+---------+-------------+----+\n",
            "|     city|total revenue|rank|\n",
            "+---------+-------------+----+\n",
            "|     pune|   1646196535|   1|\n",
            "|hyderabad|   1642443340|   2|\n",
            "|    delhi|   1639639916|   3|\n",
            "+---------+-------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 46. Unpersist when cache is no longer needed"
      ],
      "metadata": {
        "id": "n-ObqXhnTBsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApQ8O2gdTDHq",
        "outputId": "53bdd445-29ac-44d6-b8e9-1892e1e4d58f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[order_id: string, customer_id: string, city: string, category: string, product: string, amount: string, order_date: string, status: string, amount_clean: string, amount_int: int, order_date_clean: date]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 11 — FILE FORMAT STRATEGY"
      ],
      "metadata": {
        "id": "w-STDQVxau8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "48. Write the cleaned order-level dataset to Parquet"
      ],
      "metadata": {
        "id": "Y6DZ5IPPa5K2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").parquet(\"cleaned_orders.parquet\")"
      ],
      "metadata": {
        "id": "3vkLAhpMTFMK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "49. Partition the Parquet output by city\n"
      ],
      "metadata": {
        "id": "twuJNd7HbI9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"cleaned_orders_partitioned.parquet\")"
      ],
      "metadata": {
        "id": "ui-CCQb-bKMt"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "49. Partition the Parquet output by city"
      ],
      "metadata": {
        "id": "Hgv1_YkEbRnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"cleaned_orders_partitioned.parquet\")"
      ],
      "metadata": {
        "id": "YWFFPcDvbSEK"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "50. Write aggregated analytics to ORC"
      ],
      "metadata": {
        "id": "kCzQ9F-vbfTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rev_city.write.mode(\"overwrite\").orc(\"rev_city.orc\")\n",
        "rev_category.write.mode(\"overwrite\").orc(\"rev_category.orc\")\n",
        "rev_product.write.mode(\"overwrite\").orc(\"rev_product.orc\")"
      ],
      "metadata": {
        "id": "nE1Il1UgbfyR"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "51. Read both formats back and validate schema"
      ],
      "metadata": {
        "id": "U2sAuONpb0QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_parquet_read = spark.read.parquet(\"cleaned_orders.parquet\")\n",
        "print(\"Schema of Parquet file (cleaned_orders.parquet):\")\n",
        "df_parquet_read.printSchema()\n",
        "\n",
        "\n",
        "rev_city_orc_read = spark.read.orc(\"rev_city.orc\")\n",
        "print(\"\\nSchema of ORC file (rev_city.orc):\")\n",
        "rev_city_orc_read.printSchema()\n",
        "\n",
        "rev_category_orc_read = spark.read.orc(\"rev_category.orc\")\n",
        "print(\"\\nSchema of ORC file (rev_category.orc):\")\n",
        "rev_category_orc_read.printSchema()\n",
        "\n",
        "rev_product_orc_read = spark.read.orc(\"rev_product.orc\")\n",
        "print(\"\\nSchema of ORC file (rev_product.orc):\")\n",
        "rev_product_orc_read.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXNtDAsYb0tF",
        "outputId": "463d52dc-d138-4800-d034-682f1828ad46"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema of Parquet file (cleaned_orders.parquet):\n",
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- amount_clean: string (nullable = true)\n",
            " |-- amount_int: integer (nullable = true)\n",
            " |-- order_date_clean: date (nullable = true)\n",
            "\n",
            "\n",
            "Schema of ORC file (rev_city.orc):\n",
            "root\n",
            " |-- city: string (nullable = true)\n",
            " |-- total revenue: long (nullable = true)\n",
            "\n",
            "\n",
            "Schema of ORC file (rev_category.orc):\n",
            "root\n",
            " |-- category: string (nullable = true)\n",
            " |-- total revenue: long (nullable = true)\n",
            "\n",
            "\n",
            "Schema of ORC file (rev_product.orc):\n",
            "root\n",
            " |-- product: string (nullable = true)\n",
            " |-- total revenue: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "52. Compare number of output files generated"
      ],
      "metadata": {
        "id": "jp0qZjXpcBTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"Files for unpartitioned Parquet (cleaned_orders.parquet):\")\n",
        "!ls -l cleaned_orders.parquet\n",
        "\n",
        "print(\"\\nFiles for partitioned Parquet (cleaned_orders_partitioned.parquet):\")\n",
        "!ls -lR cleaned_orders_partitioned.parquet\n",
        "\n",
        "print(\"\\nFiles for ORC (rev_city.orc):\")\n",
        "!ls -l rev_city.orc\n",
        "\n",
        "print(\"\\nFiles for ORC (rev_category.orc):\")\n",
        "!ls -l rev_category.orc\n",
        "\n",
        "print(\"\\nFiles for ORC (rev_product.orc):\")\n",
        "!ls -l rev_product.orc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj-S5bG2cB7z",
        "outputId": "11d2a414-e900-4d9c-e0eb-79c55019403b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files for unpartitioned Parquet (cleaned_orders.parquet):\n",
            "total 6984\n",
            "-rw-r--r-- 1 root root 3054173 Dec 26 06:50 part-00000-875edfed-d4a5-440e-a60c-d7c265f56b11-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root 3061343 Dec 26 06:50 part-00001-875edfed-d4a5-440e-a60c-d7c265f56b11-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root 1029334 Dec 26 06:50 part-00002-875edfed-d4a5-440e-a60c-d7c265f56b11-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root       0 Dec 26 06:50 _SUCCESS\n",
            "\n",
            "Files for partitioned Parquet (cleaned_orders_partitioned.parquet):\n",
            "cleaned_orders_partitioned.parquet:\n",
            "total 28\n",
            "drwxr-xr-x 2 root root 4096 Dec 26 06:51 'city=bangalore'\n",
            "drwxr-xr-x 2 root root 4096 Dec 26 06:52 'city=chennai'\n",
            "drwxr-xr-x 2 root root 4096 Dec 26 06:52 'city=delhi'\n",
            "drwxr-xr-x 2 root root 4096 Dec 26 06:51 'city=hyderabad'\n",
            "drwxr-xr-x 2 root root 4096 Dec 26 06:52 'city=kolkata'\n",
            "drwxr-xr-x 2 root root 4096 Dec 26 06:52 'city=mumbai'\n",
            "drwxr-xr-x 2 root root 4096 Dec 26 06:52 'city=pune'\n",
            "-rw-r--r-- 1 root root    0 Dec 26 06:52  _SUCCESS\n",
            "\n",
            "'cleaned_orders_partitioned.parquet/city=bangalore':\n",
            "total 992\n",
            "-rw-r--r-- 1 root root 1013086 Dec 26 06:52 part-00000-b9acef79-eb18-42bc-9016-e8ab78e8b968.c000.snappy.parquet\n",
            "\n",
            "'cleaned_orders_partitioned.parquet/city=chennai':\n",
            "total 1000\n",
            "-rw-r--r-- 1 root root 1023115 Dec 26 06:52 part-00000-b9acef79-eb18-42bc-9016-e8ab78e8b968.c000.snappy.parquet\n",
            "\n",
            "'cleaned_orders_partitioned.parquet/city=delhi':\n",
            "total 1004\n",
            "-rw-r--r-- 1 root root 1025307 Dec 26 06:52 part-00000-b9acef79-eb18-42bc-9016-e8ab78e8b968.c000.snappy.parquet\n",
            "\n",
            "'cleaned_orders_partitioned.parquet/city=hyderabad':\n",
            "total 1008\n",
            "-rw-r--r-- 1 root root 1029448 Dec 26 06:52 part-00001-b9acef79-eb18-42bc-9016-e8ab78e8b968.c000.snappy.parquet\n",
            "\n",
            "'cleaned_orders_partitioned.parquet/city=kolkata':\n",
            "total 996\n",
            "-rw-r--r-- 1 root root 1018556 Dec 26 06:52 part-00001-b9acef79-eb18-42bc-9016-e8ab78e8b968.c000.snappy.parquet\n",
            "\n",
            "'cleaned_orders_partitioned.parquet/city=mumbai':\n",
            "total 1000\n",
            "-rw-r--r-- 1 root root 1020534 Dec 26 06:52 part-00001-b9acef79-eb18-42bc-9016-e8ab78e8b968.c000.snappy.parquet\n",
            "\n",
            "'cleaned_orders_partitioned.parquet/city=pune':\n",
            "total 1008\n",
            "-rw-r--r-- 1 root root 1028949 Dec 26 06:52 part-00002-b9acef79-eb18-42bc-9016-e8ab78e8b968.c000.snappy.parquet\n",
            "\n",
            "Files for ORC (rev_city.orc):\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 543 Dec 26 06:53 part-00000-426aae70-8870-438f-a75b-d33a142f5839-c000.zstd.orc\n",
            "-rw-r--r-- 1 root root   0 Dec 26 06:53 _SUCCESS\n",
            "\n",
            "Files for ORC (rev_category.orc):\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 525 Dec 26 06:53 part-00000-721bd625-b35f-4bdf-a8f1-f115a65ad9b1-c000.zstd.orc\n",
            "-rw-r--r-- 1 root root   0 Dec 26 06:53 _SUCCESS\n",
            "\n",
            "Files for ORC (rev_product.orc):\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 604 Dec 26 06:53 part-00000-11d53211-51c3-489d-903a-2b7142b1c54f-c000.zstd.orc\n",
            "-rw-r--r-- 1 root root   0 Dec 26 06:53 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 12 — DEBUGGING & FAILURE SCENARIOS"
      ],
      "metadata": {
        "id": "BPQATdNqcRQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "53. Explain why the following line breaks pipelines:\n",
        "\n",
        "df = df.filter(df.amount > 50000).show()"
      ],
      "metadata": {
        "id": "jxjdTfRLcZBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The line `df = df.filter(df.amount > 50000).show()` breaks pipelines for the following reason:\n",
        "#\n",
        "# 1.  **`.show()` is an Action, not a Transformation**: In Apache Spark, operations are categorized as transformations or actions.\n",
        "#     *   **Transformations** create a new DataFrame from an existing one (e.g., `filter()`, `withColumn()`, `groupBy()`). They are lazily evaluated and return a new DataFrame.\n",
        "#     *   **Actions** trigger the execution of all preceding transformations and return results to the driver program (e.g., `show()`, `count()`, `collect()`, `write()`).\n",
        "#\n",
        "# 2.  **Return Value of `.show()`**: The `.show()` method prints the contents of the DataFrame to the console but it *returns `None`*.\n",
        "#\n",
        "# 3.  **Variable Reassignment**: When you execute `df = df.filter(df.amount > 50000).show()`, the variable `df` is reassigned the return value of `.show()`, which is `None`.\n",
        "#\n",
        "# **Consequence**: After this line, `df` is no longer a Spark DataFrame object; it's `None`. Any subsequent code that attempts to perform DataFrame operations on `df` will fail with an error like `AttributeError: 'NoneType' object has no attribute 'filter'` (or similar), thus breaking the entire pipeline."
      ],
      "metadata": {
        "id": "hLwSgs7-cTlB"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "54. Create a scenario that produces a NoneType error"
      ],
      "metadata": {
        "id": "Woujx9ZxcwsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "df_sample = spark.createDataFrame([\n",
        "    (1, \"apple\"), (2, \"banana\"), (3, \"cherry\")\n",
        "], [\"id\", \"fruit\"])\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df_sample.show()\n",
        "\n",
        "print(\"\\nAttempting to incorrectly reassign df_sample with .show() result...\")\n",
        "df_sample = df_sample.filter(df_sample.id > 1).show()\n",
        "\n",
        "\n",
        "print(\"\\nAttempting to perform an operation on the now NoneType df_sample:\")\n",
        "df_sample.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "toXMzoM0cxSR",
        "outputId": "7f1305cb-b987-4e9a-be23-1ea0b4925436"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+---+------+\n",
            "| id| fruit|\n",
            "+---+------+\n",
            "|  1| apple|\n",
            "|  2|banana|\n",
            "|  3|cherry|\n",
            "+---+------+\n",
            "\n",
            "\n",
            "Attempting to incorrectly reassign df_sample with .show() result...\n",
            "+---+------+\n",
            "| id| fruit|\n",
            "+---+------+\n",
            "|  2|banana|\n",
            "|  3|cherry|\n",
            "+---+------+\n",
            "\n",
            "\n",
            "Attempting to perform an operation on the now NoneType df_sample:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'count'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-627400904.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAttempting to perform an operation on the now NoneType df_sample:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdf_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'count'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "55. Identify a transformation that causes a wide shuffle"
      ],
      "metadata": {
        "id": "xPYz2vF6d03x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common transformation that causes a 'wide shuffle' in Spark is groupBy(). This happens because Spark needs to collect all rows with the same grouping key onto the same partition to perform the aggregation. This redistribution of data across the network and executors is what constitutes a wide shuffle."
      ],
      "metadata": {
        "id": "BIGRS6cad-W9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "56. Explain how you would debug a slow Spark job"
      ],
      "metadata": {
        "id": "1SEyZ98xeDp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Check Spark UI – Identify slow stages, skewed tasks, and heavy shuffles.\n",
        "2. Look for data skew – Uneven key distribution; fix with salting or repartitioning.\n",
        "3. Optimize transformations – Use reduceByKey instead of groupByKey, filter early, cache reused data.\n",
        "4. Tune configs – Adjust executor memory/cores, spark.sql.shuffle.partitions.\n",
        "5. Monitor GC & spills – Increase memory or use Kryo serialization if needed.\n",
        "6. Avoid slow UDFs – Prefer built-in functions or Pandas UDFs.\n",
        "Check logs & metrics – Use Spark History Server and executor logs."
      ],
      "metadata": {
        "id": "JCatgSDPfKXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PHASE 13 — FINAL VALIDATION"
      ],
      "metadata": {
        "id": "wj9UvOsifbDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "57. Validate no nulls in critical columns"
      ],
      "metadata": {
        "id": "JepBhyMbfgF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of nulls in order_id:\", df.filter(col(\"order_id\").isNull()).count())\n",
        "print(\"Number of nulls in amount_int:\", df.filter(col(\"amount_int\").isNull()).count())\n",
        "print(\"Number of nulls in order_date_clean:\", df.filter(col(\"order_date_clean\").isNull()).count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1f8NWw7d1YW",
        "outputId": "b6ac75ee-1e4d-406c-f355-a00611e13ad8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nulls in order_id: 0\n",
            "Number of nulls in amount_int: 0\n",
            "Number of nulls in order_date_clean: 2261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "58. Confirm correct data types for all columns"
      ],
      "metadata": {
        "id": "562j36hcfvXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzpGbZ3lfwHP",
        "outputId": "d28c5a65-67e7-4556-fb44-a22ee0fb7d9e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- order_id: string (nullable = true)\n",
            " |-- customer_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- amount: string (nullable = true)\n",
            " |-- order_date: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- amount_clean: string (nullable = true)\n",
            " |-- amount_int: integer (nullable = true)\n",
            " |-- order_date_clean: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "59. Validate final record count"
      ],
      "metadata": {
        "id": "q3M6q9Zff39L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aX0_e-2f4mc",
        "outputId": "368bcc58-6b3e-42d9-fbfe-c4f43d385ccb"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "261095"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "60. Document three optimization decisions you made"
      ],
      "metadata": {
        "id": "4qUdHTnYgCrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Caching the df DataFrame: Caching the main DataFrame (df.cache()) after transformations speeds up repeated analytical queries by avoiding recomputing its lineage.\n",
        "\n",
        "\n",
        "2. Repartioning by city: Repartitioning the DataFrame by city before groupBy operations reduces shuffle overhead by co-locating data with the same key, improving aggregation efficiency.\n",
        "\n",
        "\n",
        "3. Writing to Columnar Formats with Partitioning: Saving data to optimized columnar formats like Parquet/ORC and partitioning by city improves read performance for analytical queries, enabling predicate pushdown and reducing I/O.1"
      ],
      "metadata": {
        "id": "yseQxhTBgP3-"
      }
    }
  ]
}