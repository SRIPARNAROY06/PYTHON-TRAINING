{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Auf05OqD3CH"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "spark = SparkSession.builder \\\n",
        ".appName('Customer Purchase Behavior & Loyalty Analysis using PySpark') \\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "dbzOo3URFIz4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 1 – Ingestion & Cleaning\n",
        "\n"
      ],
      "metadata": {
        "id": "7ap1xbQ7GF_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Read orders.csv as all StringType.\n"
      ],
      "metadata": {
        "id": "_iHrMPLnGRTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_raw = spark.read \\\n",
        ".option(\"header\", \"true\") \\\n",
        ".option(\"inferSchema\", \"false\") \\\n",
        ".csv(\"orders.csv\")"
      ],
      "metadata": {
        "id": "avcFfFyWF1b2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Trim text columns.\n"
      ],
      "metadata": {
        "id": "nkTCauy3GUoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_clean_text = orders_raw\\\n",
        ".withColumn(\"city_clean\", trim(col(\"city\")))\\\n",
        ".withColumn(\"category_clean\", trim(col(\"category\")))\\\n",
        ".withColumn(\"product_clean\", trim(col(\"product\")))\n",
        "orders_clean_text.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DpewGy0GWzP",
        "outputId": "f493b4a4-53dd-4bb7-e8e2-869fa9300aa5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+----------+--------------+-------------+\n",
            "|   order_id|customer_id|       city|   category|    product| amount|order_date|   status|city_clean|category_clean|product_clean|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+----------+--------------+-------------+\n",
            "|ORD00000000|    C000000| hyderabad |   grocery |       Oil |invalid|01/01/2024|Cancelled| hyderabad|       grocery|          Oil|\n",
            "|ORD00000001|    C000001|       Pune|    Grocery|      Sugar|  35430|2024-01-02|Completed|      Pune|       Grocery|        Sugar|\n",
            "|ORD00000002|    C000002|       Pune|Electronics|     Mobile|  65358|2024-01-03|Completed|      Pune|   Electronics|       Mobile|\n",
            "|ORD00000003|    C000003|  Bangalore|Electronics|     Laptop|   5558|2024-01-04|Completed| Bangalore|   Electronics|       Laptop|\n",
            "|ORD00000004|    C000004|       Pune|       Home|AirPurifier|  33659|2024-01-05|Completed|      Pune|          Home|  AirPurifier|\n",
            "|ORD00000005|    C000005|      Delhi|    Fashion|      Jeans|   8521|2024-01-06|Completed|     Delhi|       Fashion|        Jeans|\n",
            "|ORD00000006|    C000006|      Delhi|    Grocery|      Sugar|  42383|2024-01-07|Completed|     Delhi|       Grocery|        Sugar|\n",
            "|ORD00000007|    C000007|       Pune|    Grocery|       Rice|  45362|2024-01-08|Completed|      Pune|       Grocery|         Rice|\n",
            "|ORD00000008|    C000008|  Bangalore|    Fashion|      Jeans|  10563|2024-01-09|Completed| Bangalore|       Fashion|        Jeans|\n",
            "|ORD00000009|    C000009|    Kolkata|Electronics|     Laptop|  63715|2024-01-10|Completed|   Kolkata|   Electronics|       Laptop|\n",
            "|ORD00000010|    C000010|  Bangalore|    Grocery|      Sugar|  66576|2024-01-11|Completed| Bangalore|       Grocery|        Sugar|\n",
            "|ORD00000011|    C000011|    Kolkata|Electronics|     Tablet|  50318|12/01/2024|Completed|   Kolkata|   Electronics|       Tablet|\n",
            "|ORD00000012|    C000012|  Bangalore|    Grocery|      Sugar|  84768|2024-01-13|Completed| Bangalore|       Grocery|        Sugar|\n",
            "|ORD00000013|    C000013|       Pune|    Fashion|     TShirt|  79121|2024/01/14|Completed|      Pune|       Fashion|       TShirt|\n",
            "|ORD00000014|    C000014|     Mumbai|Electronics|     Tablet|  79469|2024-01-15|Completed|    Mumbai|   Electronics|       Tablet|\n",
            "|ORD00000015|    C000015|       Pune|Electronics|     Mobile|  81018|2024-01-16|Completed|      Pune|   Electronics|       Mobile|\n",
            "|ORD00000016|    C000016|     Mumbai|       Home|      Mixer|  64225|2024-01-17|Completed|    Mumbai|          Home|        Mixer|\n",
            "|ORD00000017|    C000017| bangalore |    Grocery|        Oil|  69582|2024-01-18|Completed| bangalore|       Grocery|          Oil|\n",
            "|ORD00000018|    C000018|    Kolkata|    Fashion|      Jeans|  50424|2024-01-19|Completed|   Kolkata|       Fashion|        Jeans|\n",
            "|ORD00000019|    C000019|     Mumbai|Electronics|     Mobile|invalid|2024-01-20|Completed|    Mumbai|   Electronics|       Mobile|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+----------+--------------+-------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Normalize city, category, product.\n"
      ],
      "metadata": {
        "id": "eXAUFJwBGXIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_clean_text = orders_clean_text \\\n",
        "    .withColumn(\"city_clean\", lower(col(\"city_clean\"))) \\\n",
        "    .withColumn(\"category_clean\", lower(col(\"category_clean\"))) \\\n",
        "    .withColumn(\"product_clean\", lower(col(\"product_clean\")))\n",
        "orders_clean_text.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvd4kZO5GbNI",
        "outputId": "09023584-71d2-4546-ee42-770ec93bab01"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+----------+--------------+-------------+\n",
            "|   order_id|customer_id|       city|   category|    product| amount|order_date|   status|city_clean|category_clean|product_clean|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+----------+--------------+-------------+\n",
            "|ORD00000000|    C000000| hyderabad |   grocery |       Oil |invalid|01/01/2024|Cancelled| hyderabad|       grocery|          oil|\n",
            "|ORD00000001|    C000001|       Pune|    Grocery|      Sugar|  35430|2024-01-02|Completed|      pune|       grocery|        sugar|\n",
            "|ORD00000002|    C000002|       Pune|Electronics|     Mobile|  65358|2024-01-03|Completed|      pune|   electronics|       mobile|\n",
            "|ORD00000003|    C000003|  Bangalore|Electronics|     Laptop|   5558|2024-01-04|Completed| bangalore|   electronics|       laptop|\n",
            "|ORD00000004|    C000004|       Pune|       Home|AirPurifier|  33659|2024-01-05|Completed|      pune|          home|  airpurifier|\n",
            "|ORD00000005|    C000005|      Delhi|    Fashion|      Jeans|   8521|2024-01-06|Completed|     delhi|       fashion|        jeans|\n",
            "|ORD00000006|    C000006|      Delhi|    Grocery|      Sugar|  42383|2024-01-07|Completed|     delhi|       grocery|        sugar|\n",
            "|ORD00000007|    C000007|       Pune|    Grocery|       Rice|  45362|2024-01-08|Completed|      pune|       grocery|         rice|\n",
            "|ORD00000008|    C000008|  Bangalore|    Fashion|      Jeans|  10563|2024-01-09|Completed| bangalore|       fashion|        jeans|\n",
            "|ORD00000009|    C000009|    Kolkata|Electronics|     Laptop|  63715|2024-01-10|Completed|   kolkata|   electronics|       laptop|\n",
            "|ORD00000010|    C000010|  Bangalore|    Grocery|      Sugar|  66576|2024-01-11|Completed| bangalore|       grocery|        sugar|\n",
            "|ORD00000011|    C000011|    Kolkata|Electronics|     Tablet|  50318|12/01/2024|Completed|   kolkata|   electronics|       tablet|\n",
            "|ORD00000012|    C000012|  Bangalore|    Grocery|      Sugar|  84768|2024-01-13|Completed| bangalore|       grocery|        sugar|\n",
            "|ORD00000013|    C000013|       Pune|    Fashion|     TShirt|  79121|2024/01/14|Completed|      pune|       fashion|       tshirt|\n",
            "|ORD00000014|    C000014|     Mumbai|Electronics|     Tablet|  79469|2024-01-15|Completed|    mumbai|   electronics|       tablet|\n",
            "|ORD00000015|    C000015|       Pune|Electronics|     Mobile|  81018|2024-01-16|Completed|      pune|   electronics|       mobile|\n",
            "|ORD00000016|    C000016|     Mumbai|       Home|      Mixer|  64225|2024-01-17|Completed|    mumbai|          home|        mixer|\n",
            "|ORD00000017|    C000017| bangalore |    Grocery|        Oil|  69582|2024-01-18|Completed| bangalore|       grocery|          oil|\n",
            "|ORD00000018|    C000018|    Kolkata|    Fashion|      Jeans|  50424|2024-01-19|Completed|   kolkata|       fashion|        jeans|\n",
            "|ORD00000019|    C000019|     Mumbai|Electronics|     Mobile|invalid|2024-01-20|Completed|    mumbai|   electronics|       mobile|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+----------+--------------+-------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Clean amount:\n",
        "Remove commas\n",
        "Convert to IntegerType\n",
        "Handle invalid values safely.\n"
      ],
      "metadata": {
        "id": "wrMjgqR9Gbhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_amount_clean = orders_clean_text\\\n",
        ".withColumn(\"amount_clean\", regexp_replace(col(\"amount\"), \",\", \"\"))\\\n",
        ".withColumn(\"amount_clean\",\n",
        "    when(col(\"amount_clean\").rlike(\"^[0-9]+$\"), col(\"amount_clean\").cast(IntegerType()))\\\n",
        "    .otherwise(None)\n",
        ")\n",
        "orders_amount_clean.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KecbcTTJGiXe",
        "outputId": "2c646b4e-fc11-43f3-97e2-00d1b1f80997"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+----------+--------------+-------------+------------+\n",
            "|   order_id|customer_id|       city|   category|    product| amount|order_date|   status|city_clean|category_clean|product_clean|amount_clean|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+----------+--------------+-------------+------------+\n",
            "|ORD00000000|    C000000| hyderabad |   grocery |       Oil |invalid|01/01/2024|Cancelled| hyderabad|       grocery|          Oil|        NULL|\n",
            "|ORD00000001|    C000001|       Pune|    Grocery|      Sugar|  35430|2024-01-02|Completed|      Pune|       Grocery|        Sugar|       35430|\n",
            "|ORD00000002|    C000002|       Pune|Electronics|     Mobile|  65358|2024-01-03|Completed|      Pune|   Electronics|       Mobile|       65358|\n",
            "|ORD00000003|    C000003|  Bangalore|Electronics|     Laptop|   5558|2024-01-04|Completed| Bangalore|   Electronics|       Laptop|        5558|\n",
            "|ORD00000004|    C000004|       Pune|       Home|AirPurifier|  33659|2024-01-05|Completed|      Pune|          Home|  AirPurifier|       33659|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+----------+--------------+-------------+------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Parse order_date into DateType → order_date_clean .\n"
      ],
      "metadata": {
        "id": "x0HSCkSlGiwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_date_clean = orders_amount_clean\\\n",
        ".withColumn(\n",
        "    \"order_date_clean\",\n",
        "    coalesce(\n",
        "    try_to_timestamp(col(\"order_date\"), lit(\"yyyy-MM-dd\")).cast(DateType()),\n",
        "    try_to_timestamp(col(\"order_date\"), lit(\"dd/MM/yyyy\")).cast(DateType()),\n",
        "    try_to_timestamp(col(\"order_date\"), lit(\"yyyy/MM/dd\")).cast(DateType())\n",
        "))\n",
        "orders_date_clean.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5kjroBqGlHn",
        "outputId": "9cc38512-b501-4328-94d4-c6f9a76b5ff7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+----------+--------------+-------------+------------+----------------+\n",
            "|   order_id|customer_id|       city|   category|    product| amount|order_date|   status|city_clean|category_clean|product_clean|amount_clean|order_date_clean|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+----------+--------------+-------------+------------+----------------+\n",
            "|ORD00000000|    C000000| hyderabad |   grocery |       Oil |invalid|01/01/2024|Cancelled| hyderabad|       grocery|          Oil|        NULL|      2024-01-01|\n",
            "|ORD00000001|    C000001|       Pune|    Grocery|      Sugar|  35430|2024-01-02|Completed|      Pune|       Grocery|        Sugar|       35430|      2024-01-02|\n",
            "|ORD00000002|    C000002|       Pune|Electronics|     Mobile|  65358|2024-01-03|Completed|      Pune|   Electronics|       Mobile|       65358|      2024-01-03|\n",
            "|ORD00000003|    C000003|  Bangalore|Electronics|     Laptop|   5558|2024-01-04|Completed| Bangalore|   Electronics|       Laptop|        5558|      2024-01-04|\n",
            "|ORD00000004|    C000004|       Pune|       Home|AirPurifier|  33659|2024-01-05|Completed|      Pune|          Home|  AirPurifier|       33659|      2024-01-05|\n",
            "+-----------+-----------+-----------+-----------+-----------+-------+----------+---------+----------+--------------+-------------+------------+----------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Remove duplicate order_id.\n"
      ],
      "metadata": {
        "id": "QRwrsq9UGmJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders_deduped = orders_date_clean.groupBy(\"order_id\").count().filter(col(\"count\") > 1)\n",
        "orders_deduped.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNaGh8o6GzjR",
        "outputId": "019bc27a-e996-43fc-ae8d-92ad6ed10d58"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|order_id|count|\n",
            "+--------+-----+\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Keep only Completed orders.\n",
        "From this point onward, the dataset is considered clean_orders_df."
      ],
      "metadata": {
        "id": "J7tgIfKiGz8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_orders_df = orders_date_clean.filter(col(\"status\") == \"Completed\")\\\n",
        "    .dropDuplicates([\"order_id\"])\n",
        "clean_orders_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ5Nj45aG3AK",
        "outputId": "e850db1f-05b2-410e-82aa-1ed5e0ac563f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+---------+-----------+-------+------+----------+---------+----------+--------------+-------------+------------+----------------+\n",
            "|   order_id|customer_id|     city|   category|product|amount|order_date|   status|city_clean|category_clean|product_clean|amount_clean|order_date_clean|\n",
            "+-----------+-----------+---------+-----------+-------+------+----------+---------+----------+--------------+-------------+------------+----------------+\n",
            "|ORD00000001|    C000001|     Pune|    Grocery|  Sugar| 35430|2024-01-02|Completed|      Pune|       Grocery|        Sugar|       35430|      2024-01-02|\n",
            "|ORD00000007|    C000007|     Pune|    Grocery|   Rice| 45362|2024-01-08|Completed|      Pune|       Grocery|         Rice|       45362|      2024-01-08|\n",
            "|ORD00000008|    C000008|Bangalore|    Fashion|  Jeans| 10563|2024-01-09|Completed| Bangalore|       Fashion|        Jeans|       10563|      2024-01-09|\n",
            "|ORD00000010|    C000010|Bangalore|    Grocery|  Sugar| 66576|2024-01-11|Completed| Bangalore|       Grocery|        Sugar|       66576|      2024-01-11|\n",
            "|ORD00000011|    C000011|  Kolkata|Electronics| Tablet| 50318|12/01/2024|Completed|   Kolkata|   Electronics|       Tablet|       50318|      2024-01-12|\n",
            "+-----------+-----------+---------+-----------+-------+------+----------+---------+----------+--------------+-------------+------------+----------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 2 – Customer Metrics\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ok54PDtrHUTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Total number of orders.\n"
      ],
      "metadata": {
        "id": "4aaDUL0UHah7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_orders = clean_orders_df.count()\n",
        "print(f\"Total number of orders: {total_orders}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dov0rsKHcDg",
        "outputId": "cf908754-d40a-4140-c136-7fd60d3bf220"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of orders: 285000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Total spending.\n"
      ],
      "metadata": {
        "id": "9Y1aJVzWHcpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_spending = clean_orders_df.select(sum(col(\"amount_clean\"))).collect()[0][0]\n",
        "print(f\"Total spending: {total_spending}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJPDP30rHjjm",
        "outputId": "4ffdc9af-6bdf-4b82-ecc4-9be866b7d9a8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total spending: 11436490724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Average order value.\n"
      ],
      "metadata": {
        "id": "fEw2k3-fHiwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_order_value = total_spending / total_orders\n",
        "print(f\"Average order value: {average_order_value:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t0aZYEDHljJ",
        "outputId": "e5a15e79-7f9e-45ee-8aff-c39df630762c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average order value: 40128.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. First purchase date.\n"
      ],
      "metadata": {
        "id": "bngCpvhhHl0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_purchase_date = clean_orders_df.select(min(col(\"order_date_clean\"))).collect()[0][0]\n",
        "print(f\"First purchase date: {first_purchase_date}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA5ocgVoHnZX",
        "outputId": "fa4eaa6d-f2f1-4fd7-c86b-d8d331fc09c6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First purchase date: 2024-01-02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Last purchase date.\n"
      ],
      "metadata": {
        "id": "dW7wRb-xHnw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_purchase_date = clean_orders_df.select(max(col(\"order_date_clean\"))).collect()[0][0]\n",
        "print(f\"Last purchase date: {last_purchase_date}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcHwSzm0Hp17",
        "outputId": "a9f90290-a0a8-4736-def4-74fd3e11c552"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last purchase date: 2024-02-29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Number of distinct cities ordered from.\n"
      ],
      "metadata": {
        "id": "rtpVoVmjHqJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_cities = clean_orders_df.select(\"city_clean\").distinct().count()\n",
        "print(f\"Number of distinct cities ordered from: {distinct_cities}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tb414QhHrv9",
        "outputId": "3c0fd41d-448b-4dfa-8aa3-50a0e3fd8055"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of distinct cities ordered from: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Number of distinct categories ordered from."
      ],
      "metadata": {
        "id": "FPVy5dyfHsIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_categories = clean_orders_df.select(\"category_clean\").distinct().count()\n",
        "print(f\"Number of distinct categories ordered from: {distinct_categories}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eNyFIbbHs03",
        "outputId": "903c8158-ae8b-4f78-e007-453bc7a557c8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of distinct categories ordered from: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 3 – Customer Segmentation\n"
      ],
      "metadata": {
        "id": "VlVZIcwZH3Yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create customer segments using business logic:\n",
        "\n",
        "Total Spend >= 200000 AND Orders >= 5 → \"VIP\"\n",
        "Total Spend >= 100000 → \"Premium\"\n",
        "Else → \"Regular\"\n",
        "\n",
        "Add a column:\n",
        "\n",
        "customer_segment\n",
        "\n",
        "Count customers in each segment."
      ],
      "metadata": {
        "id": "zPX8L_abH-eT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_metrics = clean_orders_df.groupBy(\"customer_id\").agg(\n",
        "    sum(\"amount_clean\").alias(\"total_spend\"),\n",
        "    count(\"order_id\").alias(\"total_orders\")\n",
        ")\n",
        "\n",
        "customer_segments = customer_metrics.withColumn(\"customer_segment\",\n",
        "    when((col(\"total_spend\") >= 200000) & (col(\"total_orders\") >= 5), \"VIP\")\n",
        "    .when(col(\"total_spend\") >= 100000, \"Premium\")\n",
        "    .otherwise(\"Regular\")\n",
        ")\n",
        "\n",
        "print(\"Customer segments and their counts:\")\n",
        "customer_segments.groupBy(\"customer_segment\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrDsWdg-IARq",
        "outputId": "b9a4c23b-ed5f-413c-89a9-327512469f80"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer segments and their counts:\n",
            "+----------------+-----+\n",
            "|customer_segment|count|\n",
            "+----------------+-----+\n",
            "|         Premium|12485|\n",
            "|         Regular|  623|\n",
            "|             VIP|34392|\n",
            "+----------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 4 – Window Functions\n",
        "\n"
      ],
      "metadata": {
        "id": "MNYptOZjJx_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Rank customers by total spending (overall).\n"
      ],
      "metadata": {
        "id": "vmI66fndJ68a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, dense_rank\n",
        "\n",
        "window_spec_overall = Window.orderBy(col(\"total_spend\").desc())\n",
        "\n",
        "customer_ranking_overall = customer_metrics.withColumn(\"overall_rank\", dense_rank().over(window_spec_overall))\n",
        "\n",
        "print(\"Customers ranked by total spending (overall):\")\n",
        "customer_ranking_overall.orderBy(\"overall_rank\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVaDjZDZJ8r3",
        "outputId": "2b1aebfa-ee76-4ba6-e27f-8ab71977c553"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers ranked by total spending (overall):\n",
            "+-----------+-----------+------------+------------+\n",
            "|customer_id|total_spend|total_orders|overall_rank|\n",
            "+-----------+-----------+------------+------------+\n",
            "|    C043076|     493949|           6|           1|\n",
            "|    C034689|     486879|           6|           2|\n",
            "|    C039985|     484057|           6|           3|\n",
            "|    C026691|     477147|           6|           4|\n",
            "|    C038979|     477138|           6|           5|\n",
            "|    C020762|     474717|           6|           6|\n",
            "|    C044654|     471304|           6|           7|\n",
            "|    C014292|     468617|           6|           8|\n",
            "|    C019565|     467523|           6|           9|\n",
            "|    C045487|     467050|           6|          10|\n",
            "+-----------+-----------+------------+------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Rank customers inside each city by total spending.\n"
      ],
      "metadata": {
        "id": "x1e-8vRfJ-3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_city_metrics = clean_orders_df.groupBy(\"customer_id\", \"city_clean\").agg(\n",
        "    sum(\"amount_clean\").alias(\"total_spend\"),\n",
        "    count(\"order_id\").alias(\"total_orders\")\n",
        ")\n",
        "\n",
        "window_spec_city = Window.partitionBy(\"city_clean\").orderBy(col(\"total_spend\").desc())\n",
        "\n",
        "customer_ranking_city = customer_city_metrics.withColumn(\"city_rank\", dense_rank().over(window_spec_city))\n",
        "\n",
        "print(\"Customers ranked by total spending within each city:\")\n",
        "customer_ranking_city.orderBy(\"city_clean\", \"city_rank\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgCkzA-DJ8yF",
        "outputId": "65bf5c5d-3369-4dc1-a01b-79db57dfdfa9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers ranked by total spending within each city:\n",
            "+-----------+----------+-----------+------------+---------+\n",
            "|customer_id|city_clean|total_spend|total_orders|city_rank|\n",
            "+-----------+----------+-----------+------------+---------+\n",
            "|    C011518| Bangalore|     332527|           5|        1|\n",
            "|    C024935| Bangalore|     315622|           4|        2|\n",
            "|    C025451| Bangalore|     303208|           4|        3|\n",
            "|    C008486| Bangalore|     300843|           5|        4|\n",
            "|    C039191| Bangalore|     294970|           4|        5|\n",
            "|    C006114| Bangalore|     290915|           4|        6|\n",
            "|    C028773| Bangalore|     285105|           4|        7|\n",
            "|    C045363| Bangalore|     283538|           4|        8|\n",
            "|    C043646| Bangalore|     272357|           4|        9|\n",
            "|    C032542| Bangalore|     262148|           4|       10|\n",
            "+-----------+----------+-----------+------------+---------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Identify top 3 customers per city.\n"
      ],
      "metadata": {
        "id": "gXZhfyNHKAYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_3_customers_per_city = customer_ranking_city.filter(col(\"city_rank\") <= 3)\n",
        "\n",
        "print(\"Top 3 customers per city:\")\n",
        "top_3_customers_per_city.orderBy(\"city_clean\", \"city_rank\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlv9Ui8dKC_z",
        "outputId": "8f402dee-aa6f-4230-8c65-f30578d08e59"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 customers per city:\n",
            "+-----------+----------+-----------+------------+---------+\n",
            "|customer_id|city_clean|total_spend|total_orders|city_rank|\n",
            "+-----------+----------+-----------+------------+---------+\n",
            "|    C011518| Bangalore|     332527|           5|        1|\n",
            "|    C024935| Bangalore|     315622|           4|        2|\n",
            "|    C025451| Bangalore|     303208|           4|        3|\n",
            "|    C028121|   Chennai|     340890|           5|        1|\n",
            "|    C027841|   Chennai|     287392|           5|        2|\n",
            "|    C030712|   Chennai|     284466|           4|        3|\n",
            "|    C016309|     Delhi|     325001|           5|        1|\n",
            "|    C022599|     Delhi|     314625|           4|        2|\n",
            "|    C018688|     Delhi|     306692|           4|        3|\n",
            "|    C032833| Hyderabad|     318097|           5|        1|\n",
            "|    C023269| Hyderabad|     292791|           5|        2|\n",
            "|    C013263| Hyderabad|     291679|           4|        3|\n",
            "|    C032246|   Kolkata|     304480|           4|        1|\n",
            "|    C028450|   Kolkata|     296653|           4|        2|\n",
            "|    C004753|   Kolkata|     273630|           4|        3|\n",
            "|    C048696|    Mumbai|     314320|           5|        1|\n",
            "|    C047887|    Mumbai|     307401|           4|        2|\n",
            "|    C022721|    Mumbai|     306800|           4|        3|\n",
            "|    C002564|      Pune|     315172|           5|        1|\n",
            "|    C023148|      Pune|     310061|           4|        2|\n",
            "+-----------+----------+-----------+------------+---------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Identify top 10 customers across all cities.\n",
        "This phase must use:\n",
        "\n",
        "Window.partitionBy()"
      ],
      "metadata": {
        "id": "2_NFdaQCKDWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec_global = Window.partitionBy(lit(1)).orderBy(col(\"total_spend\").desc())\n",
        "\n",
        "top_10_customers_overall = customer_metrics.withColumn(\"global_rank\", dense_rank().over(window_spec_global))\n",
        "\n",
        "print(\"Top 10 customers across all cities:\")\n",
        "top_10_customers_overall.filter(col(\"global_rank\") <= 10).orderBy(\"global_rank\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwauIw2BKEfR",
        "outputId": "14d880ed-cc3e-4659-e30e-00db7245edd0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 customers across all cities:\n",
            "+-----------+-----------+------------+-----------+\n",
            "|customer_id|total_spend|total_orders|global_rank|\n",
            "+-----------+-----------+------------+-----------+\n",
            "|    C043076|     493949|           6|          1|\n",
            "|    C034689|     486879|           6|          2|\n",
            "|    C039985|     484057|           6|          3|\n",
            "|    C026691|     477147|           6|          4|\n",
            "|    C038979|     477138|           6|          5|\n",
            "|    C020762|     474717|           6|          6|\n",
            "|    C044654|     471304|           6|          7|\n",
            "|    C014292|     468617|           6|          8|\n",
            "|    C019565|     467523|           6|          9|\n",
            "|    C045487|     467050|           6|         10|\n",
            "+-----------+-----------+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 5 – Customer Loyalty Analysis\n",
        "Define loyalty:\n",
        "A loyal customer is one who:\n",
        "Has purchases on at least 3 different dates\n",
        "Has ordered from at least 2 different categories\n",
        "Tasks:\n"
      ],
      "metadata": {
        "id": "v4ey9mMrKIni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Identify loyal customers.\n"
      ],
      "metadata": {
        "id": "UHBmw9bJKLkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import countDistinct, col\n",
        "\n",
        "customer_loyalty_metrics = clean_orders_df.groupBy(\"customer_id\").agg(\n",
        "    countDistinct(\"order_date_clean\").alias(\"distinct_purchase_dates\"),\n",
        "    countDistinct(\"category_clean\").alias(\"distinct_categories_ordered\")\n",
        ")\n",
        "\n",
        "loyal_customers = customer_loyalty_metrics.filter(\n",
        "    (col(\"distinct_purchase_dates\") >= 3) & (col(\"distinct_categories_ordered\") >= 2)\n",
        ")\n",
        "\n",
        "print(\"Loyal Customers:\")\n",
        "loyal_customers.show(5)\n",
        "print(f\"Total loyal customers: {loyal_customers.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k7tPg85KLLr",
        "outputId": "fb1d412a-415b-4b63-9527-408d1de4db90"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loyal Customers:\n",
            "+-----------+-----------------------+---------------------------+\n",
            "|customer_id|distinct_purchase_dates|distinct_categories_ordered|\n",
            "+-----------+-----------------------+---------------------------+\n",
            "|    C041802|                      3|                          4|\n",
            "|    C027664|                      3|                          3|\n",
            "|    C030828|                      3|                          3|\n",
            "|    C030695|                      3|                          4|\n",
            "|    C041085|                      3|                          4|\n",
            "+-----------+-----------------------+---------------------------+\n",
            "only showing top 5 rows\n",
            "Total loyal customers: 47450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Count loyal customers per city.\n"
      ],
      "metadata": {
        "id": "eVNMIEG2KNwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_city_map = clean_orders_df.select(\"customer_id\", \"city_clean\").distinct()\n",
        "\n",
        "loyal_customers_with_city = loyal_customers.join(customer_city_map, \"customer_id\", \"inner\")\n",
        "\n",
        "loyal_customers_per_city = loyal_customers_with_city.groupBy(\"city_clean\").count().alias(\"loyal_customer_count\")\n",
        "\n",
        "print(\"Loyal Customers per City:\")\n",
        "loyal_customers_per_city.orderBy(col(\"count\").desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qj-ySLrcKPE3",
        "outputId": "d9084858-3370-4cb1-d120-b354574f066d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loyal Customers per City:\n",
            "+----------+-----+\n",
            "|city_clean|count|\n",
            "+----------+-----+\n",
            "| Hyderabad|27683|\n",
            "|     Delhi|27669|\n",
            "|      Pune|27617|\n",
            "|   Chennai|27519|\n",
            "|   Kolkata|27477|\n",
            "|    Mumbai|27454|\n",
            "| Bangalore|27405|\n",
            "| hyderabad| 2486|\n",
            "|   chennai| 2415|\n",
            "|      pune| 2391|\n",
            "|   kolkata| 2390|\n",
            "|     delhi| 2379|\n",
            "|    mumbai| 2348|\n",
            "| bangalore| 2345|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Compare loyal vs non-loyal customer revenue contribution."
      ],
      "metadata": {
        "id": "6FikNURjKPYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loyal_customer_ids = loyal_customers.select(\"customer_id\")\n",
        "\n",
        "revenue_loyal = customer_metrics.join(loyal_customer_ids, \"customer_id\", \"inner\")\\\n",
        "    .select(sum(\"total_spend\")).collect()[0][0]\n",
        "\n",
        "revenue_non_loyal = customer_metrics.join(loyal_customer_ids, \"customer_id\", \"left_anti\")\\\n",
        "    .select(sum(\"total_spend\")).collect()[0][0]\n",
        "\n",
        "print(f\"Total revenue from loyal customers: {revenue_loyal}\")\n",
        "print(f\"Total revenue from non-loyal customers: {revenue_non_loyal}\")\n",
        "print(f\"Difference (Loyal - Non-Loyal): {revenue_loyal - revenue_non_loyal}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk7VuMLyKQcp",
        "outputId": "692ffe2b-1e3f-4ca8-ec13-ba915f0dfaf6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total revenue from loyal customers: 11423898941\n",
            "Total revenue from non-loyal customers: 12591783\n",
            "Difference (Loyal - Non-Loyal): 11411307158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 6 – Time-Based Analysis\n",
        "Using order_date_clean:\n"
      ],
      "metadata": {
        "id": "qA0PIlKuKQ-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Compute monthly revenue per city.\n"
      ],
      "metadata": {
        "id": "GmahKaZZKZiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_revenue_city = clean_orders_df.filter(col(\"order_date_clean\").isNotNull())\\\n",
        "    .withColumn(\"city_clean\", lower(col(\"city_clean\")))\\\n",
        "    .withColumn(\"order_month\", date_trunc(\"month\", col(\"order_date_clean\")))\\\n",
        "    .groupBy(\"order_month\", \"city_clean\").agg(sum(\"amount_clean\").alias(\"monthly_revenue\"))\n",
        "\n",
        "print(\"Monthly Revenue per City:\")\n",
        "monthly_revenue_city.orderBy(\"order_month\", col(\"monthly_revenue\").desc()).show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anK1DhxZKY3a",
        "outputId": "307c59fe-a06b-4862-8b36-5f606ba63b9b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monthly Revenue per City:\n",
            "+-------------------+----------+---------------+\n",
            "|        order_month|city_clean|monthly_revenue|\n",
            "+-------------------+----------+---------------+\n",
            "|2024-01-01 00:00:00|      pune|      833507124|\n",
            "|2024-01-01 00:00:00| hyderabad|      833063605|\n",
            "|2024-01-01 00:00:00|   kolkata|      824920456|\n",
            "|2024-01-01 00:00:00| bangalore|      822339117|\n",
            "|2024-01-01 00:00:00|   chennai|      818567389|\n",
            "|2024-01-01 00:00:00|     delhi|      817332633|\n",
            "|2024-01-01 00:00:00|    mumbai|      816636150|\n",
            "|2024-02-01 00:00:00|     delhi|      805877007|\n",
            "|2024-02-01 00:00:00|      pune|      797779557|\n",
            "|2024-02-01 00:00:00|   chennai|      796361427|\n",
            "+-------------------+----------+---------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Compute monthly order count per category.\n"
      ],
      "metadata": {
        "id": "38Q_pbRzKdpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_order_count_category = clean_orders_df.filter(col(\"order_date_clean\").isNotNull())\\\n",
        "    .withColumn(\"order_month\", date_trunc(\"month\", col(\"order_date_clean\")))\\\n",
        "    .withColumn(\"category_clean\", lower(col(\"category_clean\")))\\\n",
        "    .groupBy(\"order_month\", \"category_clean\").agg(count(\"order_id\").alias(\"monthly_order_count\"))\n",
        "\n",
        "print(\"Monthly Order Count per Category:\")\n",
        "monthly_order_count_category.orderBy(\"order_month\", col(\"monthly_order_count\").desc()).show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2739j3bIKfE3",
        "outputId": "368144c8-5142-4596-a3e6-980a764c3797"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monthly Order Count per Category:\n",
            "+-------------------+--------------+-------------------+\n",
            "|        order_month|category_clean|monthly_order_count|\n",
            "+-------------------+--------------+-------------------+\n",
            "|2024-01-01 00:00:00|          home|              36163|\n",
            "|2024-01-01 00:00:00|       grocery|              36018|\n",
            "|2024-01-01 00:00:00|   electronics|              35994|\n",
            "|2024-01-01 00:00:00|       fashion|              35571|\n",
            "|2024-02-01 00:00:00|   electronics|              34766|\n",
            "|2024-02-01 00:00:00|       fashion|              34720|\n",
            "|2024-02-01 00:00:00|       grocery|              34672|\n",
            "|2024-02-01 00:00:00|          home|              34631|\n",
            "+-------------------+--------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Identify growth or decline trends."
      ],
      "metadata": {
        "id": "iwpJNkQaKfaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec_month_city = Window.partitionBy(\"city_clean\").orderBy(\"order_month\")\n",
        "\n",
        "revenue_trends = monthly_revenue_city.withColumn(\"previous_month_revenue\", lag(\"monthly_revenue\", 1).over(window_spec_month_city))\n",
        "\n",
        "revenue_trends = revenue_trends.withColumn(\"revenue_change_percent\",\n",
        "    when(col(\"previous_month_revenue\").isNotNull(),\n",
        "        (col(\"monthly_revenue\") - col(\"previous_month_revenue\")) / col(\"previous_month_revenue\") * 100\n",
        "    ).otherwise(None)\n",
        ")\n",
        "\n",
        "print(\"Monthly Revenue Growth/Decline Trends:\")\n",
        "revenue_trends.orderBy(\"city_clean\", \"order_month\").show()\n",
        "\n",
        "window_spec_month_category = Window.partitionBy(\"category_clean\").orderBy(\"order_month\")\n",
        "\n",
        "order_count_trends = monthly_order_count_category.withColumn(\"previous_month_orders\", lag(\"monthly_order_count\", 1).over(window_spec_month_category))\n",
        "\n",
        "order_count_trends = order_count_trends.withColumn(\"order_count_change_percent\",\n",
        "    when(col(\"previous_month_orders\").isNotNull(),\n",
        "        (col(\"monthly_order_count\") - col(\"previous_month_orders\")) / col(\"previous_month_orders\") * 100\n",
        "    ).otherwise(None)\n",
        ")\n",
        "\n",
        "print(\"\\nMonthly Order Count Growth/Decline Trends:\")\n",
        "order_count_trends.orderBy(\"category_clean\", \"order_month\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xPyskQZKf-A",
        "outputId": "aa153c85-107a-4a6f-c7c8-f7cb080a3b11"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monthly Revenue Growth/Decline Trends:\n",
            "+-------------------+----------+---------------+----------------------+----------------------+\n",
            "|        order_month|city_clean|monthly_revenue|previous_month_revenue|revenue_change_percent|\n",
            "+-------------------+----------+---------------+----------------------+----------------------+\n",
            "|2024-01-01 00:00:00| bangalore|      822339117|                  NULL|                  NULL|\n",
            "|2024-02-01 00:00:00| bangalore|      792163305|             822339117|   -3.6695094975033276|\n",
            "|2024-01-01 00:00:00|   chennai|      818567389|                  NULL|                  NULL|\n",
            "|2024-02-01 00:00:00|   chennai|      796361427|             818567389|   -2.7127836142028374|\n",
            "|2024-01-01 00:00:00|     delhi|      817332633|                  NULL|                  NULL|\n",
            "|2024-02-01 00:00:00|     delhi|      805877007|             817332633|   -1.4015867637576633|\n",
            "|2024-01-01 00:00:00| hyderabad|      833063605|                  NULL|                  NULL|\n",
            "|2024-02-01 00:00:00| hyderabad|      796252807|             833063605|    -4.418725986715024|\n",
            "|2024-01-01 00:00:00|   kolkata|      824920456|                  NULL|                  NULL|\n",
            "|2024-02-01 00:00:00|   kolkata|      785096186|             824920456|    -4.827649709780018|\n",
            "|2024-01-01 00:00:00|    mumbai|      816636150|                  NULL|                  NULL|\n",
            "|2024-02-01 00:00:00|    mumbai|      795736235|             816636150|   -2.5592688983949583|\n",
            "|2024-01-01 00:00:00|      pune|      833507124|                  NULL|                  NULL|\n",
            "|2024-02-01 00:00:00|      pune|      797779557|             833507124|    -4.286414113480331|\n",
            "+-------------------+----------+---------------+----------------------+----------------------+\n",
            "\n",
            "\n",
            "Monthly Order Count Growth/Decline Trends:\n",
            "+-------------------+--------------+-------------------+---------------------+--------------------------+\n",
            "|        order_month|category_clean|monthly_order_count|previous_month_orders|order_count_change_percent|\n",
            "+-------------------+--------------+-------------------+---------------------+--------------------------+\n",
            "|2024-01-01 00:00:00|   electronics|              35994|                 NULL|                      NULL|\n",
            "|2024-02-01 00:00:00|   electronics|              34766|                35994|        -3.411679724398511|\n",
            "|2024-01-01 00:00:00|       fashion|              35571|                 NULL|                      NULL|\n",
            "|2024-02-01 00:00:00|       fashion|              34720|                35571|        -2.392398301987574|\n",
            "|2024-01-01 00:00:00|       grocery|              36018|                 NULL|                      NULL|\n",
            "|2024-02-01 00:00:00|       grocery|              34672|                36018|        -3.737020378699539|\n",
            "|2024-01-01 00:00:00|          home|              36163|                 NULL|                      NULL|\n",
            "|2024-02-01 00:00:00|          home|              34631|                36163|        -4.236374194618809|\n",
            "+-------------------+--------------+-------------------+---------------------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 7 – Performance Engineering"
      ],
      "metadata": {
        "id": "SxC1UdGvKlzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Identify which DataFrames are reused.\n"
      ],
      "metadata": {
        "id": "_Ks37MKdKp50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Reused DataFrames:\")\n",
        "print(\"- `clean_orders_df`: Used for various customer metrics, customer segmentation, loyalty analysis, and time-based analysis.\")\n",
        "print(\"- `customer_metrics`: Used for customer segmentation, overall customer ranking, and loyal vs. non-loyal customer revenue comparison.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kJ3ZeYcKriv",
        "outputId": "d89e1f6e-cca9-4a58-ba4a-af0e82323707"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reused DataFrames:\n",
            "- `clean_orders_df`: Used for various customer metrics, customer segmentation, loyalty analysis, and time-based analysis.\n",
            "- `customer_metrics`: Used for customer segmentation, overall customer ranking, and loyal vs. non-loyal customer revenue comparison.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Apply caching.\n"
      ],
      "metadata": {
        "id": "bu9CsGnmKsRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_orders_df.cache()\n",
        "customer_metrics.cache()\n",
        "print(\"DataFrames `clean_orders_df` and `customer_metrics` have been cached.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ab1Yo7xKt5n",
        "outputId": "85a548f8-686f-445a-99a3-395d26b46f9d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrames `clean_orders_df` and `customer_metrics` have been cached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Use explain(True) on:\n",
        "Customer aggregation\n",
        "Window ranking\n"
      ],
      "metadata": {
        "id": "dQXx1dG3KuMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Execution plan for Customer Aggregation (customer_metrics):\")\n",
        "customer_metrics.explain(True)\n",
        "\n",
        "print(\"\\nExecution plan for Overall Window Ranking (customer_ranking_overall):\")\n",
        "customer_ranking_overall.explain(True)\n",
        "\n",
        "print(\"\\nExecution plan for City-wise Window Ranking (customer_ranking_city):\")\n",
        "customer_ranking_city.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7ieHdhwKvsn",
        "outputId": "23122a90-467d-4cd6-c2da-1ba1969ffe45"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution plan for Customer Aggregation (customer_metrics):\n",
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['customer_id], ['customer_id, 'sum('amount_clean) AS total_spend#699, 'count('order_id) AS total_orders#700]\n",
            "+- Deduplicate [order_id#17]\n",
            "   +- Filter (status#24 = Completed)\n",
            "      +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, coalesce(cast(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "         +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) ELSE cast(null as int) END AS amount_clean#72]\n",
            "            +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "               +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, trim(product#21, None) AS product_clean#28]\n",
            "                  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, trim(category#20, None) AS category_clean#27]\n",
            "                     +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26]\n",
            "                        +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "customer_id: string, total_spend: bigint, total_orders: bigint\n",
            "Aggregate [customer_id#18], [customer_id#18, sum(amount_clean#72) AS total_spend#699L, count(order_id#17) AS total_orders#700L]\n",
            "+- Deduplicate [order_id#17]\n",
            "   +- Filter (status#24 = Completed)\n",
            "      +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, coalesce(cast(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "         +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) ELSE cast(null as int) END AS amount_clean#72]\n",
            "            +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "               +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, trim(product#21, None) AS product_clean#28]\n",
            "                  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, trim(category#20, None) AS category_clean#27]\n",
            "                     +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26]\n",
            "                        +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "InMemoryRelation [customer_id#18, total_spend#699L, total_orders#700L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "   +- AdaptiveSparkPlan isFinalPlan=false\n",
            "      +- HashAggregate(keys=[customer_id#18], functions=[sum(amount_clean#72), count(order_id#17)], output=[customer_id#18, total_spend#699L, total_orders#700L])\n",
            "         +- Exchange hashpartitioning(customer_id#18, 200), ENSURE_REQUIREMENTS, [plan_id=6606]\n",
            "            +- HashAggregate(keys=[customer_id#18], functions=[partial_sum(amount_clean#72), partial_count(order_id#17)], output=[customer_id#18, sum#2827L, count#754L])\n",
            "               +- InMemoryTableScan [order_id#17, customer_id#18, amount_clean#72]\n",
            "                     +- InMemoryRelation [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, order_date_clean#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                           +- AdaptiveSparkPlan isFinalPlan=false\n",
            "                              +- SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                                 +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                    +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6592]\n",
            "                                       +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                          +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                             +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                                +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                                   +- Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                                      +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- InMemoryTableScan [customer_id#18, total_spend#699L, total_orders#700L]\n",
            "      +- InMemoryRelation [customer_id#18, total_spend#699L, total_orders#700L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "            +- AdaptiveSparkPlan isFinalPlan=false\n",
            "               +- HashAggregate(keys=[customer_id#18], functions=[sum(amount_clean#72), count(order_id#17)], output=[customer_id#18, total_spend#699L, total_orders#700L])\n",
            "                  +- Exchange hashpartitioning(customer_id#18, 200), ENSURE_REQUIREMENTS, [plan_id=6606]\n",
            "                     +- HashAggregate(keys=[customer_id#18], functions=[partial_sum(amount_clean#72), partial_count(order_id#17)], output=[customer_id#18, sum#2827L, count#754L])\n",
            "                        +- InMemoryTableScan [order_id#17, customer_id#18, amount_clean#72]\n",
            "                              +- InMemoryRelation [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, order_date_clean#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                                    +- AdaptiveSparkPlan isFinalPlan=false\n",
            "                                       +- SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                                          +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                             +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6592]\n",
            "                                                +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                                   +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                                      +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                                         +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                                            +- Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                                               +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "\n",
            "\n",
            "Execution plan for Overall Window Ranking (customer_ranking_overall):\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(overall_rank, 'dense_rank() windowspecdefinition('total_spend DESC NULLS LAST, unspecifiedframe$()), None)]\n",
            "+- Aggregate [customer_id#18], [customer_id#18, sum(amount_clean#72) AS total_spend#699L, count(order_id#17) AS total_orders#700L]\n",
            "   +- Deduplicate [order_id#17]\n",
            "      +- Filter (status#24 = Completed)\n",
            "         +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, coalesce(cast(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "            +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) ELSE cast(null as int) END AS amount_clean#72]\n",
            "               +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, trim(product#21, None) AS product_clean#28]\n",
            "                     +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, trim(category#20, None) AS category_clean#27]\n",
            "                        +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26]\n",
            "                           +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "customer_id: string, total_spend: bigint, total_orders: bigint, overall_rank: int\n",
            "Project [customer_id#18, total_spend#699L, total_orders#700L, overall_rank#798]\n",
            "+- Project [customer_id#18, total_spend#699L, total_orders#700L, overall_rank#798, overall_rank#798]\n",
            "   +- Window [dense_rank(total_spend#699L) windowspecdefinition(total_spend#699L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS overall_rank#798], [total_spend#699L DESC NULLS LAST]\n",
            "      +- Project [customer_id#18, total_spend#699L, total_orders#700L]\n",
            "         +- Aggregate [customer_id#18], [customer_id#18, sum(amount_clean#72) AS total_spend#699L, count(order_id#17) AS total_orders#700L]\n",
            "            +- Deduplicate [order_id#17]\n",
            "               +- Filter (status#24 = Completed)\n",
            "                  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, coalesce(cast(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                     +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) ELSE cast(null as int) END AS amount_clean#72]\n",
            "                        +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, trim(product#21, None) AS product_clean#28]\n",
            "                              +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, trim(category#20, None) AS category_clean#27]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26]\n",
            "                                    +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Window [dense_rank(total_spend#699L) windowspecdefinition(total_spend#699L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS overall_rank#798], [total_spend#699L DESC NULLS LAST]\n",
            "+- InMemoryRelation [customer_id#18, total_spend#699L, total_orders#700L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "      +- AdaptiveSparkPlan isFinalPlan=false\n",
            "         +- HashAggregate(keys=[customer_id#18], functions=[sum(amount_clean#72), count(order_id#17)], output=[customer_id#18, total_spend#699L, total_orders#700L])\n",
            "            +- Exchange hashpartitioning(customer_id#18, 200), ENSURE_REQUIREMENTS, [plan_id=6606]\n",
            "               +- HashAggregate(keys=[customer_id#18], functions=[partial_sum(amount_clean#72), partial_count(order_id#17)], output=[customer_id#18, sum#2827L, count#754L])\n",
            "                  +- InMemoryTableScan [order_id#17, customer_id#18, amount_clean#72]\n",
            "                        +- InMemoryRelation [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, order_date_clean#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                              +- AdaptiveSparkPlan isFinalPlan=false\n",
            "                                 +- SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                                    +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                       +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6592]\n",
            "                                          +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                             +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                                +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                                   +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                                      +- Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                                         +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Window [dense_rank(total_spend#699L) windowspecdefinition(total_spend#699L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS overall_rank#798], [total_spend#699L DESC NULLS LAST]\n",
            "   +- Sort [total_spend#699L DESC NULLS LAST], false, 0\n",
            "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=6620]\n",
            "         +- InMemoryTableScan [customer_id#18, total_spend#699L, total_orders#700L]\n",
            "               +- InMemoryRelation [customer_id#18, total_spend#699L, total_orders#700L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                     +- AdaptiveSparkPlan isFinalPlan=false\n",
            "                        +- HashAggregate(keys=[customer_id#18], functions=[sum(amount_clean#72), count(order_id#17)], output=[customer_id#18, total_spend#699L, total_orders#700L])\n",
            "                           +- Exchange hashpartitioning(customer_id#18, 200), ENSURE_REQUIREMENTS, [plan_id=6606]\n",
            "                              +- HashAggregate(keys=[customer_id#18], functions=[partial_sum(amount_clean#72), partial_count(order_id#17)], output=[customer_id#18, sum#2827L, count#754L])\n",
            "                                 +- InMemoryTableScan [order_id#17, customer_id#18, amount_clean#72]\n",
            "                                       +- InMemoryRelation [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, order_date_clean#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                                             +- AdaptiveSparkPlan isFinalPlan=false\n",
            "                                                +- SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                                                   +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                                      +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6592]\n",
            "                                                         +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                                            +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                                               +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                                                  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                                                     +- Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                                                        +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "\n",
            "\n",
            "Execution plan for City-wise Window Ranking (customer_ranking_city):\n",
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(city_rank, 'dense_rank() windowspecdefinition('city_clean, 'total_spend DESC NULLS LAST, unspecifiedframe$()), None)]\n",
            "+- Aggregate [customer_id#18, city_clean#26], [customer_id#18, city_clean#26, sum(amount_clean#72) AS total_spend#884L, count(order_id#17) AS total_orders#885L]\n",
            "   +- Deduplicate [order_id#17]\n",
            "      +- Filter (status#24 = Completed)\n",
            "         +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, coalesce(cast(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "            +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) ELSE cast(null as int) END AS amount_clean#72]\n",
            "               +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, trim(product#21, None) AS product_clean#28]\n",
            "                     +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, trim(category#20, None) AS category_clean#27]\n",
            "                        +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26]\n",
            "                           +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "customer_id: string, city_clean: string, total_spend: bigint, total_orders: bigint, city_rank: int\n",
            "Project [customer_id#18, city_clean#26, total_spend#884L, total_orders#885L, city_rank#901]\n",
            "+- Project [customer_id#18, city_clean#26, total_spend#884L, total_orders#885L, city_rank#901, city_rank#901]\n",
            "   +- Window [dense_rank(total_spend#884L) windowspecdefinition(city_clean#26, total_spend#884L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS city_rank#901], [city_clean#26], [total_spend#884L DESC NULLS LAST]\n",
            "      +- Project [customer_id#18, city_clean#26, total_spend#884L, total_orders#885L]\n",
            "         +- Aggregate [customer_id#18, city_clean#26], [customer_id#18, city_clean#26, sum(amount_clean#72) AS total_spend#884L, count(order_id#17) AS total_orders#885L]\n",
            "            +- Deduplicate [order_id#17]\n",
            "               +- Filter (status#24 = Completed)\n",
            "                  +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, coalesce(cast(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                     +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) ELSE cast(null as int) END AS amount_clean#72]\n",
            "                        +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, trim(product#21, None) AS product_clean#28]\n",
            "                              +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, trim(category#20, None) AS category_clean#27]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26]\n",
            "                                    +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Window [dense_rank(total_spend#884L) windowspecdefinition(city_clean#26, total_spend#884L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS city_rank#901], [city_clean#26], [total_spend#884L DESC NULLS LAST]\n",
            "+- Aggregate [customer_id#18, city_clean#26], [customer_id#18, city_clean#26, sum(amount_clean#72) AS total_spend#884L, count(order_id#17) AS total_orders#885L]\n",
            "   +- Project [order_id#17, customer_id#18, city_clean#26, amount_clean#72]\n",
            "      +- InMemoryRelation [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, order_date_clean#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "            +- AdaptiveSparkPlan isFinalPlan=false\n",
            "               +- SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                  +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                     +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6592]\n",
            "                        +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                           +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                              +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                    +- Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                       +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Window [dense_rank(total_spend#884L) windowspecdefinition(city_clean#26, total_spend#884L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS city_rank#901], [city_clean#26], [total_spend#884L DESC NULLS LAST]\n",
            "   +- Sort [city_clean#26 ASC NULLS FIRST, total_spend#884L DESC NULLS LAST], false, 0\n",
            "      +- Exchange hashpartitioning(city_clean#26, 200), ENSURE_REQUIREMENTS, [plan_id=6641]\n",
            "         +- HashAggregate(keys=[customer_id#18, city_clean#26], functions=[sum(amount_clean#72), count(order_id#17)], output=[customer_id#18, city_clean#26, total_spend#884L, total_orders#885L])\n",
            "            +- Exchange hashpartitioning(customer_id#18, city_clean#26, 200), ENSURE_REQUIREMENTS, [plan_id=6638]\n",
            "               +- HashAggregate(keys=[customer_id#18, city_clean#26], functions=[partial_sum(amount_clean#72), partial_count(order_id#17)], output=[customer_id#18, city_clean#26, sum#3141L, count#942L])\n",
            "                  +- InMemoryTableScan [order_id#17, customer_id#18, city_clean#26, amount_clean#72]\n",
            "                        +- InMemoryRelation [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, order_date_clean#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                              +- AdaptiveSparkPlan isFinalPlan=false\n",
            "                                 +- SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                                    +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                       +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6592]\n",
            "                                          +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                             +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                                +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                                   +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                                      +- Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                                         +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Identify shuffle stages.\n"
      ],
      "metadata": {
        "id": "W7PZX8qLKwCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shuffle stages (indicated by 'Exchange hashpartitioning' or 'Exchange SinglePartition'):\")\n",
        "print(\"1. Customer Aggregation (`customer_metrics`):\")\n",
        "print(\"   - `Exchange hashpartitioning(customer_id#18, 200)`: This shuffle happens before the final hash aggregation to ensure all data for a given customer_id is on the same partition, allowing `sum` and `count` to be computed correctly.\")\n",
        "print(\"   - `Exchange hashpartitioning(order_id#17, 200)`: This shuffle happens during the deduplication process within `clean_orders_df` creation to group records by order_id.\")\n",
        "\n",
        "print(\"\\n2. Overall Window Ranking (`customer_ranking_overall`):\")\n",
        "print(\"   - `Exchange SinglePartition`: This shuffle occurs before the window function application when ordering globally. All data is collected into a single partition to allow for a global ranking based on `total_spend`. This can be a bottleneck for very large datasets.\")\n",
        "\n",
        "print(\"\\n3. City-wise Window Ranking (`customer_ranking_city`):\")\n",
        "print(\"   - `Exchange hashpartitioning(city_clean#26, 200)`: This shuffle occurs before the window function application to group data by `city_clean`. This ensures that the ranking for each city happens independently within its own partition.\")\n",
        "print(\"   - `Exchange hashpartitioning(customer_id#18, city_clean#26, 200)`: This shuffle happens during the aggregation for `customer_city_metrics` to group data by both `customer_id` and `city_clean`.\")\n",
        "print(\"   - Additional `Exchange hashpartitioning(order_id#17, 200)` stages originating from `clean_orders_df` as it's reused.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THvNiOmOKxWV",
        "outputId": "f03252e9-6946-465f-9ce4-31feb7dd2ab8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffle stages (indicated by 'Exchange hashpartitioning' or 'Exchange SinglePartition'):\n",
            "1. Customer Aggregation (`customer_metrics`):\n",
            "   - `Exchange hashpartitioning(customer_id#18, 200)`: This shuffle happens before the final hash aggregation to ensure all data for a given customer_id is on the same partition, allowing `sum` and `count` to be computed correctly.\n",
            "   - `Exchange hashpartitioning(order_id#17, 200)`: This shuffle happens during the deduplication process within `clean_orders_df` creation to group records by order_id.\n",
            "\n",
            "2. Overall Window Ranking (`customer_ranking_overall`):\n",
            "   - `Exchange SinglePartition`: This shuffle occurs before the window function application when ordering globally. All data is collected into a single partition to allow for a global ranking based on `total_spend`. This can be a bottleneck for very large datasets.\n",
            "\n",
            "3. City-wise Window Ranking (`customer_ranking_city`):\n",
            "   - `Exchange hashpartitioning(city_clean#26, 200)`: This shuffle occurs before the window function application to group data by `city_clean`. This ensures that the ranking for each city happens independently within its own partition.\n",
            "   - `Exchange hashpartitioning(customer_id#18, city_clean#26, 200)`: This shuffle happens during the aggregation for `customer_city_metrics` to group data by both `customer_id` and `city_clean`.\n",
            "   - Additional `Exchange hashpartitioning(order_id#17, 200)` stages originating from `clean_orders_df` as it's reused.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Justify any repartitioning strategy."
      ],
      "metadata": {
        "id": "JKaAinB4KxrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Justification of Repartitioning Strategy:\")\n",
        "print(\"Spark performs repartitioning (shuffles) for several key reasons, primarily to ensure data correctness and enable parallel processing for certain operations. The identified shuffle stages are necessary as follows:\")\n",
        "print(\"\\n1. **Hash Partitioning (e.g., `Exchange hashpartitioning(customer_id, 200)`):**\")\n",
        "print(\"   - **Purpose**: Used for operations that require grouping data by one or more keys, such as aggregations (sum, count) and window functions partitioned by specific columns. By hashing the key(s), Spark ensures that all data points belonging to a particular key are co-located on the same partition. This is fundamental for correctly computing results for each group.\")\n",
        "print(\"   - **Examples in our pipeline**:\")\n",
        "print(\"     - Aggregation for `customer_metrics` (`hashpartitioning(customer_id)`): Ensures all orders for a given customer are on the same partition to accurately sum their spending and count their orders.\")\n",
        "print(\"     - Deduplication within `clean_orders_df` (`hashpartitioning(order_id)`): Groups identical `order_id`s together to facilitate efficient removal of duplicates.\")\n",
        "print(\"     - City-wise window ranking (`hashpartitioning(city_clean)` and `hashpartitioning(customer_id, city_clean)`): Ensures that all data for a specific city, or customer within a city, is processed together, allowing for correct city-specific aggregations and rankings.\")\n",
        "\n",
        "print(\"\\n2. **Single Partition (e.g., `Exchange SinglePartition`):**\")\n",
        "print(\"   - **Purpose**: This strategy is employed when an operation requires a global ordering or processing of the entire dataset as a single unit, such as an overall ranking (`dense_rank()` across all customers). All data is collected into one partition to establish a single, coherent order.\")\n",
        "print(\"   - **Example in our pipeline**:\")\n",
        "print(\"     - Overall Window Ranking (`customer_ranking_overall`): To rank customers by `total_spend` globally, all data must be sorted and processed together. While ensuring correctness, this can be a performance bottleneck for very large datasets as it limits parallelism to a single executor.\")\n",
        "\n",
        "print(\"\\nIn summary, shuffles are a trade-off between network I/O and disk I/O (for data movement) and computational correctness. While they can be expensive, they are often indispensable for complex data transformations and aggregations in distributed systems like Spark.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPbdCwznKyPi",
        "outputId": "04f5522f-cd21-42bd-d6d1-ce90a68c9bc2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justification of Repartitioning Strategy:\n",
            "Spark performs repartitioning (shuffles) for several key reasons, primarily to ensure data correctness and enable parallel processing for certain operations. The identified shuffle stages are necessary as follows:\n",
            "\n",
            "1. **Hash Partitioning (e.g., `Exchange hashpartitioning(customer_id, 200)`):**\n",
            "   - **Purpose**: Used for operations that require grouping data by one or more keys, such as aggregations (sum, count) and window functions partitioned by specific columns. By hashing the key(s), Spark ensures that all data points belonging to a particular key are co-located on the same partition. This is fundamental for correctly computing results for each group.\n",
            "   - **Examples in our pipeline**:\n",
            "     - Aggregation for `customer_metrics` (`hashpartitioning(customer_id)`): Ensures all orders for a given customer are on the same partition to accurately sum their spending and count their orders.\n",
            "     - Deduplication within `clean_orders_df` (`hashpartitioning(order_id)`): Groups identical `order_id`s together to facilitate efficient removal of duplicates.\n",
            "     - City-wise window ranking (`hashpartitioning(city_clean)` and `hashpartitioning(customer_id, city_clean)`): Ensures that all data for a specific city, or customer within a city, is processed together, allowing for correct city-specific aggregations and rankings.\n",
            "\n",
            "2. **Single Partition (e.g., `Exchange SinglePartition`):**\n",
            "   - **Purpose**: This strategy is employed when an operation requires a global ordering or processing of the entire dataset as a single unit, such as an overall ranking (`dense_rank()` across all customers). All data is collected into one partition to establish a single, coherent order.\n",
            "   - **Example in our pipeline**:\n",
            "     - Overall Window Ranking (`customer_ranking_overall`): To rank customers by `total_spend` globally, all data must be sorted and processed together. While ensuring correctness, this can be a performance bottleneck for very large datasets as it limits parallelism to a single executor.\n",
            "\n",
            "In summary, shuffles are a trade-off between network I/O and disk I/O (for data movement) and computational correctness. While they can be expensive, they are often indispensable for complex data transformations and aggregations in distributed systems like Spark.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 8 – Broadcast Join (Light Use)\n"
      ],
      "metadata": {
        "id": "6NLZpBekK3c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a small lookup:\n",
        "\n",
        "segment_code,segment_label\n",
        "1,VIP\n",
        "2,Premium\n",
        "3,Regular\n",
        "\n",
        "Map:\n",
        "\n",
        "VIP → 1\n",
        "Premium → 2\n",
        "Regular → 3\n",
        "\n"
      ],
      "metadata": {
        "id": "xrOizcw1K5x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Create this as a small DataFrame.\n"
      ],
      "metadata": {
        "id": "scU9ql_9K79G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segment_data = [\n",
        "    (1, \"VIP\"),\n",
        "    (2, \"Premium\"),\n",
        "    (3, \"Regular\")\n",
        "]\n",
        "\n",
        "segment_lookup_df = spark.createDataFrame(segment_data, [\"segment_code\", \"segment_label\"])\n",
        "segment_lookup_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-U-0R9aK9ck",
        "outputId": "599f83ce-99b5-465d-da86-60992668abed"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------+\n",
            "|segment_code|segment_label|\n",
            "+------------+-------------+\n",
            "|           1|          VIP|\n",
            "|           2|      Premium|\n",
            "|           3|      Regular|\n",
            "+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Join with customer segmentation output.\n"
      ],
      "metadata": {
        "id": "bB3kbrJgK9xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "customer_segments_with_lookup = customer_segments.join(\n",
        "    broadcast(segment_lookup_df),\n",
        "    customer_segments[\"customer_segment\"] == segment_lookup_df[\"segment_label\"],\n",
        "    \"inner\"\n",
        ")\n",
        "\n",
        "print(\"Customer segments joined with lookup:\")\n",
        "customer_segments_with_lookup.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tUnCpGpK_NZ",
        "outputId": "b7f26884-cc78-4bab-ed5e-dedc41a1be04"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer segments joined with lookup:\n",
            "+-----------+-----------+------------+----------------+------------+-------------+\n",
            "|customer_id|total_spend|total_orders|customer_segment|segment_code|segment_label|\n",
            "+-----------+-----------+------------+----------------+------------+-------------+\n",
            "|    C007013|     241427|           6|             VIP|           1|          VIP|\n",
            "|    C016502|     318813|           6|             VIP|           1|          VIP|\n",
            "|    C030046|     276423|           6|             VIP|           1|          VIP|\n",
            "|    C036809|     284063|           6|             VIP|           1|          VIP|\n",
            "|    C022166|     266454|           6|             VIP|           1|          VIP|\n",
            "+-----------+-----------+------------+----------------+------------+-------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Force broadcast join.\n"
      ],
      "metadata": {
        "id": "dC8kLFWoK_h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_segments_with_lookup_forced_broadcast = customer_segments.join(\n",
        "    broadcast(segment_lookup_df),\n",
        "    customer_segments[\"customer_segment\"] == segment_lookup_df[\"segment_label\"],\n",
        "    \"inner\"\n",
        ")\n",
        "print(\"Broadcast join applied successfully (if segment_lookup_df is small enough).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5om-B7oHLA05",
        "outputId": "e538405e-5f8e-4cfd-ce7b-0f80d437cc40"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Broadcast join applied successfully (if segment_lookup_df is small enough).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Verify BroadcastHashJoin in plan."
      ],
      "metadata": {
        "id": "85-LDwycLBHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Execution plan for Broadcast Join verification:\")\n",
        "customer_segments_with_lookup_forced_broadcast.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MESnRPvLLD_U",
        "outputId": "f21a1f1e-5c3b-465c-a9dd-cb1d5df726c2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution plan for Broadcast Join verification:\n",
            "== Parsed Logical Plan ==\n",
            "Join Inner, (customer_segment#716 = segment_label#3143)\n",
            ":- Project [customer_id#18, total_spend#699L, total_orders#700L, CASE WHEN ((total_spend#699L >= cast(200000 as bigint)) AND (total_orders#700L >= cast(5 as bigint))) THEN VIP WHEN (total_spend#699L >= cast(100000 as bigint)) THEN Premium ELSE Regular END AS customer_segment#716]\n",
            ":  +- Aggregate [customer_id#18], [customer_id#18, sum(amount_clean#72) AS total_spend#699L, count(order_id#17) AS total_orders#700L]\n",
            ":     +- Deduplicate [order_id#17]\n",
            ":        +- Filter (status#24 = Completed)\n",
            ":           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, coalesce(cast(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            ":              +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) ELSE cast(null as int) END AS amount_clean#72]\n",
            ":                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            ":                    +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, trim(product#21, None) AS product_clean#28]\n",
            ":                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, trim(category#20, None) AS category_clean#27]\n",
            ":                          +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26]\n",
            ":                             +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [segment_code#3142L, segment_label#3143], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "customer_id: string, total_spend: bigint, total_orders: bigint, customer_segment: string, segment_code: bigint, segment_label: string\n",
            "Join Inner, (customer_segment#716 = segment_label#3143)\n",
            ":- Project [customer_id#18, total_spend#699L, total_orders#700L, CASE WHEN ((total_spend#699L >= cast(200000 as bigint)) AND (total_orders#700L >= cast(5 as bigint))) THEN VIP WHEN (total_spend#699L >= cast(100000 as bigint)) THEN Premium ELSE Regular END AS customer_segment#716]\n",
            ":  +- Aggregate [customer_id#18], [customer_id#18, sum(amount_clean#72) AS total_spend#699L, count(order_id#17) AS total_orders#700L]\n",
            ":     +- Deduplicate [order_id#17]\n",
            ":        +- Filter (status#24 = Completed)\n",
            ":           +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, coalesce(cast(try_to_timestamp(order_date#23, Some(yyyy-MM-dd), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(dd/MM/yyyy), TimestampType, Some(Etc/UTC), false) as date), cast(try_to_timestamp(order_date#23, Some(yyyy/MM/dd), TimestampType, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            ":              +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) ELSE cast(null as int) END AS amount_clean#72]\n",
            ":                 +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            ":                    +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, trim(product#21, None) AS product_clean#28]\n",
            ":                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, trim(category#20, None) AS category_clean#27]\n",
            ":                          +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26]\n",
            ":                             +- Relation [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] csv\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [segment_code#3142L, segment_label#3143], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Join Inner, (customer_segment#716 = segment_label#3143), rightHint=(strategy=broadcast)\n",
            ":- Project [customer_id#18, total_spend#699L, total_orders#700L, CASE WHEN ((total_spend#699L >= 200000) AND (total_orders#700L >= 5)) THEN VIP WHEN (total_spend#699L >= 100000) THEN Premium ELSE Regular END AS customer_segment#716]\n",
            ":  +- InMemoryRelation [customer_id#18, total_spend#699L, total_orders#700L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            ":        +- AdaptiveSparkPlan isFinalPlan=true\n",
            "            +- == Final Plan ==\n",
            "               ResultQueryStage 2\n",
            "               +- *(2) HashAggregate(keys=[customer_id#18], functions=[sum(amount_clean#72), count(order_id#17)], output=[customer_id#18, total_spend#699L, total_orders#700L])\n",
            "                  +- ShuffleQueryStage 1\n",
            "                     +- Exchange hashpartitioning(customer_id#18, 200), ENSURE_REQUIREMENTS, [plan_id=6768]\n",
            "                        +- *(1) HashAggregate(keys=[customer_id#18], functions=[partial_sum(amount_clean#72), partial_count(order_id#17)], output=[customer_id#18, sum#2827L, count#754L])\n",
            "                           +- TableCacheQueryStage 0\n",
            "                              +- InMemoryTableScan [order_id#17, customer_id#18, amount_clean#72]\n",
            "                                    +- InMemoryRelation [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, order_date_clean#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                                          +- AdaptiveSparkPlan isFinalPlan=true\n",
            "                              +- == Final Plan ==\n",
            "                                 ResultQueryStage 1\n",
            "                                 +- SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                                    +- *(2) Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                       +- ShuffleQueryStage 0\n",
            "                                          +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6729]\n",
            "                                             +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                                +- *(1) Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                                   +- *(1) Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                                      +- *(1) Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                                         +- *(1) Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                                            +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "                              +- == Initial Plan ==\n",
            "                                 SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                                 +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                    +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6592]\n",
            "                                       +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                          +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                             +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                                +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                                   +- Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                                      +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "            +- == Initial Plan ==\n",
            "               HashAggregate(keys=[customer_id#18], functions=[sum(amount_clean#72), count(order_id#17)], output=[customer_id#18, total_spend#699L, total_orders#700L])\n",
            "               +- Exchange hashpartitioning(customer_id#18, 200), ENSURE_REQUIREMENTS, [plan_id=6606]\n",
            "                  +- HashAggregate(keys=[customer_id#18], functions=[partial_sum(amount_clean#72), partial_count(order_id#17)], output=[customer_id#18, sum#2827L, count#754L])\n",
            "                     +- InMemoryTableScan [order_id#17, customer_id#18, amount_clean#72]\n",
            "                           +- InMemoryRelation [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, order_date_clean#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                                 +- AdaptiveSparkPlan isFinalPlan=true\n",
            "                     +- == Final Plan ==\n",
            "                        ResultQueryStage 1\n",
            "                        +- SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                           +- *(2) Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                              +- ShuffleQueryStage 0\n",
            "                                 +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6729]\n",
            "                                    +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                       +- *(1) Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                          +- *(1) Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                             +- *(1) Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                                +- *(1) Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                                   +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "                     +- == Initial Plan ==\n",
            "                        SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                        +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                           +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6592]\n",
            "                              +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                 +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                    +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                          +- Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                             +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "+- Filter isnotnull(segment_label#3143)\n",
            "   +- LogicalRDD [segment_code#3142L, segment_label#3143], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- BroadcastHashJoin [customer_segment#716], [segment_label#3143], Inner, BuildRight, false\n",
            "   :- Project [customer_id#18, total_spend#699L, total_orders#700L, CASE WHEN ((total_spend#699L >= 200000) AND (total_orders#700L >= 5)) THEN VIP WHEN (total_spend#699L >= 100000) THEN Premium ELSE Regular END AS customer_segment#716]\n",
            "   :  +- InMemoryTableScan [customer_id#18, total_orders#700L, total_spend#699L]\n",
            "   :        +- InMemoryRelation [customer_id#18, total_spend#699L, total_orders#700L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "   :              +- AdaptiveSparkPlan isFinalPlan=true\n",
            "                     +- == Final Plan ==\n",
            "                        ResultQueryStage 2\n",
            "                        +- *(2) HashAggregate(keys=[customer_id#18], functions=[sum(amount_clean#72), count(order_id#17)], output=[customer_id#18, total_spend#699L, total_orders#700L])\n",
            "                           +- ShuffleQueryStage 1\n",
            "                              +- Exchange hashpartitioning(customer_id#18, 200), ENSURE_REQUIREMENTS, [plan_id=6768]\n",
            "                                 +- *(1) HashAggregate(keys=[customer_id#18], functions=[partial_sum(amount_clean#72), partial_count(order_id#17)], output=[customer_id#18, sum#2827L, count#754L])\n",
            "                                    +- TableCacheQueryStage 0\n",
            "                                       +- InMemoryTableScan [order_id#17, customer_id#18, amount_clean#72]\n",
            "                                             +- InMemoryRelation [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, order_date_clean#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                                                   +- AdaptiveSparkPlan isFinalPlan=true\n",
            "                              +- == Final Plan ==\n",
            "                                 ResultQueryStage 1\n",
            "                                 +- SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                                    +- *(2) Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                       +- ShuffleQueryStage 0\n",
            "                                          +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6729]\n",
            "                                             +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                                +- *(1) Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                                   +- *(1) Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                                      +- *(1) Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                                         +- *(1) Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                                            +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "                              +- == Initial Plan ==\n",
            "                                 SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                                 +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                    +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6592]\n",
            "                                       +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                          +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                             +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                                +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                                   +- Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                                      +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "                     +- == Initial Plan ==\n",
            "                        HashAggregate(keys=[customer_id#18], functions=[sum(amount_clean#72), count(order_id#17)], output=[customer_id#18, total_spend#699L, total_orders#700L])\n",
            "                        +- Exchange hashpartitioning(customer_id#18, 200), ENSURE_REQUIREMENTS, [plan_id=6606]\n",
            "                           +- HashAggregate(keys=[customer_id#18], functions=[partial_sum(amount_clean#72), partial_count(order_id#17)], output=[customer_id#18, sum#2827L, count#754L])\n",
            "                              +- InMemoryTableScan [order_id#17, customer_id#18, amount_clean#72]\n",
            "                                    +- InMemoryRelation [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, amount_clean#72, order_date_clean#163], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                                          +- AdaptiveSparkPlan isFinalPlan=true\n",
            "                     +- == Final Plan ==\n",
            "                        ResultQueryStage 1\n",
            "                        +- SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                           +- *(2) Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                              +- ShuffleQueryStage 0\n",
            "                                 +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6729]\n",
            "                                    +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                       +- *(1) Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                          +- *(1) Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                             +- *(1) Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                                +- *(1) Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                                   +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "                     +- == Initial Plan ==\n",
            "                        SortAggregate(key=[order_id#17], functions=[first(customer_id#18, false), first(city#19, false), first(category#20, false), first(product#21, false), first(amount#22, false), first(order_date#23, false), first(status#24, false), first(city_clean#26, false), first(category_clean#27, false), first(product_clean#28, false), first(amount_clean#72, false), first(order_date_clean#163, false)], output=[order_id#17, customer_id#2495, city#2497, category#2499, product#2501, amount#2503, order_date#2505, status#2507, city_clean#2509, category_clean#2511, product_clean#2513, amount_clean#2515, order_date_clean#2517])\n",
            "                        +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                           +- Exchange hashpartitioning(order_id#17, 200), ENSURE_REQUIREMENTS, [plan_id=6592]\n",
            "                              +- SortAggregate(key=[order_id#17], functions=[partial_first(customer_id#18, false), partial_first(city#19, false), partial_first(category#20, false), partial_first(product#21, false), partial_first(amount#22, false), partial_first(order_date#23, false), partial_first(status#24, false), partial_first(city_clean#26, false), partial_first(category_clean#27, false), partial_first(product_clean#28, false), partial_first(amount_clean#72, false), partial_first(order_date_clean#163, false)], output=[order_id#17, first#2542, valueSet#2543, first#2544, valueSet#2545, first#2546, valueSet#2547, first#2548, valueSet#2549, first#2550, valueSet#2551, first#2552, valueSet#2553, first#2554, valueSet#2555, first#2556, valueSet#2557, first#2558, valueSet#2559, first#2560, valueSet#2561, first#2562, valueSet#2563, first#2564, valueSet#2565])\n",
            "                                 +- Sort [order_id#17 ASC NULLS FIRST], false, 0\n",
            "                                    +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, city_clean#26, category_clean#27, product_clean#28, CASE WHEN RLIKE(amount_clean#71, ^[0-9]+$) THEN cast(amount_clean#71 as int) END AS amount_clean#72, coalesce(cast(gettimestamp(order_date#23, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date), cast(gettimestamp(order_date#23, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), false) as date)) AS order_date_clean#163]\n",
            "                                       +- Project [order_id#17, customer_id#18, city#19, category#20, product#21, amount#22, order_date#23, status#24, trim(city#19, None) AS city_clean#26, trim(category#20, None) AS category_clean#27, trim(product#21, None) AS product_clean#28, regexp_replace(amount#22, ,, , 1) AS amount_clean#71]\n",
            "                                          +- Filter (isnotnull(status#24) AND (status#24 = Completed))\n",
            "                                             +- FileScan csv [order_id#17,customer_id#18,city#19,category#20,product#21,amount#22,order_date#23,status#24] Batched: false, DataFilters: [isnotnull(status#24), (status#24 = Completed)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status), EqualTo(status,Completed)], ReadSchema: struct<order_id:string,customer_id:string,city:string,category:string,product:string,amount:strin...\n",
            "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=6829]\n",
            "      +- Filter isnotnull(segment_label#3143)\n",
            "         +- Scan ExistingRDD[segment_code#3142L,segment_label#3143]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 9 – Sorting & Set Operations\n"
      ],
      "metadata": {
        "id": "-XzFBhnkLIMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Sort customers by:\n",
        "\n",
        "Total spend descending\n",
        "Order count descending\n"
      ],
      "metadata": {
        "id": "OUZlK19ILKDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_metrics_sorted = customer_metrics.orderBy(col(\"total_spend\").desc(), col(\"total_orders\").desc())\n",
        "\n",
        "print(\"Customers sorted by total spend (descending) and order count (descending):\")\n",
        "customer_metrics_sorted.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kPkOt9ALPs5",
        "outputId": "bd9c1d3c-a181-4c91-d334-025d27174b74"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers sorted by total spend (descending) and order count (descending):\n",
            "+-----------+-----------+------------+\n",
            "|customer_id|total_spend|total_orders|\n",
            "+-----------+-----------+------------+\n",
            "|    C043076|     493949|           6|\n",
            "|    C034689|     486879|           6|\n",
            "|    C039985|     484057|           6|\n",
            "|    C026691|     477147|           6|\n",
            "|    C038979|     477138|           6|\n",
            "|    C020762|     474717|           6|\n",
            "|    C044654|     471304|           6|\n",
            "|    C014292|     468617|           6|\n",
            "|    C019565|     467523|           6|\n",
            "|    C045487|     467050|           6|\n",
            "+-----------+-----------+------------+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create two sets:\n",
        "Customers who bought Electronics\n",
        "Customers who bought Grocery\n"
      ],
      "metadata": {
        "id": "UIeNl0-uLQEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_electronics = clean_orders_df.filter(col(\"category_clean\") == \"electronics\").select(\"customer_id\").distinct()\n",
        "customers_grocery = clean_orders_df.filter(col(\"category_clean\") == \"grocery\").select(\"customer_id\").distinct()\n",
        "\n",
        "print(\"Customers who bought Electronics (first 5):\")\n",
        "customers_electronics.show(5)\n",
        "print(\"Customers who bought Grocery (first 5):\")\n",
        "customers_grocery.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xuf1-7vYLUCN",
        "outputId": "51c39444-1326-4b3e-fb28-57b1e770d5a4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers who bought Electronics (first 5):\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|    C006671|\n",
            "|    C010704|\n",
            "|    C003239|\n",
            "|    C015794|\n",
            "|    C024319|\n",
            "+-----------+\n",
            "only showing top 5 rows\n",
            "Customers who bought Grocery (first 5):\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|    C042451|\n",
            "|    C042423|\n",
            "|    C020869|\n",
            "|    C044407|\n",
            "|    C007669|\n",
            "+-----------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Find:\n",
        "Customers in both sets\n",
        "Customers in only one set"
      ],
      "metadata": {
        "id": "bl3F5SzkLUWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customers_both_sets = customers_electronics.intersect(customers_grocery)\n",
        "print(\"Customers who bought both Electronics and Grocery (first 5):\")\n",
        "customers_both_sets.show(5)\n",
        "print(f\"Total customers who bought both: {customers_both_sets.count()}\")\n",
        "\n",
        "customers_only_electronics = customers_electronics.exceptAll(customers_grocery)\n",
        "customers_only_grocery = customers_grocery.exceptAll(customers_electronics)\n",
        "\n",
        "customers_only_one_set = customers_only_electronics.union(customers_only_grocery)\n",
        "print(\"\\nCustomers who bought only one of Electronics or Grocery (first 5):\")\n",
        "customers_only_one_set.show(5)\n",
        "print(f\"Total customers who bought only one set: {customers_only_one_set.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq27O3EsLU6b",
        "outputId": "2b822662-e4d1-4bf4-989c-b67509a6c8bb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers who bought both Electronics and Grocery (first 5):\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "+-----------+\n",
            "\n",
            "Total customers who bought both: 0\n",
            "\n",
            "Customers who bought only one of Electronics or Grocery (first 5):\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|    C047101|\n",
            "|    C031251|\n",
            "|    C003053|\n",
            "|    C026830|\n",
            "|    C035805|\n",
            "+-----------+\n",
            "only showing top 5 rows\n",
            "Total customers who bought only one set: 4537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 10 – Storage Strategy\n"
      ],
      "metadata": {
        "id": "ZvgCMIeMLYDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write customer master dataset to:\n",
        "\n",
        "Parquet\n",
        "\n",
        "Partitioned by:\n",
        "\n",
        "customer_segment\n",
        "\n"
      ],
      "metadata": {
        "id": "fhMl6NRYLaBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_master_df = customer_segments.alias(\"customer_master\")\n",
        "\n",
        "customer_master_df.write.mode(\"overwrite\").partitionBy(\"customer_segment\").parquet(\"customer_master.parquet\")\n",
        "\n",
        "print(\"Customer master dataset written to Parquet, partitioned by customer_segment.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACJRQNFHLbvc",
        "outputId": "609b05a8-8fba-45b0-bbf0-f5832ab326df"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer master dataset written to Parquet, partitioned by customer_segment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write monthly analytics to:\n",
        "\n",
        "ORC\n",
        "\n"
      ],
      "metadata": {
        "id": "qrj_d1FCLeZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "monthly_revenue_city.write.mode(\"overwrite\").orc(\"monthly_revenue_city.orc\")\n",
        "print(\"Monthly revenue per city written to ORC.\")\n",
        "\n",
        "monthly_order_count_category.write.mode(\"overwrite\").orc(\"monthly_order_count_category.orc\")\n",
        "print(\"Monthly order count per category written to ORC.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiOOr3XILf7r",
        "outputId": "2d5b3264-df0d-4d1b-eba8-cac9be7e8bf2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monthly revenue per city written to ORC.\n",
            "Monthly order count per category written to ORC.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Read back and validate."
      ],
      "metadata": {
        "id": "fJCBXUKxLgQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_master_read = spark.read.parquet(\"customer_master.parquet\")\n",
        "print(\"Customer Master (read from Parquet):\")\n",
        "customer_master_read.show(5)\n",
        "\n",
        "monthly_revenue_city_read = spark.read.orc(\"monthly_revenue_city.orc\")\n",
        "print(\"Monthly Revenue per City (read from ORC):\")\n",
        "monthly_revenue_city_read.show(5)\n",
        "\n",
        "monthly_order_count_category_read = spark.read.orc(\"monthly_order_count_category.orc\")\n",
        "print(\"Monthly Order Count per Category (read from ORC):\")\n",
        "monthly_order_count_category_read.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oMPIBxsLg1K",
        "outputId": "2a432365-04b3-4766-d2de-076d7d9df446"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer Master (read from Parquet):\n",
            "+-----------+-----------+------------+----------------+\n",
            "|customer_id|total_spend|total_orders|customer_segment|\n",
            "+-----------+-----------+------------+----------------+\n",
            "|    C017846|     259374|           6|             VIP|\n",
            "|    C031570|     265242|           6|             VIP|\n",
            "|    C008283|     329252|           6|             VIP|\n",
            "|    C035348|     258062|           6|             VIP|\n",
            "|    C042917|     291846|           6|             VIP|\n",
            "+-----------+-----------+------------+----------------+\n",
            "only showing top 5 rows\n",
            "Monthly Revenue per City (read from ORC):\n",
            "+-------------------+----------+---------------+\n",
            "|        order_month|city_clean|monthly_revenue|\n",
            "+-------------------+----------+---------------+\n",
            "|2024-02-01 00:00:00| bangalore|      792163305|\n",
            "|2024-01-01 00:00:00| bangalore|      822339117|\n",
            "|2024-01-01 00:00:00| hyderabad|      833063605|\n",
            "|2024-01-01 00:00:00|   kolkata|      824920456|\n",
            "|2024-01-01 00:00:00|    mumbai|      816636150|\n",
            "+-------------------+----------+---------------+\n",
            "only showing top 5 rows\n",
            "Monthly Order Count per Category (read from ORC):\n",
            "+-------------------+--------------+-------------------+\n",
            "|        order_month|category_clean|monthly_order_count|\n",
            "+-------------------+--------------+-------------------+\n",
            "|2024-01-01 00:00:00|       grocery|              36018|\n",
            "|2024-01-01 00:00:00|       fashion|              35571|\n",
            "|2024-01-01 00:00:00|   electronics|              35994|\n",
            "|2024-02-01 00:00:00|   electronics|              34766|\n",
            "|2024-02-01 00:00:00|       grocery|              34672|\n",
            "+-------------------+--------------+-------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 11 – Debugging\n"
      ],
      "metadata": {
        "id": "XKaggkvILnfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain why this is dangerous:\n",
        "\n",
        "df = df.groupBy(\"customer_id\").sum(\"amount\").show()\n",
        "\n",
        "Explain:\n",
        "What df becomes\n",
        "Why pipeline breaks\n",
        "Correct approach"
      ],
      "metadata": {
        "id": "eHuw_un3LskB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Explain why this is dangerous:\n",
        "# df = df.groupBy(\"customer_id\").sum(\"amount\").show()\n",
        "\n",
        "# What df becomes:\n",
        "# The `.show()` method is an action in Spark DataFrames. It triggers the computation and prints the results to the console. However, it returns `None`. Therefore, `df` would become `None`.\n",
        "\n",
        "# Why pipeline breaks:\n",
        "# If `df` becomes `None`, any subsequent operations or transformations attempted on `df` will raise an `AttributeError` (e.g., 'NoneType' object has no attribute 'withColumn') because `df` is no longer a DataFrame object.\n",
        "\n",
        "# Correct approach:\n",
        "# The action (`.show()`) should not be chained with an assignment back to the DataFrame variable if you intend to continue using the DataFrame. Instead, separate the action:\n",
        "#\n",
        "# # Option 1: Assign the result of transformations to a new DataFrame variable\n",
        "# aggregated_df = df.groupBy(\"customer_id\").sum(\"amount\")\n",
        "# aggregated_df.show()\n",
        "#\n",
        "# # Option 2: Perform the action as a separate step if you intend to reuse the original DataFrame reference (not recommended if the aggregation is the desired next step in the pipeline)\n",
        "# df_aggregated = df.groupBy(\"customer_id\").sum(\"amount\")\n",
        "# df_aggregated.show()"
      ],
      "metadata": {
        "id": "9EYDbkzGLvGH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}