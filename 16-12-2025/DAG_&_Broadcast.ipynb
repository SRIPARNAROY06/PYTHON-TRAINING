{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Ud0Z24jWfGT"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder\\\n",
        ".appName(\"DAG and Broadcast demo\")\\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders_data = [\n",
        "    (\"O001\",\"Hyderabad\",1200),\n",
        "    (\"O002\",\"Delhi\",800),\n",
        "    (\"O003\",\"Mumbai\",1500),\n",
        "    (\"O004\",\"Bangalore\",400),\n",
        "    (\"O005\",\"Hyderabad\",300),\n",
        "    (\"O006\",\"Delhi\",2000),\n",
        "    (\"O007\",\"Mumbai\",700),\n",
        "    (\"O008\",\"Bangalore\",1800),\n",
        "    (\"O009\",\"Delhi\",350),\n",
        "    (\"O010\",\"Hyderabad\",900)\n",
        "]\n",
        "\n",
        "orders_cols = [\"order_id\",\"city\",\"order_amount\"]\n",
        "\n",
        "orders_df = spark.createDataFrame(orders_data, orders_cols)\n",
        "orders_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO4gxP1qZg_k",
        "outputId": "26759c69-cf09-4962-bff0-868b172c7ebd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+------------+\n",
            "|order_id|     city|order_amount|\n",
            "+--------+---------+------------+\n",
            "|    O001|Hyderabad|        1200|\n",
            "|    O002|    Delhi|         800|\n",
            "|    O003|   Mumbai|        1500|\n",
            "|    O004|Bangalore|         400|\n",
            "|    O005|Hyderabad|         300|\n",
            "|    O006|    Delhi|        2000|\n",
            "|    O007|   Mumbai|         700|\n",
            "|    O008|Bangalore|        1800|\n",
            "|    O009|    Delhi|         350|\n",
            "|    O010|Hyderabad|         900|\n",
            "+--------+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "city_data = [\n",
        "    (\"Hyderabad\",\"Tier-1\"),\n",
        "    (\"Delhi\",\"Tier-1\"),\n",
        "    (\"Mumbai\",\"Tier-1\"),\n",
        "    (\"Bangalore\",\"Tier-1\")\n",
        "]\n",
        "\n",
        "city_cols = [\"city\",\"city_category\"]"
      ],
      "metadata": {
        "id": "oQIhpaSVZonl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_df = spark.createDataFrame(city_data, city_cols)\n",
        "city_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgqaItKNZvN-",
        "outputId": "fc7355d5-a55c-4375-91dd-9e672654acda"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "|     city|city_category|\n",
            "+---------+-------------+\n",
            "|Hyderabad|       Tier-1|\n",
            "|    Delhi|       Tier-1|\n",
            "|   Mumbai|       Tier-1|\n",
            "|Bangalore|       Tier-1|\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "filtered_orders = orders_df.filter(col(\"order_amount\") > 500)\n",
        "\n",
        "joined_df = filtered_orders.join(\n",
        "    city_df,\n",
        "    on=\"city\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "final_df = joined_df.select(\n",
        "    \"order_id\",\n",
        "    \"city\",\n",
        "    \"city_category\",\n",
        "    \"order_amount\"\n",
        ")"
      ],
      "metadata": {
        "id": "EdIbBHqIZwjz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_uZ6wPmZ4ak",
        "outputId": "ea3412d0-e927-4208-afdb-e1ef5b380f2d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-------------+------------+\n",
            "|order_id|     city|city_category|order_amount|\n",
            "+--------+---------+-------------+------------+\n",
            "|    O008|Bangalore|       Tier-1|        1800|\n",
            "|    O002|    Delhi|       Tier-1|         800|\n",
            "|    O006|    Delhi|       Tier-1|        2000|\n",
            "|    O001|Hyderabad|       Tier-1|        1200|\n",
            "|    O010|Hyderabad|       Tier-1|         900|\n",
            "|    O003|   Mumbai|       Tier-1|        1500|\n",
            "|    O007|   Mumbai|       Tier-1|         700|\n",
            "+--------+---------+-------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjHUV53taNe9",
        "outputId": "234228c1-0861-4b44-e1f6-01472645b0c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=true\n",
            "+- == Final Plan ==\n",
            "   ResultQueryStage 2\n",
            "   +- *(5) Project [order_id#0, city#1, city_category#14, order_amount#2L]\n",
            "      +- *(5) SortMergeJoin [city#1], [city#13], Inner\n",
            "         :- *(3) Sort [city#1 ASC NULLS FIRST], false, 0\n",
            "         :  +- AQEShuffleRead coalesced\n",
            "         :     +- ShuffleQueryStage 0\n",
            "         :        +- Exchange hashpartitioning(city#1, 200), ENSURE_REQUIREMENTS, [plan_id=62]\n",
            "         :           +- *(1) Filter ((isnotnull(order_amount#2L) AND (order_amount#2L > 500)) AND isnotnull(city#1))\n",
            "         :              +- *(1) Scan ExistingRDD[order_id#0,city#1,order_amount#2L]\n",
            "         +- *(4) Sort [city#13 ASC NULLS FIRST], false, 0\n",
            "            +- AQEShuffleRead coalesced\n",
            "               +- ShuffleQueryStage 1\n",
            "                  +- Exchange hashpartitioning(city#13, 200), ENSURE_REQUIREMENTS, [plan_id=75]\n",
            "                     +- *(2) Filter isnotnull(city#13)\n",
            "                        +- *(2) Scan ExistingRDD[city#13,city_category#14]\n",
            "+- == Initial Plan ==\n",
            "   Project [order_id#0, city#1, city_category#14, order_amount#2L]\n",
            "   +- SortMergeJoin [city#1], [city#13], Inner\n",
            "      :- Sort [city#1 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#1, 200), ENSURE_REQUIREMENTS, [plan_id=49]\n",
            "      :     +- Filter ((isnotnull(order_amount#2L) AND (order_amount#2L > 500)) AND isnotnull(city#1))\n",
            "      :        +- Scan ExistingRDD[order_id#0,city#1,order_amount#2L]\n",
            "      +- Sort [city#13 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#13, 200), ENSURE_REQUIREMENTS, [plan_id=50]\n",
            "            +- Filter isnotnull(city#13)\n",
            "               +- Scan ExistingRDD[city#13,city_category#14]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "broadcast_join_df=filtered_orders.join(\n",
        "    broadcast(city_df),\n",
        "    on=\"city\",\n",
        "    how=\"inner\"\n",
        "\n",
        ")\n",
        "\n",
        "final_broadcast_df=broadcast_join_df.select(\n",
        "    \"order_id\",\n",
        "    \"city\",\n",
        "    \"city_category\",\n",
        "    \"order_amount\"\n",
        ")"
      ],
      "metadata": {
        "id": "hGAeuZQhaQxF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_broadcast_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNLsLC6JbFD3",
        "outputId": "39943364-e7a1-4786-dcec-be384c5d76cb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-------------+------------+\n",
            "|order_id|     city|city_category|order_amount|\n",
            "+--------+---------+-------------+------------+\n",
            "|    O001|Hyderabad|       Tier-1|        1200|\n",
            "|    O002|    Delhi|       Tier-1|         800|\n",
            "|    O003|   Mumbai|       Tier-1|        1500|\n",
            "|    O006|    Delhi|       Tier-1|        2000|\n",
            "|    O007|   Mumbai|       Tier-1|         700|\n",
            "|    O008|Bangalore|       Tier-1|        1800|\n",
            "|    O010|Hyderabad|       Tier-1|         900|\n",
            "+--------+---------+-------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_broadcast_df.explain(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-IwR5fzbHDZ",
        "outputId": "67f8acf5-212c-4824-ab50-816e27f702a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project ['order_id, 'city, 'city_category, 'order_amount]\n",
            "+- Project [city#1, order_id#0, order_amount#2L, city_category#14]\n",
            "   +- Join Inner, (city#1 = city#13)\n",
            "      :- Filter (order_amount#2L > cast(500 as bigint))\n",
            "      :  +- LogicalRDD [order_id#0, city#1, order_amount#2L], false\n",
            "      +- ResolvedHint (strategy=broadcast)\n",
            "         +- LogicalRDD [city#13, city_category#14], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "order_id: string, city: string, city_category: string, order_amount: bigint\n",
            "Project [order_id#0, city#1, city_category#14, order_amount#2L]\n",
            "+- Project [city#1, order_id#0, order_amount#2L, city_category#14]\n",
            "   +- Join Inner, (city#1 = city#13)\n",
            "      :- Filter (order_amount#2L > cast(500 as bigint))\n",
            "      :  +- LogicalRDD [order_id#0, city#1, order_amount#2L], false\n",
            "      +- ResolvedHint (strategy=broadcast)\n",
            "         +- LogicalRDD [city#13, city_category#14], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [order_id#0, city#1, city_category#14, order_amount#2L]\n",
            "+- Join Inner, (city#1 = city#13), rightHint=(strategy=broadcast)\n",
            "   :- Filter ((isnotnull(order_amount#2L) AND (order_amount#2L > 500)) AND isnotnull(city#1))\n",
            "   :  +- LogicalRDD [order_id#0, city#1, order_amount#2L], false\n",
            "   +- Filter isnotnull(city#13)\n",
            "      +- LogicalRDD [city#13, city_category#14], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [order_id#0, city#1, city_category#14, order_amount#2L]\n",
            "   +- BroadcastHashJoin [city#1], [city#13], Inner, BuildRight, false\n",
            "      :- Filter ((isnotnull(order_amount#2L) AND (order_amount#2L > 500)) AND isnotnull(city#1))\n",
            "      :  +- Scan ExistingRDD[order_id#0,city#1,order_amount#2L]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=344]\n",
            "         +- Filter isnotnull(city#13)\n",
            "            +- Scan ExistingRDD[city#13,city_category#14]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Broadcast HashJoin\n"
      ],
      "metadata": {
        "id": "vgyyjiZOcd5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Broadcast Hash Join is an optimization technique used in Apache Spark to improve the performance of joins when one of the DataFrames being joined is significantly smaller than the other. Instead of shuffling both DataFrames across the network, Spark broadcasts the smaller DataFrame to all nodes in the cluster. This allows each node to perform the join locally without needing to shuffle the larger DataFrame, leading to faster execution times.\n",
        "\n",
        "In the final_broadcast_df.explain(True) output, you can see BroadcastHashJoin in the Physical Plan. Specifically, the line +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=344] indicates that the city_df (the smaller DataFrame in this case) was broadcasted to all executors, which then allows the join to be performed more efficiently."
      ],
      "metadata": {
        "id": "mhbeib2ad7_-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IOsRmoTYbiy-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}