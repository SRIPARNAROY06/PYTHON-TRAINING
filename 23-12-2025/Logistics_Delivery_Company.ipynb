{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rNQBzkuaYyqH"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "spark = SparkSession.builder \\\n",
        ".appName('Logistics Delivery Company') \\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "C_KrHCWrZAp-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "A56q98ZuZfNJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 1 — SCHEMA DESIGN & INGESTION"
      ],
      "metadata": {
        "id": "epSHarkDazub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define explicit schemas for all datasets"
      ],
      "metadata": {
        "id": "umAKVNBma2p2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load raw delivery data using schema enforcement"
      ],
      "metadata": {
        "id": "DaqlwHlSa7eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_data = [\n",
        "(\"DLV001\",\"Delhi \",\"D001\",\"Delivered\",\"120\",\"2024-01-05 10:30\"),\n",
        "(\"DLV002\",\"Mumbai\",\"D002\",\"Delivered\",\"90\",\"05/01/2024 11:00\"),\n",
        "(\"DLV003\",\"Bangalore\",\"D003\",\"In Transit\",\"200\",\"2024/01/06 09:45\"),\n",
        "(\"DLV004\",\"Delhi\",\"D004\",\"Cancelled\",\"\",\"2024-01-07 14:00\"),\n",
        "(\"DLV005\",\"Chennai\",\"D002\",\"Delivered\",\"invalid\",\"2024-01-08 16:20\"),\n",
        "(\"DLV006\",\"Mumbai\",\"D005\",\"Delivered\",None,\"2024-01-08 18:10\"),\n",
        "(\"DLV007\",\"Delhi\",\"D001\",\"Delivered\",\"140\",\"09-01-2024 12:30\"),\n",
        "(\"DLV008\",\"Bangalore\",\"D003\",\"Delivered\",\"160\",\"2024-01-09 15:45\"),\n",
        "(\"DLV009\",\"Mumbai\",\"D004\",\"Delivered\",\"110\",\"2024-01-10 13:20\"),\n",
        "(\"DLV009\",\"Mumbai\",\"D004\",\"Delivered\",\"110\",\"2024-01-10 13:20\")\n",
        "]\n",
        "\n",
        "column = [\"delivery_id\", \"city\", \"driver_id\", \"status\", \"delivery_time_minutes\", \"delivery_timestamp\"]\n"
      ],
      "metadata": {
        "id": "oa2D_OOkZBnT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_schema = StructType([\n",
        "    StructField(\"delivery_id\", StringType(), True),\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"driver_id\", StringType(), True),\n",
        "    StructField(\"status\", StringType(), True),\n",
        "    StructField(\"delivery_time_minutes\", IntegerType(), True),\n",
        "    StructField(\"delivery_timestamp\", StringType(), True)\n",
        "])\n",
        "delivery_df = spark.createDataFrame(data=delivery_data, schema=column)"
      ],
      "metadata": {
        "id": "qvWA7DmqZPaL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver_data = [\n",
        "(\"D001\",\"Ravi\",\"Senior\"),\n",
        "(\"D002\",\"Amit\",\"Junior\"),\n",
        "(\"D003\",\"Sneha\",\"Senior\"),\n",
        "(\"D004\",\"Karan\",\"Junior\"),\n",
        "(\"D005\",\"Neha\",\"Senior\")\n",
        "]"
      ],
      "metadata": {
        "id": "tZ24zVVmaV9c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver_schema = StructType([\n",
        "    StructField(\"driver_id\", StringType(), True),\n",
        "    StructField(\"driver_name\", StringType(), True),\n",
        "    StructField(\"driver_type\", StringType(), True)\n",
        "])\n",
        "driver_df = spark.createDataFrame(data=driver_data, schema=driver_schema)"
      ],
      "metadata": {
        "id": "PAN24G-QadEr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_zone_data = [\n",
        "(\"Delhi\",\"North\"),\n",
        "(\"Mumbai\",\"West\"),\n",
        "(\"Bangalore\",\"South\"),\n",
        "(\"Chennai\",\"South\")\n",
        "]"
      ],
      "metadata": {
        "id": "hSNJm7nMamyH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_zone_schema = StructType([\n",
        "    StructField(\"city\", StringType(), True),\n",
        "    StructField(\"zone\", StringType(), True)\n",
        "])\n",
        "city_zone_df = spark.createDataFrame(data=city_zone_data, schema=city_zone_schema)"
      ],
      "metadata": {
        "id": "7x8LMPKBaqQj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Identify and flag corrupt records"
      ],
      "metadata": {
        "id": "lOn6mXY3bGCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df = delivery_df.withColumn(\n",
        "    \"is_corrupt\",\n",
        "    F.when(\n",
        "    (F.col(\"delivery_time_minutes\").isNull()) | (F.col(\"delivery_time_minutes\").rlike(\"^[0-9]+$\")),\n",
        "    True).otherwise(False))"
      ],
      "metadata": {
        "id": "w1Y3p0P1avnX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Validate schema correctness"
      ],
      "metadata": {
        "id": "0nJpt8cXb8s8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDmDNnDCb8CJ",
        "outputId": "b6928985-2206-4c46-af40-e6c6f17d80cc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- delivery_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- driver_id: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- delivery_time_minutes: string (nullable = true)\n",
            " |-- delivery_timestamp: string (nullable = true)\n",
            " |-- is_corrupt: boolean (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 2 — DATA CLEANING & STANDARDIZATION"
      ],
      "metadata": {
        "id": "7jtUiNOucEnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Trim all string columns\n"
      ],
      "metadata": {
        "id": "xXfRdrefcGiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for c in delivery_df.columns:\n",
        "  delivery_df = delivery_df.withColumn(c, F.trim(col(c)))"
      ],
      "metadata": {
        "id": "RT1YlOv_cAiK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Standardize status values\n"
      ],
      "metadata": {
        "id": "GA_jAATfcKUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df = delivery_df.withColumn(\n",
        "    \"status\",\n",
        "    F.lower(F.col(\"status\")\n",
        "))"
      ],
      "metadata": {
        "id": "TNi_3HGrcMHo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Convert delivery_time_minutes to IntegerType\n"
      ],
      "metadata": {
        "id": "uGwjeLeAcMdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df = delivery_df.withColumn(\n",
        "    \"delivery_time_minutes\",\n",
        "    F.when(\n",
        "        F.col(\"delivery_time_minutes\").rlike(\"^[0-9]+$\"),\n",
        "        F.col(\"delivery_time_minutes\").cast(IntegerType())\n",
        "    ).otherwise(F.lit(None).cast(IntegerType()))\n",
        ")"
      ],
      "metadata": {
        "id": "0rOpWUY8cOHh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Handle invalid and null delivery times\n"
      ],
      "metadata": {
        "id": "IviFgofvcP59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df = delivery_df.withColumn(\n",
        "    \"delivery_time_minutes\",\n",
        "    F.when(\n",
        "        F.col(\"delivery_time_minutes\").isNull() | (F.col(\"delivery_time_minutes\") < 0),\n",
        "        0\n",
        "    ).otherwise(F.col(\"delivery_time_minutes\"))\n",
        ")"
      ],
      "metadata": {
        "id": "0kOAEalXcSpy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Parse multiple timestamp formats into TimestampType\n"
      ],
      "metadata": {
        "id": "X10MBJyOcS_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df = delivery_df.withColumn(\n",
        "    \"delivery_timestamp\",\n",
        "    F.coalesce(\n",
        "        F.try_to_timestamp(F.col(\"delivery_timestamp\"), F.lit(\"yyyy-MM-dd HH:mm\")),\n",
        "        F.try_to_timestamp(F.col(\"delivery_timestamp\"), F.lit(\"dd/MM/yyyy HH:mm\")),\n",
        "        F.try_to_timestamp(F.col(\"delivery_timestamp\"), F.lit(\"yyyy/MM/dd HH:mm\")),\n",
        "        F.try_to_timestamp(F.col(\"delivery_timestamp\"), F.lit(\"dd-MM-yyyy HH:mm\"))\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "edOVZODzcUuZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Remove duplicate delivery IDs"
      ],
      "metadata": {
        "id": "qjnbDtMxcVIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df = delivery_df.drop_duplicates(subset=['delivery_id'])"
      ],
      "metadata": {
        "id": "489kR_dvcV0N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 3 — BUSINESS FILTERING"
      ],
      "metadata": {
        "id": "X-blFMqCdP-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Keep only Delivered deliveries\n"
      ],
      "metadata": {
        "id": "b0fLY9CKdTnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df = delivery_df.filter(F.col(\"status\") == \"delivered\")"
      ],
      "metadata": {
        "id": "pU-Wry-RdW1N"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Remove cancelled and in-transit deliveries\n"
      ],
      "metadata": {
        "id": "Gddef97tdXKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This step is implicitly handled by step 11, which filtered to keep only 'delivered' statuses."
      ],
      "metadata": {
        "id": "gzLe6CGTdYxq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Validate record counts before and after filtering"
      ],
      "metadata": {
        "id": "DfkxPc4jdZHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_count = spark.createDataFrame(data=delivery_data, schema=column).count()\n",
        "filtered_count = delivery_df.count()\n",
        "print(f\"Original record count: {original_count}\")\n",
        "print(f\"Filtered record count (delivered status only): {filtered_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XmT4lQ1dZ_w",
        "outputId": "27a9dfaf-d241-4519-965b-9090ccdc9cca"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original record count: 10\n",
            "Filtered record count (delivered status only): 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 4 — DATA ENRICHMENT & JOINS"
      ],
      "metadata": {
        "id": "iqbf-43hdnYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Join delivery data with driver master\n"
      ],
      "metadata": {
        "id": "kC6Esj_9dpQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df = delivery_df.join(driver_df, on='driver_id', how='inner')"
      ],
      "metadata": {
        "id": "RifJaJnldvGa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Join enriched data with city zone lookup\n"
      ],
      "metadata": {
        "id": "syWDBNv3dvbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df = delivery_df.join(city_zone_df, on='city', how='inner')"
      ],
      "metadata": {
        "id": "BdZm0TRtdy71"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Use broadcast join where appropriate\n"
      ],
      "metadata": {
        "id": "WUBD3fD2d0AM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "delivery_df = delivery_df.join(broadcast(city_zone_df), on='city', how='inner')"
      ],
      "metadata": {
        "id": "ChsKWtudd3AG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Explain join strategy using explain(True)"
      ],
      "metadata": {
        "id": "0IRnVDfcd3yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GhnSaq2d6Bu",
        "outputId": "e0dfdd49-aac1-48b2-fb50-99bca1a10517"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Project [city#13, driver_id#14, delivery_id#12, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8, zone#10]\n",
            ":  +- Join Inner, (city#13 = city#9)\n",
            ":     :- Project [driver_id#14, delivery_id#12, city#13, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8]\n",
            ":     :  +- Join Inner, (driver_id#14 = driver_id#6)\n",
            ":     :     :- Filter (status#19 = delivered)\n",
            ":     :     :  +- Deduplicate [delivery_id#12]\n",
            ":     :     :     +- Project [delivery_id#12, city#13, driver_id#14, status#19, delivery_time_minutes#21, coalesce(try_to_timestamp(delivery_timestamp#17, Some(yyyy-MM-dd HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(dd/MM/yyyy HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(yyyy/MM/dd HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(dd-MM-yyyy HH:mm), TimestampType, Some(Etc/UTC), false)) AS delivery_timestamp#22, is_corrupt#18]\n",
            ":     :     :        +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN (isnull(delivery_time_minutes#20) OR (delivery_time_minutes#20 < 0)) THEN 0 ELSE delivery_time_minutes#20 END AS delivery_time_minutes#21, delivery_timestamp#17, is_corrupt#18]\n",
            ":     :     :           +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN RLIKE(delivery_time_minutes#16, ^[0-9]+$) THEN cast(delivery_time_minutes#16 as int) ELSE cast(null as int) END AS delivery_time_minutes#20, delivery_timestamp#17, is_corrupt#18]\n",
            ":     :     :              +- Project [delivery_id#12, city#13, driver_id#14, lower(status#15) AS status#19, delivery_time_minutes#16, delivery_timestamp#17, is_corrupt#18]\n",
            ":     :     :                 +- Project [delivery_id#12, city#13, driver_id#14, status#15, delivery_time_minutes#16, delivery_timestamp#17, trim(cast(is_corrupt#11 as string), None) AS is_corrupt#18]\n",
            ":     :     :                    +- Project [delivery_id#12, city#13, driver_id#14, status#15, delivery_time_minutes#16, trim(delivery_timestamp#5, None) AS delivery_timestamp#17, is_corrupt#11]\n",
            ":     :     :                       +- Project [delivery_id#12, city#13, driver_id#14, status#15, trim(delivery_time_minutes#4, None) AS delivery_time_minutes#16, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                          +- Project [delivery_id#12, city#13, driver_id#14, trim(status#3, None) AS status#15, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                             +- Project [delivery_id#12, city#13, trim(driver_id#2, None) AS driver_id#14, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                                +- Project [delivery_id#12, trim(city#1, None) AS city#13, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                                   +- Project [trim(delivery_id#0, None) AS delivery_id#12, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                                      +- Project [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, CASE WHEN (isnull(delivery_time_minutes#4) OR RLIKE(delivery_time_minutes#4, ^[0-9]+$)) THEN true ELSE false END AS is_corrupt#11]\n",
            ":     :     :                                         +- LogicalRDD [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5], false\n",
            ":     :     +- LogicalRDD [driver_id#6, driver_name#7, driver_type#8], false\n",
            ":     +- LogicalRDD [city#9, zone#10], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#74, zone#75], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, driver_id: string, delivery_id: string, status: string, delivery_time_minutes: int, delivery_timestamp: timestamp, is_corrupt: string, driver_name: string, driver_type: string, zone: string, zone: string\n",
            "Project [city#13, driver_id#14, delivery_id#12, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8, zone#10, zone#75]\n",
            "+- Join Inner, (city#13 = city#74)\n",
            "   :- Project [city#13, driver_id#14, delivery_id#12, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8, zone#10]\n",
            "   :  +- Join Inner, (city#13 = city#9)\n",
            "   :     :- Project [driver_id#14, delivery_id#12, city#13, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8]\n",
            "   :     :  +- Join Inner, (driver_id#14 = driver_id#6)\n",
            "   :     :     :- Filter (status#19 = delivered)\n",
            "   :     :     :  +- Deduplicate [delivery_id#12]\n",
            "   :     :     :     +- Project [delivery_id#12, city#13, driver_id#14, status#19, delivery_time_minutes#21, coalesce(try_to_timestamp(delivery_timestamp#17, Some(yyyy-MM-dd HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(dd/MM/yyyy HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(yyyy/MM/dd HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(dd-MM-yyyy HH:mm), TimestampType, Some(Etc/UTC), false)) AS delivery_timestamp#22, is_corrupt#18]\n",
            "   :     :     :        +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN (isnull(delivery_time_minutes#20) OR (delivery_time_minutes#20 < 0)) THEN 0 ELSE delivery_time_minutes#20 END AS delivery_time_minutes#21, delivery_timestamp#17, is_corrupt#18]\n",
            "   :     :     :           +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN RLIKE(delivery_time_minutes#16, ^[0-9]+$) THEN cast(delivery_time_minutes#16 as int) ELSE cast(null as int) END AS delivery_time_minutes#20, delivery_timestamp#17, is_corrupt#18]\n",
            "   :     :     :              +- Project [delivery_id#12, city#13, driver_id#14, lower(status#15) AS status#19, delivery_time_minutes#16, delivery_timestamp#17, is_corrupt#18]\n",
            "   :     :     :                 +- Project [delivery_id#12, city#13, driver_id#14, status#15, delivery_time_minutes#16, delivery_timestamp#17, trim(cast(is_corrupt#11 as string), None) AS is_corrupt#18]\n",
            "   :     :     :                    +- Project [delivery_id#12, city#13, driver_id#14, status#15, delivery_time_minutes#16, trim(delivery_timestamp#5, None) AS delivery_timestamp#17, is_corrupt#11]\n",
            "   :     :     :                       +- Project [delivery_id#12, city#13, driver_id#14, status#15, trim(delivery_time_minutes#4, None) AS delivery_time_minutes#16, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                          +- Project [delivery_id#12, city#13, driver_id#14, trim(status#3, None) AS status#15, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                             +- Project [delivery_id#12, city#13, trim(driver_id#2, None) AS driver_id#14, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                                +- Project [delivery_id#12, trim(city#1, None) AS city#13, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                                   +- Project [trim(delivery_id#0, None) AS delivery_id#12, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                                      +- Project [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, CASE WHEN (isnull(delivery_time_minutes#4) OR RLIKE(delivery_time_minutes#4, ^[0-9]+$)) THEN true ELSE false END AS is_corrupt#11]\n",
            "   :     :     :                                         +- LogicalRDD [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5], false\n",
            "   :     :     +- LogicalRDD [driver_id#6, driver_name#7, driver_type#8], false\n",
            "   :     +- LogicalRDD [city#9, zone#10], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#74, zone#75], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#77, driver_id#79, delivery_id#12, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8, zone#10, zone#75]\n",
            "+- Join Inner, (city#77 = city#74), rightHint=(strategy=broadcast)\n",
            "   :- Project [city#77, driver_id#79, delivery_id#12, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8, zone#10]\n",
            "   :  +- Join Inner, (city#77 = city#9)\n",
            "   :     :- Project [driver_id#79, delivery_id#12, city#77, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8]\n",
            "   :     :  +- Join Inner, (driver_id#79 = driver_id#6)\n",
            "   :     :     :- Filter ((isnotnull(status#81) AND (status#81 = delivered)) AND (isnotnull(driver_id#79) AND isnotnull(city#77)))\n",
            "   :     :     :  +- Aggregate [delivery_id#12], [delivery_id#12, first(city#13, false) AS city#77, first(driver_id#14, false) AS driver_id#79, first(status#19, false) AS status#81, first(delivery_time_minutes#21, false) AS delivery_time_minutes#83, first(delivery_timestamp#22, false) AS delivery_timestamp#85, first(is_corrupt#18, false) AS is_corrupt#87]\n",
            "   :     :     :     +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN (isnull(delivery_time_minutes#20) OR (delivery_time_minutes#20 < 0)) THEN 0 ELSE delivery_time_minutes#20 END AS delivery_time_minutes#21, coalesce(gettimestamp(delivery_timestamp#17, yyyy-MM-dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, dd/MM/yyyy HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, yyyy/MM/dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, dd-MM-yyyy HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS delivery_timestamp#22, is_corrupt#18]\n",
            "   :     :     :        +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN RLIKE(delivery_time_minutes#16, ^[0-9]+$) THEN cast(delivery_time_minutes#16 as int) END AS delivery_time_minutes#20, delivery_timestamp#17, is_corrupt#18]\n",
            "   :     :     :           +- Project [trim(delivery_id#0, None) AS delivery_id#12, trim(city#1, None) AS city#13, trim(driver_id#2, None) AS driver_id#14, lower(trim(status#3, None)) AS status#19, trim(delivery_time_minutes#4, None) AS delivery_time_minutes#16, trim(delivery_timestamp#5, None) AS delivery_timestamp#17, trim(cast(((isnull(delivery_time_minutes#4) OR RLIKE(delivery_time_minutes#4, ^[0-9]+$)) <=> true) as string), None) AS is_corrupt#18]\n",
            "   :     :     :              +- LogicalRDD [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5], false\n",
            "   :     :     +- Filter isnotnull(driver_id#6)\n",
            "   :     :        +- LogicalRDD [driver_id#6, driver_name#7, driver_type#8], false\n",
            "   :     +- Filter isnotnull(city#9)\n",
            "   :        +- LogicalRDD [city#9, zone#10], false\n",
            "   +- Filter isnotnull(city#74)\n",
            "      +- LogicalRDD [city#74, zone#75], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#77, driver_id#79, delivery_id#12, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8, zone#10, zone#75]\n",
            "   +- BroadcastHashJoin [city#77], [city#74], Inner, BuildRight, false\n",
            "      :- Project [city#77, driver_id#79, delivery_id#12, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8, zone#10]\n",
            "      :  +- SortMergeJoin [city#77], [city#9], Inner\n",
            "      :     :- Sort [city#77 ASC NULLS FIRST], false, 0\n",
            "      :     :  +- Exchange hashpartitioning(city#77, 200), ENSURE_REQUIREMENTS, [plan_id=267]\n",
            "      :     :     +- Project [driver_id#79, delivery_id#12, city#77, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8]\n",
            "      :     :        +- SortMergeJoin [driver_id#79], [driver_id#6], Inner\n",
            "      :     :           :- Sort [driver_id#79 ASC NULLS FIRST], false, 0\n",
            "      :     :           :  +- Exchange hashpartitioning(driver_id#79, 200), ENSURE_REQUIREMENTS, [plan_id=259]\n",
            "      :     :           :     +- Filter ((isnotnull(status#81) AND (status#81 = delivered)) AND (isnotnull(driver_id#79) AND isnotnull(city#77)))\n",
            "      :     :           :        +- SortAggregate(key=[delivery_id#12], functions=[first(city#13, false), first(driver_id#14, false), first(status#19, false), first(delivery_time_minutes#21, false), first(delivery_timestamp#22, false), first(is_corrupt#18, false)], output=[delivery_id#12, city#77, driver_id#79, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87])\n",
            "      :     :           :           +- Sort [delivery_id#12 ASC NULLS FIRST], false, 0\n",
            "      :     :           :              +- Exchange hashpartitioning(delivery_id#12, 200), ENSURE_REQUIREMENTS, [plan_id=253]\n",
            "      :     :           :                 +- SortAggregate(key=[delivery_id#12], functions=[partial_first(city#13, false), partial_first(driver_id#14, false), partial_first(status#19, false), partial_first(delivery_time_minutes#21, false), partial_first(delivery_timestamp#22, false), partial_first(is_corrupt#18, false)], output=[delivery_id#12, first#100, valueSet#101, first#102, valueSet#103, first#104, valueSet#105, first#106, valueSet#107, first#108, valueSet#109, first#110, valueSet#111])\n",
            "      :     :           :                    +- Sort [delivery_id#12 ASC NULLS FIRST], false, 0\n",
            "      :     :           :                       +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN (isnull(delivery_time_minutes#20) OR (delivery_time_minutes#20 < 0)) THEN 0 ELSE delivery_time_minutes#20 END AS delivery_time_minutes#21, coalesce(gettimestamp(delivery_timestamp#17, yyyy-MM-dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, dd/MM/yyyy HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, yyyy/MM/dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, dd-MM-yyyy HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS delivery_timestamp#22, is_corrupt#18]\n",
            "      :     :           :                          +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN RLIKE(delivery_time_minutes#16, ^[0-9]+$) THEN cast(delivery_time_minutes#16 as int) END AS delivery_time_minutes#20, delivery_timestamp#17, is_corrupt#18]\n",
            "      :     :           :                             +- Project [trim(delivery_id#0, None) AS delivery_id#12, trim(city#1, None) AS city#13, trim(driver_id#2, None) AS driver_id#14, lower(trim(status#3, None)) AS status#19, trim(delivery_time_minutes#4, None) AS delivery_time_minutes#16, trim(delivery_timestamp#5, None) AS delivery_timestamp#17, trim(cast(((isnull(delivery_time_minutes#4) OR RLIKE(delivery_time_minutes#4, ^[0-9]+$)) <=> true) as string), None) AS is_corrupt#18]\n",
            "      :     :           :                                +- Scan ExistingRDD[delivery_id#0,city#1,driver_id#2,status#3,delivery_time_minutes#4,delivery_timestamp#5]\n",
            "      :     :           +- Sort [driver_id#6 ASC NULLS FIRST], false, 0\n",
            "      :     :              +- Exchange hashpartitioning(driver_id#6, 200), ENSURE_REQUIREMENTS, [plan_id=260]\n",
            "      :     :                 +- Filter isnotnull(driver_id#6)\n",
            "      :     :                    +- Scan ExistingRDD[driver_id#6,driver_name#7,driver_type#8]\n",
            "      :     +- Sort [city#9 ASC NULLS FIRST], false, 0\n",
            "      :        +- Exchange hashpartitioning(city#9, 200), ENSURE_REQUIREMENTS, [plan_id=268]\n",
            "      :           +- Filter isnotnull(city#9)\n",
            "      :              +- Scan ExistingRDD[city#9,zone#10]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=274]\n",
            "         +- Filter isnotnull(city#74)\n",
            "            +- Scan ExistingRDD[city#74,zone#75]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 5 — ANALYTICS & WINDOW FUNCTIONS"
      ],
      "metadata": {
        "id": "YuJGzo8JeRYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Average delivery time per city"
      ],
      "metadata": {
        "id": "XnxNE4XheEr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_delivery_time_per_city_df = delivery_df.groupBy(\"city\").agg(\n",
        "    F.avg(\"delivery_time_minutes\").alias(\"avg_delivery_time_minutes\"))"
      ],
      "metadata": {
        "id": "Ai-A1hrqd8FJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Average delivery time per driver\n"
      ],
      "metadata": {
        "id": "GahsbGMHeHBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_delivery_time_per_driver_df = delivery_df.groupBy(\"driver_id\").agg(\n",
        "    F.avg(\"delivery_time_minutes\").alias(\"avg_delivery_time_minutes\"))"
      ],
      "metadata": {
        "id": "hgAtW5eoeIwE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Rank drivers by performance within each city\n"
      ],
      "metadata": {
        "id": "vA0Id6ypd_tF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "window_spec_city = Window.partitionBy(\"city\").orderBy(F.col(\"delivery_time_minutes\").asc())\n",
        "\n",
        "driver_rank_per_city_df = delivery_df.withColumn(\n",
        "    \"rank_per_city\",\n",
        "    F.rank().over(window_spec_city)\n",
        ")"
      ],
      "metadata": {
        "id": "NSVr2Qe4eLKZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Identify fastest driver per zone\n"
      ],
      "metadata": {
        "id": "F81udvTBeLkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "# The 'delivery_df' currently has two 'zone' columns due to previous joins,\n",
        "# leading to an 'AMBIGUOUS_REFERENCE' error when trying to access 'zone'.\n",
        "# To resolve this, we first drop all 'zone' columns from delivery_df.\n",
        "df_no_ambiguous_zone = delivery_df.drop('zone')\n",
        "\n",
        "# Then, we re-join with 'city_zone_df' to bring in a single, unambiguous 'zone' column.\n",
        "df_for_zone_analysis = df_no_ambiguous_zone.join(city_zone_df, on='city', how='inner')\n",
        "\n",
        "# Calculate average delivery time per driver per zone\n",
        "# Now, 'zone' is unambiguous, so we can group by it.\n",
        "avg_delivery_time_per_driver_zone = df_for_zone_analysis.groupBy(\"zone\", \"driver_id\", \"driver_name\") \\\n",
        "    .agg(F.avg(\"delivery_time_minutes\").alias(\"avg_delivery_time_minutes\"))\n",
        "\n",
        "# Define window specification for ranking within each zone\n",
        "zone_window_spec = Window.partitionBy(\"zone\").orderBy(F.col(\"avg_delivery_time_minutes\").asc())\n",
        "\n",
        "# Rank drivers by average delivery time within each zone and filter for rank 1 (fastest)\n",
        "fastest_driver_per_zone_df = avg_delivery_time_per_driver_zone.withColumn(\n",
        "    \"rank\",\n",
        "    F.rank().over(zone_window_spec)\n",
        ").filter(F.col(\"rank\") == 1)\n",
        "\n",
        "fastest_driver_per_zone_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzQEc9QXeN_o",
        "outputId": "cbd5aeac-d6fa-4653-f39b-63f5a8c7da9e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+-----------+-------------------------+----+\n",
            "| zone|driver_id|driver_name|avg_delivery_time_minutes|rank|\n",
            "+-----+---------+-----------+-------------------------+----+\n",
            "|North|     D001|       Ravi|                    130.0|   1|\n",
            "|South|     D002|       Amit|                      0.0|   1|\n",
            "| West|     D005|       Neha|                      0.0|   1|\n",
            "+-----+---------+-----------+-------------------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Identify top 2 drivers per city"
      ],
      "metadata": {
        "id": "dL5Z4TEEeOWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "avg_delivery_time_per_driver_city = delivery_df.groupBy(\"city\", \"driver_id\", \"driver_name\")\\\n",
        "    .agg(F.avg(\"delivery_time_minutes\").alias(\"avg_delivery_time_minutes\"))\n",
        "\n",
        "window_spec_city_driver = Window.partitionBy(\"city\").orderBy(F.col(\"avg_delivery_time_minutes\").asc())\n",
        "\n",
        "top_2_drivers_per_city_df = avg_delivery_time_per_driver_city.withColumn(\n",
        "    \"rank_in_city\",\n",
        "    F.rank().over(window_spec_city_driver)\n",
        ").filter(F.col(\"rank_in_city\") <= 2)\n",
        "\n",
        "top_2_drivers_per_city_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1ulHaepePTk",
        "outputId": "27dbc483-6f60-4498-8cbf-2fdd3eb00fcb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+-------------------------+------------+\n",
            "|     city|driver_id|driver_name|avg_delivery_time_minutes|rank_in_city|\n",
            "+---------+---------+-----------+-------------------------+------------+\n",
            "|Bangalore|     D003|      Sneha|                    160.0|           1|\n",
            "|  Chennai|     D002|       Amit|                      0.0|           1|\n",
            "|    Delhi|     D001|       Ravi|                    130.0|           1|\n",
            "|   Mumbai|     D005|       Neha|                      0.0|           1|\n",
            "|   Mumbai|     D002|       Amit|                     90.0|           2|\n",
            "+---------+---------+-----------+-------------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 6 — PERFORMANCE OPTIMIZATION"
      ],
      "metadata": {
        "id": "qhhAbS8Bf2q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Identify DataFrames reused multiple times\n"
      ],
      "metadata": {
        "id": "4khVP4oTfHpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkYBKI1GfLLv",
        "outputId": "6fdc9de8-530b-4283-ce88-17689889fd65"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[city: string, driver_id: string, delivery_id: string, status: string, delivery_time_minutes: int, delivery_timestamp: timestamp, is_corrupt: string, driver_name: string, driver_type: string, zone: string, zone: string]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Apply caching appropriately\n"
      ],
      "metadata": {
        "id": "hheBuw4_fLln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP2CkkcOfPri",
        "outputId": "d75608b6-d5bc-4276-dc94-8b02aac6b53a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Compare execution plans with and without cache\n"
      ],
      "metadata": {
        "id": "fb-S4LRjfP_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0vMDD4RfRV9",
        "outputId": "588d4bee-9f12-4f7c-8ff3-4f80a2619fa9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Project [city#13, driver_id#14, delivery_id#12, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8, zone#10]\n",
            ":  +- Join Inner, (city#13 = city#9)\n",
            ":     :- Project [driver_id#14, delivery_id#12, city#13, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8]\n",
            ":     :  +- Join Inner, (driver_id#14 = driver_id#6)\n",
            ":     :     :- Filter (status#19 = delivered)\n",
            ":     :     :  +- Deduplicate [delivery_id#12]\n",
            ":     :     :     +- Project [delivery_id#12, city#13, driver_id#14, status#19, delivery_time_minutes#21, coalesce(try_to_timestamp(delivery_timestamp#17, Some(yyyy-MM-dd HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(dd/MM/yyyy HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(yyyy/MM/dd HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(dd-MM-yyyy HH:mm), TimestampType, Some(Etc/UTC), false)) AS delivery_timestamp#22, is_corrupt#18]\n",
            ":     :     :        +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN (isnull(delivery_time_minutes#20) OR (delivery_time_minutes#20 < 0)) THEN 0 ELSE delivery_time_minutes#20 END AS delivery_time_minutes#21, delivery_timestamp#17, is_corrupt#18]\n",
            ":     :     :           +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN RLIKE(delivery_time_minutes#16, ^[0-9]+$) THEN cast(delivery_time_minutes#16 as int) ELSE cast(null as int) END AS delivery_time_minutes#20, delivery_timestamp#17, is_corrupt#18]\n",
            ":     :     :              +- Project [delivery_id#12, city#13, driver_id#14, lower(status#15) AS status#19, delivery_time_minutes#16, delivery_timestamp#17, is_corrupt#18]\n",
            ":     :     :                 +- Project [delivery_id#12, city#13, driver_id#14, status#15, delivery_time_minutes#16, delivery_timestamp#17, trim(cast(is_corrupt#11 as string), None) AS is_corrupt#18]\n",
            ":     :     :                    +- Project [delivery_id#12, city#13, driver_id#14, status#15, delivery_time_minutes#16, trim(delivery_timestamp#5, None) AS delivery_timestamp#17, is_corrupt#11]\n",
            ":     :     :                       +- Project [delivery_id#12, city#13, driver_id#14, status#15, trim(delivery_time_minutes#4, None) AS delivery_time_minutes#16, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                          +- Project [delivery_id#12, city#13, driver_id#14, trim(status#3, None) AS status#15, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                             +- Project [delivery_id#12, city#13, trim(driver_id#2, None) AS driver_id#14, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                                +- Project [delivery_id#12, trim(city#1, None) AS city#13, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                                   +- Project [trim(delivery_id#0, None) AS delivery_id#12, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                                      +- Project [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, CASE WHEN (isnull(delivery_time_minutes#4) OR RLIKE(delivery_time_minutes#4, ^[0-9]+$)) THEN true ELSE false END AS is_corrupt#11]\n",
            ":     :     :                                         +- LogicalRDD [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5], false\n",
            ":     :     +- LogicalRDD [driver_id#6, driver_name#7, driver_type#8], false\n",
            ":     +- LogicalRDD [city#9, zone#10], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#74, zone#75], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, driver_id: string, delivery_id: string, status: string, delivery_time_minutes: int, delivery_timestamp: timestamp, is_corrupt: string, driver_name: string, driver_type: string, zone: string, zone: string\n",
            "Project [city#13, driver_id#14, delivery_id#12, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8, zone#10, zone#75]\n",
            "+- Join Inner, (city#13 = city#74)\n",
            "   :- Project [city#13, driver_id#14, delivery_id#12, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8, zone#10]\n",
            "   :  +- Join Inner, (city#13 = city#9)\n",
            "   :     :- Project [driver_id#14, delivery_id#12, city#13, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8]\n",
            "   :     :  +- Join Inner, (driver_id#14 = driver_id#6)\n",
            "   :     :     :- Filter (status#19 = delivered)\n",
            "   :     :     :  +- Deduplicate [delivery_id#12]\n",
            "   :     :     :     +- Project [delivery_id#12, city#13, driver_id#14, status#19, delivery_time_minutes#21, coalesce(try_to_timestamp(delivery_timestamp#17, Some(yyyy-MM-dd HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(dd/MM/yyyy HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(yyyy/MM/dd HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(dd-MM-yyyy HH:mm), TimestampType, Some(Etc/UTC), false)) AS delivery_timestamp#22, is_corrupt#18]\n",
            "   :     :     :        +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN (isnull(delivery_time_minutes#20) OR (delivery_time_minutes#20 < 0)) THEN 0 ELSE delivery_time_minutes#20 END AS delivery_time_minutes#21, delivery_timestamp#17, is_corrupt#18]\n",
            "   :     :     :           +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN RLIKE(delivery_time_minutes#16, ^[0-9]+$) THEN cast(delivery_time_minutes#16 as int) ELSE cast(null as int) END AS delivery_time_minutes#20, delivery_timestamp#17, is_corrupt#18]\n",
            "   :     :     :              +- Project [delivery_id#12, city#13, driver_id#14, lower(status#15) AS status#19, delivery_time_minutes#16, delivery_timestamp#17, is_corrupt#18]\n",
            "   :     :     :                 +- Project [delivery_id#12, city#13, driver_id#14, status#15, delivery_time_minutes#16, delivery_timestamp#17, trim(cast(is_corrupt#11 as string), None) AS is_corrupt#18]\n",
            "   :     :     :                    +- Project [delivery_id#12, city#13, driver_id#14, status#15, delivery_time_minutes#16, trim(delivery_timestamp#5, None) AS delivery_timestamp#17, is_corrupt#11]\n",
            "   :     :     :                       +- Project [delivery_id#12, city#13, driver_id#14, status#15, trim(delivery_time_minutes#4, None) AS delivery_time_minutes#16, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                          +- Project [delivery_id#12, city#13, driver_id#14, trim(status#3, None) AS status#15, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                             +- Project [delivery_id#12, city#13, trim(driver_id#2, None) AS driver_id#14, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                                +- Project [delivery_id#12, trim(city#1, None) AS city#13, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                                   +- Project [trim(delivery_id#0, None) AS delivery_id#12, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                                      +- Project [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, CASE WHEN (isnull(delivery_time_minutes#4) OR RLIKE(delivery_time_minutes#4, ^[0-9]+$)) THEN true ELSE false END AS is_corrupt#11]\n",
            "   :     :     :                                         +- LogicalRDD [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5], false\n",
            "   :     :     +- LogicalRDD [driver_id#6, driver_name#7, driver_type#8], false\n",
            "   :     +- LogicalRDD [city#9, zone#10], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#74, zone#75], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#77, driver_id#79, delivery_id#12, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8, zone#10, zone#75]\n",
            "+- Join Inner, (city#77 = city#74), rightHint=(strategy=broadcast)\n",
            "   :- Project [city#77, driver_id#79, delivery_id#12, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8, zone#10]\n",
            "   :  +- Join Inner, (city#77 = city#9)\n",
            "   :     :- Project [driver_id#79, delivery_id#12, city#77, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8]\n",
            "   :     :  +- Join Inner, (driver_id#79 = driver_id#6)\n",
            "   :     :     :- Filter ((isnotnull(status#81) AND (status#81 = delivered)) AND (isnotnull(driver_id#79) AND isnotnull(city#77)))\n",
            "   :     :     :  +- Aggregate [delivery_id#12], [delivery_id#12, first(city#13, false) AS city#77, first(driver_id#14, false) AS driver_id#79, first(status#19, false) AS status#81, first(delivery_time_minutes#21, false) AS delivery_time_minutes#83, first(delivery_timestamp#22, false) AS delivery_timestamp#85, first(is_corrupt#18, false) AS is_corrupt#87]\n",
            "   :     :     :     +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN (isnull(delivery_time_minutes#20) OR (delivery_time_minutes#20 < 0)) THEN 0 ELSE delivery_time_minutes#20 END AS delivery_time_minutes#21, coalesce(gettimestamp(delivery_timestamp#17, yyyy-MM-dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, dd/MM/yyyy HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, yyyy/MM/dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, dd-MM-yyyy HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS delivery_timestamp#22, is_corrupt#18]\n",
            "   :     :     :        +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN RLIKE(delivery_time_minutes#16, ^[0-9]+$) THEN cast(delivery_time_minutes#16 as int) END AS delivery_time_minutes#20, delivery_timestamp#17, is_corrupt#18]\n",
            "   :     :     :           +- Project [trim(delivery_id#0, None) AS delivery_id#12, trim(city#1, None) AS city#13, trim(driver_id#2, None) AS driver_id#14, lower(trim(status#3, None)) AS status#19, trim(delivery_time_minutes#4, None) AS delivery_time_minutes#16, trim(delivery_timestamp#5, None) AS delivery_timestamp#17, trim(cast(((isnull(delivery_time_minutes#4) OR RLIKE(delivery_time_minutes#4, ^[0-9]+$)) <=> true) as string), None) AS is_corrupt#18]\n",
            "   :     :     :              +- LogicalRDD [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5], false\n",
            "   :     :     +- Filter isnotnull(driver_id#6)\n",
            "   :     :        +- LogicalRDD [driver_id#6, driver_name#7, driver_type#8], false\n",
            "   :     +- Filter isnotnull(city#9)\n",
            "   :        +- LogicalRDD [city#9, zone#10], false\n",
            "   +- Filter isnotnull(city#74)\n",
            "      +- LogicalRDD [city#74, zone#75], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#77, driver_id#79, delivery_id#12, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8, zone#10, zone#75]\n",
            "   +- BroadcastHashJoin [city#77], [city#74], Inner, BuildRight, false\n",
            "      :- Project [city#77, driver_id#79, delivery_id#12, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8, zone#10]\n",
            "      :  +- SortMergeJoin [city#77], [city#9], Inner\n",
            "      :     :- Sort [city#77 ASC NULLS FIRST], false, 0\n",
            "      :     :  +- Exchange hashpartitioning(city#77, 200), ENSURE_REQUIREMENTS, [plan_id=267]\n",
            "      :     :     +- Project [driver_id#79, delivery_id#12, city#77, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8]\n",
            "      :     :        +- SortMergeJoin [driver_id#79], [driver_id#6], Inner\n",
            "      :     :           :- Sort [driver_id#79 ASC NULLS FIRST], false, 0\n",
            "      :     :           :  +- Exchange hashpartitioning(driver_id#79, 200), ENSURE_REQUIREMENTS, [plan_id=259]\n",
            "      :     :           :     +- Filter ((isnotnull(status#81) AND (status#81 = delivered)) AND (isnotnull(driver_id#79) AND isnotnull(city#77)))\n",
            "      :     :           :        +- SortAggregate(key=[delivery_id#12], functions=[first(city#13, false), first(driver_id#14, false), first(status#19, false), first(delivery_time_minutes#21, false), first(delivery_timestamp#22, false), first(is_corrupt#18, false)], output=[delivery_id#12, city#77, driver_id#79, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87])\n",
            "      :     :           :           +- Sort [delivery_id#12 ASC NULLS FIRST], false, 0\n",
            "      :     :           :              +- Exchange hashpartitioning(delivery_id#12, 200), ENSURE_REQUIREMENTS, [plan_id=253]\n",
            "      :     :           :                 +- SortAggregate(key=[delivery_id#12], functions=[partial_first(city#13, false), partial_first(driver_id#14, false), partial_first(status#19, false), partial_first(delivery_time_minutes#21, false), partial_first(delivery_timestamp#22, false), partial_first(is_corrupt#18, false)], output=[delivery_id#12, first#100, valueSet#101, first#102, valueSet#103, first#104, valueSet#105, first#106, valueSet#107, first#108, valueSet#109, first#110, valueSet#111])\n",
            "      :     :           :                    +- Sort [delivery_id#12 ASC NULLS FIRST], false, 0\n",
            "      :     :           :                       +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN (isnull(delivery_time_minutes#20) OR (delivery_time_minutes#20 < 0)) THEN 0 ELSE delivery_time_minutes#20 END AS delivery_time_minutes#21, coalesce(gettimestamp(delivery_timestamp#17, yyyy-MM-dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, dd/MM/yyyy HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, yyyy/MM/dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, dd-MM-yyyy HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS delivery_timestamp#22, is_corrupt#18]\n",
            "      :     :           :                          +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN RLIKE(delivery_time_minutes#16, ^[0-9]+$) THEN cast(delivery_time_minutes#16 as int) END AS delivery_time_minutes#20, delivery_timestamp#17, is_corrupt#18]\n",
            "      :     :           :                             +- Project [trim(delivery_id#0, None) AS delivery_id#12, trim(city#1, None) AS city#13, trim(driver_id#2, None) AS driver_id#14, lower(trim(status#3, None)) AS status#19, trim(delivery_time_minutes#4, None) AS delivery_time_minutes#16, trim(delivery_timestamp#5, None) AS delivery_timestamp#17, trim(cast(((isnull(delivery_time_minutes#4) OR RLIKE(delivery_time_minutes#4, ^[0-9]+$)) <=> true) as string), None) AS is_corrupt#18]\n",
            "      :     :           :                                +- Scan ExistingRDD[delivery_id#0,city#1,driver_id#2,status#3,delivery_time_minutes#4,delivery_timestamp#5]\n",
            "      :     :           +- Sort [driver_id#6 ASC NULLS FIRST], false, 0\n",
            "      :     :              +- Exchange hashpartitioning(driver_id#6, 200), ENSURE_REQUIREMENTS, [plan_id=260]\n",
            "      :     :                 +- Filter isnotnull(driver_id#6)\n",
            "      :     :                    +- Scan ExistingRDD[driver_id#6,driver_name#7,driver_type#8]\n",
            "      :     +- Sort [city#9 ASC NULLS FIRST], false, 0\n",
            "      :        +- Exchange hashpartitioning(city#9, 200), ENSURE_REQUIREMENTS, [plan_id=268]\n",
            "      :           +- Filter isnotnull(city#9)\n",
            "      :              +- Scan ExistingRDD[city#9,zone#10]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=274]\n",
            "         +- Filter isnotnull(city#74)\n",
            "            +- Scan ExistingRDD[city#74,zone#75]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Repartition data by city"
      ],
      "metadata": {
        "id": "YBWlf2FsfRr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repartition_df = delivery_df.repartition(col(\"city\"))"
      ],
      "metadata": {
        "id": "v4XycxmxfSU7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Explain why repartitioning improves performance"
      ],
      "metadata": {
        "id": "6_X4QxokfpU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reduces skew during aggregations\n",
        "#Improves parallelism\n",
        "#Optimizes shuffle boundaries"
      ],
      "metadata": {
        "id": "tSPP_NSkfvy4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 7 — FILE FORMAT STRATEGY"
      ],
      "metadata": {
        "id": "l13GJrqPfxx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Write cleaned delivery data to Parquet\n"
      ],
      "metadata": {
        "id": "ye7V844mfUw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df_cleaned_for_write = delivery_df.drop(delivery_df.columns[9]) # Assuming the second 'zone' is at index 9\n",
        "delivery_df_cleaned_for_write.write.mode(\"overwrite\").parquet(\"delivery_data.parquet\")"
      ],
      "metadata": {
        "id": "JhjuYi3dfXRy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Write aggregated analytics to ORC\n"
      ],
      "metadata": {
        "id": "0DhxKudXfXo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fastest_driver_per_zone_df.write.mode(\"overwrite\").orc(\"fastest_drivers_per_zone.orc\")"
      ],
      "metadata": {
        "id": "71miBZZLfZLf"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Compare file output structure\n"
      ],
      "metadata": {
        "id": "N7Vo6AkyfZh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSchema of Parquet file (delivery_data.parquet):\")\n",
        "parquet_df_read = spark.read.parquet(\"delivery_data.parquet\")\n",
        "parquet_df_read.printSchema()\n",
        "\n",
        "print(\"\\nSchema of ORC file (fastest_drivers_per_zone.orc):\")\n",
        "orc_df_read = spark.read.orc(\"fastest_drivers_per_zone.orc\")\n",
        "orc_df_read.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6eFSSPRfbFm",
        "outputId": "23b148f8-092e-44a1-a4ba-59ad0aee2e5d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Schema of Parquet file (delivery_data.parquet):\n",
            "root\n",
            " |-- city: string (nullable = true)\n",
            " |-- driver_id: string (nullable = true)\n",
            " |-- delivery_id: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- delivery_time_minutes: integer (nullable = true)\n",
            " |-- delivery_timestamp: timestamp (nullable = true)\n",
            " |-- is_corrupt: string (nullable = true)\n",
            " |-- driver_name: string (nullable = true)\n",
            " |-- driver_type: string (nullable = true)\n",
            "\n",
            "\n",
            "Schema of ORC file (fastest_drivers_per_zone.orc):\n",
            "root\n",
            " |-- zone: string (nullable = true)\n",
            " |-- driver_id: string (nullable = true)\n",
            " |-- driver_name: string (nullable = true)\n",
            " |-- avg_delivery_time_minutes: double (nullable = true)\n",
            " |-- rank: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Explain why Avro is suitable for future real-time tracking"
      ],
      "metadata": {
        "id": "U-n6qMyTfbeu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Schema evolution\n",
        "#Row-based\n",
        "#Streaming compatibility"
      ],
      "metadata": {
        "id": "OwZHlXZsfcHB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PHASE 8 — DEBUGGING & ERROR ANALYSIS"
      ],
      "metadata": {
        "id": "69nr2jerfzWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Identify potential NoneType errors\n"
      ],
      "metadata": {
        "id": "vJaRSxlpg8Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(delivery_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d25wxa6XnC2M",
        "outputId": "646d7b88-548c-447d-cf7d-dfaa9b718c75"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['city', 'driver_id', 'delivery_id', 'status', 'delivery_time_minutes', 'delivery_timestamp', 'is_corrupt', 'driver_name', 'driver_type', 'zone', 'zone']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "from collections import Counter\n",
        "\n",
        "# --- 1) Make column names unique (so Spark can reference them reliably)\n",
        "if len(delivery_df.columns) != len(set(delivery_df.columns)):\n",
        "    seen = {}\n",
        "    new_cols = []\n",
        "    for c in delivery_df.columns:\n",
        "        if c not in seen:\n",
        "            seen[c] = 0\n",
        "            new_cols.append(c)\n",
        "        else:\n",
        "            seen[c] += 1\n",
        "            new_cols.append(f\"{c}_dup{seen[c]}\")\n",
        "    delivery_df = delivery_df.toDF(*new_cols)\n",
        "\n",
        "# --- 2) Coalesce duplicate 'zone' columns into a single 'zone'\n",
        "zone_like = [c for c in delivery_df.columns if c == \"zone\" or c.startswith(\"zone_dup\")]\n",
        "if \"zone\" in delivery_df.columns and len(zone_like) > 1:\n",
        "    # Prefer original 'zone', then any zone_dup* columns\n",
        "    delivery_df = delivery_df.withColumn(\"zone\", F.coalesce(*[F.col(c) for c in [\"zone\"] + [c for c in zone_like if c != \"zone\"]]))\n",
        "    # Drop the duplicate zone columns\n",
        "    delivery_df = delivery_df.drop(*[c for c in zone_like if c != \"zone\"])\n",
        "\n",
        "# --- 3) Quick checks for potential None/NULL issues\n",
        "# 3a) Count NULLs per column (safe, no ambiguous references)\n",
        "null_counts = delivery_df.select([\n",
        "    F.count(F.when(F.col(c).isNull(), F.lit(1))).alias(c) for c in delivery_df.columns\n",
        "]).collect()[0].asDict()\n",
        "\n",
        "print(\"NULL counts per column:\", null_counts)\n",
        "\n",
        "# 3b) Flag if the join key 'zone' is NULL anywhere (common source of NoneType errors later)\n",
        "zone_nulls = delivery_df.filter(F.col(\"zone\").isNull()).limit(1).count()\n",
        "if zone_nulls > 0:\n",
        "    print(f\"Warning: 'zone' has NULLs; downstream joins/expressions may hit NoneType/ambiguous errors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRbNM9YrpXFJ",
        "outputId": "0ad56dca-3a87-48e2-b7f1-07d87aae9cb0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NULL counts per column: {'city': 0, 'driver_id': 0, 'delivery_id': 0, 'status': 0, 'delivery_time_minutes': 0, 'delivery_timestamp': 0, 'is_corrupt': 0, 'driver_name': 0, 'driver_type': 0, 'zone': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Identify schema mismatch risks\n"
      ],
      "metadata": {
        "id": "cEDnRq87hJyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# --- Fix duplicate 'zone' columns ---\n",
        "zone_cols = [c for c in delivery_df.columns if c == \"zone\"]\n",
        "if len(zone_cols) > 1:\n",
        "    # Rename duplicates to make them accessible\n",
        "    delivery_df = delivery_df.toDF(*[\n",
        "        f\"{c}_{i}\" if c == \"zone\" and i > 0 else c\n",
        "        for i, c in enumerate(delivery_df.columns)\n",
        "    ])\n",
        "    # Coalesce zone columns into one\n",
        "    zone_variants = [c for c in delivery_df.columns if c.startswith(\"zone\")]\n",
        "    delivery_df = delivery_df.withColumn(\"zone\", F.coalesce(*[F.col(c) for c in zone_variants]))\n",
        "    # Drop extra zone columns\n",
        "    delivery_df = delivery_df.drop(*[c for c in zone_variants if c != \"zone\"])\n",
        "\n",
        "# --- Identify potential NoneType risks ---\n",
        "null_counts = delivery_df.select([\n",
        "    F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in delivery_df.columns\n",
        "]).collect()[0].asDict()\n",
        "\n",
        "print(\"NULL counts per column:\", null_counts)\n",
        "if null_counts.get(\"zone\", 0) > 0:\n",
        "    print(\"⚠ Warning: 'zone' has NULL values. Consider fillna or filter before joins.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v88iHzLbhL6N",
        "outputId": "2c9272e0-d9fa-4fb0-c79a-8ed3bda29655"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NULL counts per column: {'city': 0, 'driver_id': 0, 'delivery_id': 0, 'status': 0, 'delivery_time_minutes': 0, 'delivery_timestamp': 0, 'is_corrupt': 0, 'driver_name': 0, 'driver_type': 0, 'zone': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Debug an intentionally broken transformation\n"
      ],
      "metadata": {
        "id": "KN1A0ad9hMPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intentionally broken transformation: Attempting numeric aggregation on a string column with non-numeric values.\n",
        "\n",
        "# 1. Create a DataFrame that mirrors the initial raw data state for 'delivery_time_minutes'\n",
        "#    where it's treated as a StringType, including non-numeric entries ('invalid', '').\n",
        "broken_df = spark.createDataFrame(data=delivery_data, schema=column)\n",
        "\n",
        "print(\"Schema of the intentionally broken DataFrame before aggregation:\")\n",
        "broken_df.printSchema()\n",
        "\n",
        "print(\"\\nAttempting to calculate the sum of 'delivery_time_minutes'...\")\n",
        "try:\n",
        "    # This line is expected to fail due to non-numeric values in 'delivery_time_minutes'\n",
        "    broken_df.groupBy(\"city\").agg(F.sum(\"delivery_time_minutes\").alias(\"total_delivery_time\")).show()\n",
        "except Exception as e:\n",
        "    print(\"\\n--- ERROR ENCOUNTERED ---\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    print(\"\\nThis `NumberFormatException` occurs because the 'delivery_time_minutes' column is a StringType and contains non-numeric string values (like 'invalid', empty strings, or None) that Spark cannot implicitly convert to a numeric type for the sum aggregation. Spark expects numeric types for such operations.\")\n",
        "    print(\"\\n--- Proposed Fix ---\")\n",
        "    print(\"To fix this, we need to explicitly cast the 'delivery_time_minutes' column to a numeric type (e.g., IntegerType or DoubleType), and handle the malformed data gracefully. A robust approach involves checking if the string values are numeric before casting, and providing a default value (like 0 or null) for non-numeric entries.\")\n",
        "    print(\"\\nHere's an example of how to fix this transformation:\")\n",
        "    print(\"\"\"\n",
        "fixed_broken_df = broken_df.withColumn(\n",
        "    \"delivery_time_minutes_fixed\",\n",
        "    F.when(\n",
        "        F.col(\"delivery_time_minutes\").rlike(\"^[0-9]+$\"), # Check if string contains only digits\n",
        "        F.col(\"delivery_time_minutes\").cast(IntegerType())\n",
        "    ).otherwise(F.lit(0).cast(IntegerType())) # Default to 0 for non-numeric or nulls\n",
        ")\n",
        "\n",
        "print(\"\\nSchema after applying fix:\")\n",
        "fixed_broken_df.printSchema()\n",
        "\n",
        "print(\"\\nResult after applying fix (sum of 'delivery_time_minutes_fixed'):\")\n",
        "fixed_broken_df.groupBy(\"city\").agg(F.sum(\"delivery_time_minutes_fixed\").alias(\"total_delivery_time\")).show()\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqn_7_N3hRtd",
        "outputId": "23a9e3cf-0138-49d5-fd7a-d95e212eea0a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema of the intentionally broken DataFrame before aggregation:\n",
            "root\n",
            " |-- delivery_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- driver_id: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- delivery_time_minutes: string (nullable = true)\n",
            " |-- delivery_timestamp: string (nullable = true)\n",
            "\n",
            "\n",
            "Attempting to calculate the sum of 'delivery_time_minutes'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "{\"ts\": \"2025-12-23 09:59:47.469\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[CAST_INVALID_INPUT] The value '' of the type \\\"STRING\\\" cannot be cast to \\\"DOUBLE\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\", \"context\": {\"file\": \"java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\", \"line\": \"\", \"fragment\": \"sum\", \"errorClass\": \"CAST_INVALID_INPUT\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o997.showString.\\n: org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value '' of the type \\\"STRING\\\" cannot be cast to \\\"DOUBLE\\\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\\n== DataFrame ==\\n\\\"sum\\\" was called from\\njava.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:145)\\n\\tat org.apache.spark.sql.errors.QueryExecutionErrors.invalidInputInCastToNumberError(QueryExecutionErrors.scala)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregate_sum_0$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doConsume_0$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\\n\\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\\n\\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\\n\\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\\n\\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:143)\\n\\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:57)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:111)\\n\\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\\n\\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\\n\\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\\n\\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\\n\\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\\n\\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ERROR ENCOUNTERED ---\n",
            "Error Type: NumberFormatException\n",
            "Error Message: [CAST_INVALID_INPUT] The value '' of the type \"STRING\" cannot be cast to \"DOUBLE\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. SQLSTATE: 22018\n",
            "== DataFrame ==\n",
            "\"sum\" was called from\n",
            "java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\n",
            "\n",
            "This `NumberFormatException` occurs because the 'delivery_time_minutes' column is a StringType and contains non-numeric string values (like 'invalid', empty strings, or None) that Spark cannot implicitly convert to a numeric type for the sum aggregation. Spark expects numeric types for such operations.\n",
            "\n",
            "--- Proposed Fix ---\n",
            "To fix this, we need to explicitly cast the 'delivery_time_minutes' column to a numeric type (e.g., IntegerType or DoubleType), and handle the malformed data gracefully. A robust approach involves checking if the string values are numeric before casting, and providing a default value (like 0 or null) for non-numeric entries.\n",
            "\n",
            "Here's an example of how to fix this transformation:\n",
            "\n",
            "fixed_broken_df = broken_df.withColumn(\n",
            "    \"delivery_time_minutes_fixed\",\n",
            "    F.when(\n",
            "        F.col(\"delivery_time_minutes\").rlike(\"^[0-9]+$\"), # Check if string contains only digits\n",
            "        F.col(\"delivery_time_minutes\").cast(IntegerType())\n",
            "    ).otherwise(F.lit(0).cast(IntegerType())) # Default to 0 for non-numeric or nulls\n",
            ")\n",
            "\n",
            "print(\"\n",
            "Schema after applying fix:\")\n",
            "fixed_broken_df.printSchema()\n",
            "\n",
            "print(\"\n",
            "Result after applying fix (sum of 'delivery_time_minutes_fixed'):\")\n",
            "fixed_broken_df.groupBy(\"city\").agg(F.sum(\"delivery_time_minutes_fixed\").alias(\"total_delivery_time\")).show()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Use explain plan to find inefficient operations"
      ],
      "metadata": {
        "id": "W8Yk3QtPhSdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delivery_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMm1ZhI1hTS7",
        "outputId": "78bc5bb2-9b13-44b3-be67-3dd19689c86f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Project [city#13, driver_id#14, delivery_id#12, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8, zone#10]\n",
            ":  +- Join Inner, (city#13 = city#9)\n",
            ":     :- Project [driver_id#14, delivery_id#12, city#13, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8]\n",
            ":     :  +- Join Inner, (driver_id#14 = driver_id#6)\n",
            ":     :     :- Filter (status#19 = delivered)\n",
            ":     :     :  +- Deduplicate [delivery_id#12]\n",
            ":     :     :     +- Project [delivery_id#12, city#13, driver_id#14, status#19, delivery_time_minutes#21, coalesce(try_to_timestamp(delivery_timestamp#17, Some(yyyy-MM-dd HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(dd/MM/yyyy HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(yyyy/MM/dd HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(dd-MM-yyyy HH:mm), TimestampType, Some(Etc/UTC), false)) AS delivery_timestamp#22, is_corrupt#18]\n",
            ":     :     :        +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN (isnull(delivery_time_minutes#20) OR (delivery_time_minutes#20 < 0)) THEN 0 ELSE delivery_time_minutes#20 END AS delivery_time_minutes#21, delivery_timestamp#17, is_corrupt#18]\n",
            ":     :     :           +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN RLIKE(delivery_time_minutes#16, ^[0-9]+$) THEN cast(delivery_time_minutes#16 as int) ELSE cast(null as int) END AS delivery_time_minutes#20, delivery_timestamp#17, is_corrupt#18]\n",
            ":     :     :              +- Project [delivery_id#12, city#13, driver_id#14, lower(status#15) AS status#19, delivery_time_minutes#16, delivery_timestamp#17, is_corrupt#18]\n",
            ":     :     :                 +- Project [delivery_id#12, city#13, driver_id#14, status#15, delivery_time_minutes#16, delivery_timestamp#17, trim(cast(is_corrupt#11 as string), None) AS is_corrupt#18]\n",
            ":     :     :                    +- Project [delivery_id#12, city#13, driver_id#14, status#15, delivery_time_minutes#16, trim(delivery_timestamp#5, None) AS delivery_timestamp#17, is_corrupt#11]\n",
            ":     :     :                       +- Project [delivery_id#12, city#13, driver_id#14, status#15, trim(delivery_time_minutes#4, None) AS delivery_time_minutes#16, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                          +- Project [delivery_id#12, city#13, driver_id#14, trim(status#3, None) AS status#15, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                             +- Project [delivery_id#12, city#13, trim(driver_id#2, None) AS driver_id#14, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                                +- Project [delivery_id#12, trim(city#1, None) AS city#13, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                                   +- Project [trim(delivery_id#0, None) AS delivery_id#12, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            ":     :     :                                      +- Project [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, CASE WHEN (isnull(delivery_time_minutes#4) OR RLIKE(delivery_time_minutes#4, ^[0-9]+$)) THEN true ELSE false END AS is_corrupt#11]\n",
            ":     :     :                                         +- LogicalRDD [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5], false\n",
            ":     :     +- LogicalRDD [driver_id#6, driver_name#7, driver_type#8], false\n",
            ":     +- LogicalRDD [city#9, zone#10], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#74, zone#75], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, driver_id: string, delivery_id: string, status: string, delivery_time_minutes: int, delivery_timestamp: timestamp, is_corrupt: string, driver_name: string, driver_type: string, zone: string, zone: string\n",
            "Project [city#13, driver_id#14, delivery_id#12, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8, zone#10, zone#75]\n",
            "+- Join Inner, (city#13 = city#74)\n",
            "   :- Project [city#13, driver_id#14, delivery_id#12, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8, zone#10]\n",
            "   :  +- Join Inner, (city#13 = city#9)\n",
            "   :     :- Project [driver_id#14, delivery_id#12, city#13, status#19, delivery_time_minutes#21, delivery_timestamp#22, is_corrupt#18, driver_name#7, driver_type#8]\n",
            "   :     :  +- Join Inner, (driver_id#14 = driver_id#6)\n",
            "   :     :     :- Filter (status#19 = delivered)\n",
            "   :     :     :  +- Deduplicate [delivery_id#12]\n",
            "   :     :     :     +- Project [delivery_id#12, city#13, driver_id#14, status#19, delivery_time_minutes#21, coalesce(try_to_timestamp(delivery_timestamp#17, Some(yyyy-MM-dd HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(dd/MM/yyyy HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(yyyy/MM/dd HH:mm), TimestampType, Some(Etc/UTC), false), try_to_timestamp(delivery_timestamp#17, Some(dd-MM-yyyy HH:mm), TimestampType, Some(Etc/UTC), false)) AS delivery_timestamp#22, is_corrupt#18]\n",
            "   :     :     :        +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN (isnull(delivery_time_minutes#20) OR (delivery_time_minutes#20 < 0)) THEN 0 ELSE delivery_time_minutes#20 END AS delivery_time_minutes#21, delivery_timestamp#17, is_corrupt#18]\n",
            "   :     :     :           +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN RLIKE(delivery_time_minutes#16, ^[0-9]+$) THEN cast(delivery_time_minutes#16 as int) ELSE cast(null as int) END AS delivery_time_minutes#20, delivery_timestamp#17, is_corrupt#18]\n",
            "   :     :     :              +- Project [delivery_id#12, city#13, driver_id#14, lower(status#15) AS status#19, delivery_time_minutes#16, delivery_timestamp#17, is_corrupt#18]\n",
            "   :     :     :                 +- Project [delivery_id#12, city#13, driver_id#14, status#15, delivery_time_minutes#16, delivery_timestamp#17, trim(cast(is_corrupt#11 as string), None) AS is_corrupt#18]\n",
            "   :     :     :                    +- Project [delivery_id#12, city#13, driver_id#14, status#15, delivery_time_minutes#16, trim(delivery_timestamp#5, None) AS delivery_timestamp#17, is_corrupt#11]\n",
            "   :     :     :                       +- Project [delivery_id#12, city#13, driver_id#14, status#15, trim(delivery_time_minutes#4, None) AS delivery_time_minutes#16, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                          +- Project [delivery_id#12, city#13, driver_id#14, trim(status#3, None) AS status#15, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                             +- Project [delivery_id#12, city#13, trim(driver_id#2, None) AS driver_id#14, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                                +- Project [delivery_id#12, trim(city#1, None) AS city#13, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                                   +- Project [trim(delivery_id#0, None) AS delivery_id#12, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, is_corrupt#11]\n",
            "   :     :     :                                      +- Project [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5, CASE WHEN (isnull(delivery_time_minutes#4) OR RLIKE(delivery_time_minutes#4, ^[0-9]+$)) THEN true ELSE false END AS is_corrupt#11]\n",
            "   :     :     :                                         +- LogicalRDD [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5], false\n",
            "   :     :     +- LogicalRDD [driver_id#6, driver_name#7, driver_type#8], false\n",
            "   :     +- LogicalRDD [city#9, zone#10], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#74, zone#75], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#77, driver_id#79, delivery_id#12, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8, zone#10, zone#75]\n",
            "+- Join Inner, (city#77 = city#74), rightHint=(strategy=broadcast)\n",
            "   :- Project [city#77, driver_id#79, delivery_id#12, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8, zone#10]\n",
            "   :  +- Join Inner, (city#77 = city#9)\n",
            "   :     :- Project [driver_id#79, delivery_id#12, city#77, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8]\n",
            "   :     :  +- Join Inner, (driver_id#79 = driver_id#6)\n",
            "   :     :     :- Filter ((isnotnull(status#81) AND (status#81 = delivered)) AND (isnotnull(driver_id#79) AND isnotnull(city#77)))\n",
            "   :     :     :  +- Aggregate [delivery_id#12], [delivery_id#12, first(city#13, false) AS city#77, first(driver_id#14, false) AS driver_id#79, first(status#19, false) AS status#81, first(delivery_time_minutes#21, false) AS delivery_time_minutes#83, first(delivery_timestamp#22, false) AS delivery_timestamp#85, first(is_corrupt#18, false) AS is_corrupt#87]\n",
            "   :     :     :     +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN (isnull(delivery_time_minutes#20) OR (delivery_time_minutes#20 < 0)) THEN 0 ELSE delivery_time_minutes#20 END AS delivery_time_minutes#21, coalesce(gettimestamp(delivery_timestamp#17, yyyy-MM-dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, dd/MM/yyyy HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, yyyy/MM/dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, dd-MM-yyyy HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS delivery_timestamp#22, is_corrupt#18]\n",
            "   :     :     :        +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN RLIKE(delivery_time_minutes#16, ^[0-9]+$) THEN cast(delivery_time_minutes#16 as int) END AS delivery_time_minutes#20, delivery_timestamp#17, is_corrupt#18]\n",
            "   :     :     :           +- Project [trim(delivery_id#0, None) AS delivery_id#12, trim(city#1, None) AS city#13, trim(driver_id#2, None) AS driver_id#14, lower(trim(status#3, None)) AS status#19, trim(delivery_time_minutes#4, None) AS delivery_time_minutes#16, trim(delivery_timestamp#5, None) AS delivery_timestamp#17, trim(cast(((isnull(delivery_time_minutes#4) OR RLIKE(delivery_time_minutes#4, ^[0-9]+$)) <=> true) as string), None) AS is_corrupt#18]\n",
            "   :     :     :              +- LogicalRDD [delivery_id#0, city#1, driver_id#2, status#3, delivery_time_minutes#4, delivery_timestamp#5], false\n",
            "   :     :     +- Filter isnotnull(driver_id#6)\n",
            "   :     :        +- LogicalRDD [driver_id#6, driver_name#7, driver_type#8], false\n",
            "   :     +- Filter isnotnull(city#9)\n",
            "   :        +- LogicalRDD [city#9, zone#10], false\n",
            "   +- Filter isnotnull(city#74)\n",
            "      +- LogicalRDD [city#74, zone#75], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#77, driver_id#79, delivery_id#12, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8, zone#10, zone#75]\n",
            "   +- BroadcastHashJoin [city#77], [city#74], Inner, BuildRight, false\n",
            "      :- Project [city#77, driver_id#79, delivery_id#12, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8, zone#10]\n",
            "      :  +- SortMergeJoin [city#77], [city#9], Inner\n",
            "      :     :- Sort [city#77 ASC NULLS FIRST], false, 0\n",
            "      :     :  +- Exchange hashpartitioning(city#77, 200), ENSURE_REQUIREMENTS, [plan_id=267]\n",
            "      :     :     +- Project [driver_id#79, delivery_id#12, city#77, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87, driver_name#7, driver_type#8]\n",
            "      :     :        +- SortMergeJoin [driver_id#79], [driver_id#6], Inner\n",
            "      :     :           :- Sort [driver_id#79 ASC NULLS FIRST], false, 0\n",
            "      :     :           :  +- Exchange hashpartitioning(driver_id#79, 200), ENSURE_REQUIREMENTS, [plan_id=259]\n",
            "      :     :           :     +- Filter ((isnotnull(status#81) AND (status#81 = delivered)) AND (isnotnull(driver_id#79) AND isnotnull(city#77)))\n",
            "      :     :           :        +- SortAggregate(key=[delivery_id#12], functions=[first(city#13, false), first(driver_id#14, false), first(status#19, false), first(delivery_time_minutes#21, false), first(delivery_timestamp#22, false), first(is_corrupt#18, false)], output=[delivery_id#12, city#77, driver_id#79, status#81, delivery_time_minutes#83, delivery_timestamp#85, is_corrupt#87])\n",
            "      :     :           :           +- Sort [delivery_id#12 ASC NULLS FIRST], false, 0\n",
            "      :     :           :              +- Exchange hashpartitioning(delivery_id#12, 200), ENSURE_REQUIREMENTS, [plan_id=253]\n",
            "      :     :           :                 +- SortAggregate(key=[delivery_id#12], functions=[partial_first(city#13, false), partial_first(driver_id#14, false), partial_first(status#19, false), partial_first(delivery_time_minutes#21, false), partial_first(delivery_timestamp#22, false), partial_first(is_corrupt#18, false)], output=[delivery_id#12, first#100, valueSet#101, first#102, valueSet#103, first#104, valueSet#105, first#106, valueSet#107, first#108, valueSet#109, first#110, valueSet#111])\n",
            "      :     :           :                    +- Sort [delivery_id#12 ASC NULLS FIRST], false, 0\n",
            "      :     :           :                       +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN (isnull(delivery_time_minutes#20) OR (delivery_time_minutes#20 < 0)) THEN 0 ELSE delivery_time_minutes#20 END AS delivery_time_minutes#21, coalesce(gettimestamp(delivery_timestamp#17, yyyy-MM-dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, dd/MM/yyyy HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, yyyy/MM/dd HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false), gettimestamp(delivery_timestamp#17, dd-MM-yyyy HH:mm, TimestampType, try_to_timestamp, Some(Etc/UTC), false)) AS delivery_timestamp#22, is_corrupt#18]\n",
            "      :     :           :                          +- Project [delivery_id#12, city#13, driver_id#14, status#19, CASE WHEN RLIKE(delivery_time_minutes#16, ^[0-9]+$) THEN cast(delivery_time_minutes#16 as int) END AS delivery_time_minutes#20, delivery_timestamp#17, is_corrupt#18]\n",
            "      :     :           :                             +- Project [trim(delivery_id#0, None) AS delivery_id#12, trim(city#1, None) AS city#13, trim(driver_id#2, None) AS driver_id#14, lower(trim(status#3, None)) AS status#19, trim(delivery_time_minutes#4, None) AS delivery_time_minutes#16, trim(delivery_timestamp#5, None) AS delivery_timestamp#17, trim(cast(((isnull(delivery_time_minutes#4) OR RLIKE(delivery_time_minutes#4, ^[0-9]+$)) <=> true) as string), None) AS is_corrupt#18]\n",
            "      :     :           :                                +- Scan ExistingRDD[delivery_id#0,city#1,driver_id#2,status#3,delivery_time_minutes#4,delivery_timestamp#5]\n",
            "      :     :           +- Sort [driver_id#6 ASC NULLS FIRST], false, 0\n",
            "      :     :              +- Exchange hashpartitioning(driver_id#6, 200), ENSURE_REQUIREMENTS, [plan_id=260]\n",
            "      :     :                 +- Filter isnotnull(driver_id#6)\n",
            "      :     :                    +- Scan ExistingRDD[driver_id#6,driver_name#7,driver_type#8]\n",
            "      :     +- Sort [city#9 ASC NULLS FIRST], false, 0\n",
            "      :        +- Exchange hashpartitioning(city#9, 200), ENSURE_REQUIREMENTS, [plan_id=268]\n",
            "      :           +- Filter isnotnull(city#9)\n",
            "      :              +- Scan ExistingRDD[city#9,zone#10]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=274]\n",
            "         +- Filter isnotnull(city#74)\n",
            "            +- Scan ExistingRDD[city#74,zone#75]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}